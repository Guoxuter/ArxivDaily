### 1 Controllable LLM Reasoning via Sparse Autoencoder-Based Steering

**link**: https://arxiv.org/pdf/2601.03595.pdf
**date**: 2026-01-08
**keywords**: cs.AI
**abs**: 大型推理模型（LRMs）在推理过程中展现出类人的认知策略（如回溯、交叉验证），但自主选择常导致低效或错误路径。现有方法因隐藏状态中的概念纠缠难以控制细粒度策略。本文利用稀疏自编码器（SAEs）解纠缠隐藏状态，提出SAE-Steering两阶段管道：先召回放大策略关键词的特征，过滤99%以上特征；再通过控制效果排序。实验表明，该方法在控制效果上优于现有方法15%，并可将LRMs从错误路径重定向，实现7%的绝对准确率提升。

---

### 2 Advances and Challenges in Semantic Textual Similarity: A Comprehensive Survey

**link**: https://arxiv.org/pdf/2601.03270.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 语义文本相似度（STS）研究自2021年以来快速发展，受transformer架构和对比学习推动。本综述回顾六个领域进展：基于transformer的模型（如FarSSiBERT）、对比学习方法（如AspectCSE）、领域适应方案（如医疗CXR-BERT和金融Financial-STS）、多模态方法、基于图的方法及知识增强技术。分析显示，STS模型在准确性上显著提升，但领域定制和实际应用仍存挑战，为研究人员提供新兴趋势和未来机遇的见解。

---

### 3 HEEGNet: Hyperbolic Embeddings for EEG

**link**: https://arxiv.org/pdf/2601.03322.pdf
**date**: 2026-01-08
**keywords**: cs.LG
**abs**: 脑电图（EEG）脑机接口因跨域分布偏移导致泛化能力差。本文提出HEEGNet混合双曲网络架构，利用EEG中潜在的层次结构：先通过实验证明EEG数据具双曲性，双曲嵌入可提升泛化；网络结合欧几里得和双曲编码器，采用粗到细域适应策略。在多个EEG数据集（如视觉诱发电位和情感识别）上实验，HEEGNet实现最先进性能，验证了双曲空间对层次数据的有效性。

---

### 4 Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models

**link**: https://arxiv.org/pdf/2601.03388.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 隐喻是否影响大型语言模型（LLMs）推理路径？本研究发现在训练数据中，隐喻与LLMs跨域错位程度存在强因果关系。通过在预训练、微调和再对齐阶段干预隐喻，模型错位显著变化。分析表明，隐喻与LLMs的全局和局部潜在特征激活关联；基于此设计检测器，可高精度预测错位内容，为错位问题提供新机制洞察。

---

### 5 Reasoning Pattern Alignment Merging for Adaptive Reasoning

**link**: https://arxiv.org/pdf/2601.03506.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 大型推理模型（LRMs）生成冗长推理路径导致计算开销大。本文研究模型合并作为轻量级替代，合并长思维链（Long-CoT）与短思维链（Short-CoT）模型，获得自适应推理器。提出推理模式对齐合并（RPAM）框架：基于特征对齐进行分层合并，构建模式标记校准集优化合并系数，并使用对比目标远离非选定模型。在七个基准测试中，RPAM大幅降低推理成本同时保持高性能。

---

### 6 Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents

**link**: https://arxiv.org/pdf/2601.03515.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 长期记忆是多模态大型语言模型（MLLM）代理的关键能力。本文引入Mem-Gallery基准，评估多模态长期对话记忆，包含基于视觉和文本信息的多会话对话。提出评估框架，从记忆提取与测试时适应、记忆推理、记忆知识管理三维度分析。对十三个系统的测试揭示：需显式多模态信息保留和记忆组织、推理和管理的持续局限，以及模型效率瓶颈。

---

### 7 Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models

**link**: https://arxiv.org/pdf/2601.03542.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）多跳推理的内部机制不明。本文挑战“跳对齐电路假设”，发现后跳答案实体可能比桥接实体更早解码（层序反转），且随跳数增加增强。提出“概率召回-提取”框架：浅层MLP层广泛概率召回，深层注意力层选择性提取。通过探测分析验证，解释思维链增益，并为多跳失败提供机制诊断。

---

### 8 Learning Shrinks the Hard Tail: Training-Dependent Inference Scaling in a Solvable Linear Model

**link**: https://arxiv.org/pdf/2601.03764.pdf
**date**: 2026-01-08
**keywords**: cs.LG
**abs**: 在潜在实例难度（LID）模型中分析神经缩放定律，目标方差由潜在“精度”控制。泛化损失符合标准缩放定律，但pass@k失败率呈幂律衰减k^{-β_eff}，指数β_eff随样本量N增长，在内在极限β处饱和。这表明学习缩小误差分布的“硬尾”：泛化改善使pass@k曲线变陡，直到不可约目标方差主导。模型预测在CIFAR-10H和数学任务中验证。

---

### 9 Implicit Graph, Explicit Retrieval: Towards Efficient and Interpretable Long-horizon Memory for Large Language Models

**link**: https://arxiv.org/pdf/2601.03417.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 长程应用需大型语言模型（LLMs）在稀疏证据中回答查询。本文提出LatentGraphMem框架，结合隐式图记忆与显式子图检索：图结构记忆存储在潜在空间确保效率，显式接口返回紧凑子图用于推理和检查。训练时，显式图视图实例化以监督问答；推理时，潜在空间检索。在长程基准测试中优于显式和潜在记忆基线，支持参数高效适应。

---

### 10 Towards Compositional Generalization of LLMs via Skill Taxonomy Guided Data Synthesis

**link**: https://arxiv.org/pdf/2601.03676.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）因数据瓶颈难以实现组合泛化。提出STEPS框架：基于技能分类学和熵合成组合挑战性数据。通过揭示技能间潜在关系构建层级技能分类学，将数据合成表述为约束信息最大化问题，选择最大化边际结构信息且语义连贯的技能组合。实验表明，STEPS在指令跟随基准上优于基线，提升下游智能体任务的组合泛化能力。

---

### 11 A Pre-trained Reaction Embedding Descriptor Capturing Bond Transformation Patterns

**link**: https://arxiv.org/pdf/2601.03689.pdf
**date**: 2026-01-08
**keywords**: cs.LG
**abs**: 有效反应描述符稀缺。本文引入RXNEmb反应级描述符，源自RXNGraphormer模型：预训练区分真实反应与虚构反应，学习键形成和断裂模式。在USPTO-50k数据集上，RXNEmb实现数据驱动重新聚类，分类更直接反映键变化相似性；结合降维可视化反应空间多样性。注意力权重分析揭示化学关键位点，为反应分析提供可解释工具。

---

### 12 ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning

**link**: https://arxiv.org/pdf/2601.03646.pdf
**date**: 2026-01-08
**keywords**: cs.LG
**abs**: 作业调度在制造系统中应用广泛，但现有方法在规模增大时性能不足。提出ReLA强化学习调度器：通过自注意力和卷积的实体内学习模块及交叉注意力的实体间学习模块，从作业操作和机器中学习多样化表示；多尺度架构聚合输出支持决策。实验表明，ReLA在中小大型实例上优于最新方案，最优性差距降至7.3%和2.1%，实现快速作业完成。

---

### 13 In Search of Grandmother Cells: Tracing Interpretable Neurons in Tabular Representations

**link**: https://arxiv.org/pdf/2601.03657.pdf
**date**: 2026-01-08
**keywords**: cs.LG
**abs**: 基础模型决策过程不透明。本文提出信息论度量量化神经元对单一概念的显著性和选择性，应用于TabPFN表格模型。通过简单搜索神经元-概念对，发现某些神经元对高级概念表现出统计显著的显著性和选择性。这表明可解释神经元可自然出现，无需复杂技术即可识别，为模型透明度提供新途径。

---

### 14 e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings

**link**: https://arxiv.org/pdf/2601.03666.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 全模态嵌入模型需处理异质模态，但现有方法依赖隐式对齐导致问题：相似性尺度不一致、批内负样本失效、嵌入统计不匹配。提出e5-omni轻量级显式对齐方法：结合模态感知温度校准对齐尺度、可控负样本课程聚焦易混淆样本、协方差正则化批白化匹配跨模态几何。在MMEB-V2和AudioCaps上优于基线，验证其鲁棒性。

---

### 15 On the Identifiability of Regime-Switching Models with Multi-Lag Dependencies

**link**: https://arxiv.org/pdf/2601.03325.pdf
**date**: 2026-01-08
**keywords**: stat.ML
**abs**: 深度潜变量模型的可识别性对可解释性至关重要。本文为多滞后机制切换模型（RSM）开发理论框架：对马尔可夫切换模型（MSMs），证明在非线性高斯设置下机制数量和多滞后转移的可识别性；对切换动力系统（SDSs），通过时间结构确立潜变量在置换和缩放下的可识别性，进而得出机制依赖的潜因果图可识别性条件。结果在神经科学和金融数据中验证。

---

### 16 CALM: Culturally Self-Aware Language Models

**link**: https://arxiv.org/pdf/2601.03483.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 语言模型文化意识常忽视文化动态性。提出CALM框架赋予模型文化自我意识：分离任务语义与文化概念，通过对比学习塑造结构化文化簇；交叉注意力对齐建立特征交互，专家混合机制自适应整合；统一表示与原始知识融合，通过自我提示反思实现持续适应。在跨文化基准上实验，CALM持续优于最先进方法。

---

### 17 LLM-MC-Affect: LLM-Based Monte Carlo Modeling of Affective Trajectories and Latent Ambiguity for Interpersonal Dynamic Insight

**link**: https://arxiv.org/pdf/2601.03645.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 情感协调是人类互动核心。本文提出LLM-MC-Affect概率框架：将情感表征为潜在概率分布，利用随机LLM解码和蒙特卡洛估计推导高保真情感轨迹，量化情感倾向和感知模糊性；通过顺序互相关和斜率指标分析人际耦合。以师生对话为例，该方法成功提炼如有效支架等高层互动洞察，为社交研究提供通用工具。

---

### 18 Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning

**link**: https://arxiv.org/pdf/2601.03823.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 强化学习与可验证奖励（RLVR）方法在长链推理中优势估计粒度粗。本文提出无需训练的探测机制，提取中间置信度和正确性组合为Step Potential信号，显式估计推理进度。基于此，Step Potential Advantage Estimation（SPAE）进行细粒度信用分配：放大潜在收益、惩罚潜在下降，并在饱和后鼓励终止。实验表明，SPAE提高准确性同时缩短响应长度，优于高效推理基线。

---

### 19 Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion

**link**: https://arxiv.org/pdf/2601.04056.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 生成建模在离散（文本）和连续（图像）数据间存在分野。提出CoM-DAD概率框架统一多模态生成：高层语义规划通过连续潜在扩散建模；低层令牌合成通过离散吸收扩散过程，由演化语义先验调节。引入随机混合模态传输策略对齐模态，无需双编码器。该方法优于标准掩码建模，为统一文本-图像生成建立新范式。

---

### 20 When Models Decide and When They Bind: A Two-Stage Computation for Multiple-Choice Question-Answering

**link**: https://arxiv.org/pdf/2601.03914.pdf
**date**: 2026-01-08
**keywords**: cs.CL
**abs**: 多项选择问答（MCQA）将推理错误与符号绑定失败混淆。通过表征分析和因果干预研究语言模型内部机制：选项边界残差状态包含强可解码信号；赢家身份探测揭示两阶段进展—内容空间先选赢家，再绑定到输出符号。测试支持此机制，模型在内容空间决策后在符号层路由答案。