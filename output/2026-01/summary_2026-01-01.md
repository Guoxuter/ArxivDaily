### 1 Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study

**link**: https://arxiv.org/pdf/2512.24102.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 本文基于先前介绍的架构，对GP-VAE模型中的潜在自回归进行了基于消融的分析。语言模型通常依赖于对tokens的自回归分解，而作者先前的工作提出通过因果高斯过程将序列结构转移到潜在空间，同时使用非自回归解码器。本文系统地研究了潜在自回归的作用，比较了（i）具有自回归潜在动态的完整GP-VAE模型、（ii）潜在变量独立的非自回归消融模型以及（iii）标准的token级自回归Transformer。结果表明，在中等规模语料库和短训练上下文条件下，潜在自回归诱导的潜在轨迹与高斯过程先验更兼容，并表现出更强的长期稳定性；而移除自回归会导致潜在结构退化和长期行为不稳定。这些发现强调潜在自回归是组织长程结构的有效机制，且与token级自回归建模互补，应被视为对表征结构的实证分析而非新架构提议。

---

### 2 Trellis: Learning to Compress Key-Value Memory in Attention Models

**link**: https://arxiv.org/pdf/2512.23852.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: Transformer虽然功能强大，但存在二次计算复杂度和注意力机制中不断增长的键值（KV）缓存问题。本文介绍了Trellis，一种具有有界内存的新型Transformer架构，能够在测试时动态学习如何压缩其键值内存。Trellis用固定大小的内存替换标准KV缓存，并训练双通循环压缩机制将新的键和值存储到内存中。为实现这一点，它利用带有遗忘门的在线梯度下降过程，使压缩内存能够递归更新，同时学习在测试时保留来自输入标记的重要上下文信息。在语言建模、常识推理、回忆密集型任务和时间序列上的大量实验表明，所提出的架构优于强基线。值得注意的是，随着序列长度的增长，其性能增益增加，突出了其在长上下文应用中的潜力。

---

### 3 Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining

**link**: https://arxiv.org/pdf/2512.23862.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 本研究调查了小型语言模型（SLMs）的小规模预训练，以实现有限数据和计算资源的高效利用，提高低资源环境中的可访问性并降低成本。为增强紧凑模型的长上下文外推能力，我们重点研究了Infini-attention，它从过去的片段构建压缩内存，同时保留局部注意力。在工作中，我们使用3亿参数的LLaMA模型进行了实证研究，这些模型通过Infini-attention进行预训练。该模型表现出训练稳定性，并在长上下文检索任务上优于基线。我们发现平衡因子是模型性能的关键部分，并且发现随着长序列上重复的内存压缩，检索准确性会下降。尽管如此，Infini-attention仍然有效地弥补了SLM参数有限的不足。特别是，尽管在16,384 token的上下文下性能有所下降，但Infini-attention模型的准确率仍比基线高出31%。我们的研究结果表明，在SLMs中实现强大的长上下文能力得益于Infini-attention等架构内存。

---

### 4 FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading

**link**: https://arxiv.org/pdf/2512.23773.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 期货是在预定日期和价格交换资产的合约，因其高杠杆和流动性在加密市场中表现突出。强化学习（RL）已广泛应用于各种量化任务，但大多数方法专注于现货市场，无法直接应用于高杠杆的期货市场，主要面临两个挑战：一是高杠杆放大了奖励波动，导致训练具有随机性且难以收敛；二是现有工作缺乏对能力边界的自我认知，在遇到新的市场状态（如COVID-19等黑天鹅事件）时面临重大损失风险。为解决这些问题，本文提出了用于期货交易的高效且具有风险感知的集成强化学习框架（FineFT），这是一种新颖的三阶段集成RL框架，具有稳定的训练和适当的风险管理能力。在第一阶段，集成Q学习器通过集成TD误差进行选择性更新以提高收敛性。在第二阶段，基于学习器的盈利能力对其进行筛选，并在市场状态上训练变分自编码器（VAEs）以识别学习器的能力边界。在第三阶段，在经过训练的VAEs的指导下，从筛选后的集成和保守策略中进行选择，以在新的市场状态下保持盈利能力并降低风险。通过在高保真度和5倍杠杆的高频交易环境中对加密期货进行广泛实验，结果表明FineFT在6个财务指标上优于12个最先进的基线，与第二名相比，风险降低了40%以上，同时实现了更高的盈利能力。选择性更新机制的可视化显示不同智能体专门处理不同的市场动态，消融研究证明使用VAEs进行路由可有效降低最大回撤，选择性更新提高了收敛性和性能。

---

### 5 Geometric Scaling of Bayesian Inference in LLMs

**link**: https://arxiv.org/pdf/2512.23752.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 近期研究表明，在受控的"风洞"环境中训练的小型Transformer能够实现精确的贝叶斯推理，其训练动态产生了一种几何基底——低维值流形和渐进正交的键——用于编码后验结构。本文研究了这种几何特征是否在生产级语言模型中持续存在。在Pythia、Phi-2、Llama-3和Mistral系列模型中，研究发现最后一层的值表示沿着单一主导轴组织，其位置与预测熵强相关，且领域受限的提示会将这种结构压缩为在合成环境中观察到的相同低维流形。

---

### 6 HINTS: Extraction of Human Insights from Time-Series Without External Sources

**link**: https://arxiv.org/pdf/2512.23755.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 人类决策、情感和集体心理是塑造金融和经济系统中观察到的时间动态的复杂因素。许多近期的时间序列预测模型利用外部来源（如新闻和社交媒体）来捕捉人类因素，但这些方法在财务、计算和实际应用方面带来了高昂的数据依赖成本。在本研究中，我们提出HINTS，一种自监督学习框架，无需外部数据，从时间序列残差中内生地提取这些潜在因素。HINTS利用Friedkin-Johnsen（FJ）意见动态模型作为结构归纳偏置，来建模演化的社会影响、记忆和偏见模式。提取的人类因素作为注意力图集成到最先进的骨干模型中。使用九个真实世界和基准数据集的实验结果表明，HINTS一致提高了预测准确性。此外，多个案例研究和消融研究验证了HINTS的可解释性，证明提取的因素与现实世界事件之间具有强烈的语义对齐，展示了HINTS的实际效用。

---

### 7 Information-Theoretic Quality Metric of Low-Dimensional Embeddings

**link**: https://arxiv.org/pdf/2512.23981.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 本文从信息论角度研究低维嵌入的质量。经典评估指标如应力、基于秩的邻域准则或局部普罗克拉斯提斯分析量化距离或局部几何的失真，但未直接评估将高维数据投影到低维空间时保留的信息量。为解决此局限，本文引入熵秩保持度量（ERPM），这是一种基于邻域矩阵奇异值谱的香农熵和稳定秩的局部度量，用于量化原始表示与其降维投影之间的不确定性变化，提供邻域级指标和全局汇总统计量。通过金融时间序列和文献中常见的流形验证，ERPM与基于距离的平均相对秩误差（MRRE）和基于几何性质的局部普罗克拉斯提斯分析相比，距离基准则与几何和谱度量相关性很低，而ERPM和局部普罗克拉斯提斯分析显示强平均相关性，但在局部区域存在显著差异，表明ERPM通过识别信息严重丢失的邻域补充现有指标，从而在信息敏感应用（如构建预警指标）中实现更全面的嵌入评估。

---

### 8 Hyperspherical Graph Representation Learning via Adaptive Neighbor-Mean Alignment and Uniformity

**link**: https://arxiv.org/pdf/2512.24062.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 图表示学习（GRL）旨在将图结构数据的结构和语义依赖编码为低维嵌入。现有GRL方法常依赖代理对比目标或互信息最大化，需要复杂架构、负采样策略和敏感超参数调优，可能导致过平滑、过压缩和训练不稳定性。本文提出HyperGRL，一种统一的超球面图表示学习框架，通过自适应邻域均值对齐和无采样均匀性实现。HyperGRL通过两个对抗耦合目标将节点嵌入到单位超球面：邻域均值对齐目标使用每个节点局部邻域的平均表示构建语义接地的稳定目标，捕捉共享结构和特征模式；均匀性目标通过基于L2的超球面正则化制定分散度，鼓励全局均匀的嵌入分布同时保留判别信息。此外，引入熵引导的自适应平衡机制动态调节对齐和均匀性之间的相互作用，无需手动调优。实验表明，HyperGRL在节点分类、聚类和链接预测任务上优于现有方法，证明了基于几何的无采样对比目标对图表示学习的有效性。

---

### 9 How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns

**link**: https://arxiv.org/pdf/2512.24063.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）表现出显著不同的泛化行为：监督微调（SFT）通常会缩小能力范围，而强化学习（RL）调优倾向于保留能力。这种差异的原因尚不清楚，因为先前研究主要依赖粗略的准确性指标。本文通过引入一个新基准解决此差距，该基准将推理分解为原子核心技能，如计算、事实检索、模拟、枚举和诊断，为解决LLMs中推理构成的基本问题提供了具体框架。通过分离和测量这些核心技能，该基准提供了更细致的视角，了解特定认知能力在训练后如何出现、迁移有时崩溃。结合对低级别统计模式（如分布差异和参数统计）的分析，能够精细研究SFT和RL在数学、科学推理和非推理任务中的泛化演变。元探测框架跟踪不同训练阶段的模型行为，发现RL调优模型保持更稳定的行为轮廓并抵抗推理技能崩溃，而SFT模型表现出更剧烈的漂移并过拟合表面模式。这项工作为LLMs中推理的本质提供了新见解，并为设计促进广泛、稳健泛化的训练策略指明了方向。

---

### 10 Gradient Descent as Implicit EM in Distance-Based Neural Models

**link**: https://arxiv.org/pdf/2512.24780.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 该论文探讨了神经网络在标准目标训练下表现出的概率推理行为，如软聚类、原型特化和贝叶斯不确定性跟踪。研究发现，对于任何具有基于距离或能量的log-sum-exp结构的目标函数，梯度下降隐含地执行期望最大化（EM）算法，其中组件的后验责任（responsibilities）通过梯度直接体现，无需显式计算辅助变量。这一结果统一了无监督混合建模、注意力机制和交叉熵分类等场景下的学习机制，揭示了优化与推理过程的内在一致性，而非涌现特性。

---

### 11 Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning

**link**: https://arxiv.org/pdf/2512.24404.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 多模态智能在视觉理解和高级推理方面最近取得了显著进展。然而，大多数推理系统仍然依赖文本信息作为主要推理媒介，这限制了它们在视觉导航和地理定位等空间任务中的有效性。本文讨论了该领域的潜在范围，并最终提出了一种视觉推理范式——地理一致性视觉规划，即我们引入的名为ViReLoc（Visual Reasoning for Localization）的框架，该框架仅使用视觉表示进行规划和定位。所提出的框架学习基于文本的推理往往难以理解的空间依赖关系和几何关系。通过在视觉域中编码逐步推理并使用基于强化学习的目标进行优化，ViReLoc规划两个给定地面图像之间的路线。该系统还集成了对比学习和自适应特征交互，以对齐跨视图视角并减少视点差异。在不同导航和定位场景的实验表明，该方法在空间推理准确性和跨视图检索性能方面取得了一致的改进。这些结果确立了视觉推理作为导航和定位的强大补充方法，并表明此类任务可以在没有实时全球定位系统数据的情况下执行，从而实现更安全的导航解决方案。

---

### 12 HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors

**link**: https://arxiv.org/pdf/2512.24478.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 从观测数据中发现因果关系在根本上受到可识别性约束的限制。最近的工作探索利用大型语言模型（LLMs）作为先验因果知识的来源，但现有方法依赖于缺乏理论基础的启发式集成。我们引入HOLOGRAPH框架，该框架通过层积理论将LLM引导的因果发现形式化——将局部因果信念表示为变量子集上预层的截面。我们的核心见解是，连贯的全局因果结构对应于全局截面的存在，而拓扑障碍表现为非零层积上同调。我们提出代数潜变量投影（Algebraic Latent Projection）来处理隐藏的混杂因素，并在信念流形上使用自然梯度下降进行原则性优化。在合成和现实世界基准上的实验表明，HOLOGRAPH提供了严格的数学基础，同时在具有50-100个变量的因果发现任务上取得了有竞争力的性能。我们的层积理论分析表明，虽然恒等性、传递性和粘合公理在数值精度（<10^{-6}）内得到满足，但对于较大的图，局部性公理失败，这表明潜变量投影中存在基本的非局部耦合。

---

### 13 Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space

**link**: https://arxiv.org/pdf/2512.24617.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）对所有标记应用统一计算，尽管语言表现出高度非均匀的信息密度。这种标记统一机制在局部可预测的片段上浪费容量，而在语义关键的转换上分配不足的计算资源。我们提出了动态大型概念模型（DLCM），这是一种分层语言建模框架，它从潜在表示中学习语义边界，并将计算从标记转移到压缩的概念空间，在该空间中推理更高效。DLCM 端到端地发现变长概念，无需依赖预定义的语言单元。分层压缩从根本上改变了缩放行为。我们引入了第一个压缩感知缩放定律，该定律将标记级容量、概念级推理容量和压缩比解耦，能够在固定 FLOPs 下进行有原则的计算分配。为了稳定训练这种异构架构，我们进一步开发了一种解耦的 μP 参数化方法，支持跨宽度和压缩机制的零样本超参数迁移。在实际设置中（R=4，对应平均每个概念四个标记），DLCM 将大约三分之一的推理计算重新分配到更高容量的推理主干中，在匹配的推理 FLOPs 下，在 12 个零样本基准测试中平均提高了 2.69%。

---

### 14 Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling

**link**: https://arxiv.org/pdf/2512.23959.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 多步检索增强生成（RAG）已成为增强大型语言模型（LLMs）在需要全局理解和密集推理任务上性能的广泛采用策略。许多RAG系统包含工作记忆模块来整合检索到的信息。然而，现有记忆设计主要作为被动存储，积累孤立事实以浓缩长输入并通过演绎生成新子查询。这种静态特性忽略了原始事实之间关键的高阶相关性，而这些相关性的组合通常能为后续步骤提供更强指导。因此，它们的表征能力以及对多步推理和知识演化的影响有限，导致在扩展上下文中出现碎片化推理和弱全局感知能力。本文引入HGMem，一种基于超图的记忆机制，将记忆概念从简单存储扩展为用于复杂推理和全局理解的动态、表达性结构。在我们的方法中，记忆表示为超图，其超边对应不同的记忆单元，能够在记忆中逐步形成高阶交互。该机制围绕焦点问题连接事实和思想，演化为集成的情境化知识结构，为后续步骤的深度推理提供强有力的命题。我们在多个为全局感知设计的具有挑战性的数据集上评估HGMem。大量实验和深入分析表明，我们的方法持续改进多步RAG，并在不同任务上显著优于强大的基线系统。

---

### 15 Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process

**link**: https://arxiv.org/pdf/2512.23988.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 尽管最近大型语言模型（LLMs）的推理能力不断增强，但其推理过程中的内部机制仍未被充分探索。现有方法通常依赖人类定义的概念（如过度思考、反思）在词级别以监督方式分析推理。然而，此类方法存在局限性，因为无法捕捉潜在推理行为的全部范围，其中许多行为难以在 token 空间中定义。在这项工作中，我们提出一种无监督框架（即RISE：通过稀疏自编码器实现推理行为可解释性）来发现推理向量，我们将其定义为激活空间中编码不同推理行为的方向。通过将思维链轨迹分割为句子级“步骤”并在步骤级激活上训练稀疏自编码器（SAEs），我们发现了与可解释行为（如反思和回溯）对应的解纠缠特征。可视化和聚类分析表明，这些行为在解码器列空间中占据可分离区域。此外，对SAE衍生向量的定向干预可以可控地放大或抑制特定推理行为，无需重新训练即可改变推理轨迹。除了特定行为的解纠缠，SAEs还捕获响应长度等结构属性，揭示长推理轨迹与短推理轨迹的集群。更有趣的是，SAEs能够发现超越人类监督的新行为。我们通过在SAE解码器空间中识别与置信度相关的向量，证明了控制响应置信度的能力。这些发现强调了无监督潜在发现对于解释和可控引导LLMs推理的潜力。

---

### 16 Attribution-Guided Distillation of Matryoshka Sparse Autoencoders

**link**: https://arxiv.org/pdf/2512.24975.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 稀疏自编码器（SAE）旨在将模型激活分解为单义的、人类可解释的特征。但在实践中，学习到的特征往往存在冗余，且在不同训练轮次和稀疏度水平下会发生变化，导致解释难以迁移和复用。本文提出了蒸馏嵌套稀疏自编码器（DMSAEs），这是一种训练流水线，能够蒸馏出一组紧凑且持续有用的核心特征，并利用该核心训练新的SAE。DMSAEs执行迭代蒸馏循环：首先训练具有共享核心的嵌套SAE，然后使用梯度X激活来衡量每个特征对最深层重构中下一token损失的贡献，并仅保留能解释固定比例归因的最小子集。跨循环仅迁移核心编码器权重向量；核心解码器和所有非核心潜变量在每次循环中都会重新初始化。在Gemma-2-2B第12层残差流激活上，经过七次蒸馏循环（5亿token，65k宽度），得到了包含197个被重复选择特征的蒸馏核心。使用该蒸馏核心进行训练改善了多项SAEBench指标，并证明了一致的潜特征集可以跨稀疏度水平迁移。

---

### 17 On the geometry and topology of representations: the manifolds of modular addition

**link**: https://arxiv.org/pdf/2512.25060.pdf
**date**: 2026-01-01
**keywords**: cs.LG
**abs**: 与具有均匀注意力或可学习注意力的架构相关的“时钟和披萨”解释，最初被用于论证不同架构设计会产生不同的模块化加法电路。本文表明事实并非如此，均匀注意力和可训练注意力架构通过拓扑和几何等效的表示实现了相同的算法。该方法超越了对单个神经元和权重的解释，而是识别与每个学习表示对应的所有神经元，并将这些神经元群体作为一个整体进行研究。这种方法揭示出每个学习表示都是一个流形，可利用拓扑学工具对其进行研究。基于这一见解，通过对数百个电路的学习表示进行统计分析，可以证明由常见深度学习范式自然产生的模块化加法电路之间的相似性。

---

### 18 Implicit geometric regularization in flow matching via density weighted Stein operators

**link**: https://arxiv.org/pdf/2512.23956.pdf
**date**: 2026-01-01
**keywords**: stat.ML
**abs**: 流匹配（FM）已成为连续归一化流的强大范式，但标准FM在整个环境空间上隐式执行未加权的L²回归。在高维情况下，这导致了一个基本低效问题：积分域的绝大部分由低密度“空白”区域组成，这些区域中的目标速度场通常是混乱或定义不清的。在本文中，我们提出了γ-流匹配（γ-FM），这是一种密度加权变体，可使回归几何与潜在概率流对齐。尽管密度加权是理想的，但朴素实现需要评估难以处理的目标密度。我们通过引入动态密度加权策略来规避这一问题，该策略直接从训练粒子估计目标密度。这种方法允许我们在空白区域动态降低回归损失的权重，同时不影响FM的无模拟特性。理论上，我们证明γ-FM在赋予γ-Stein度量的统计流形上最小化传输成本。谱分析进一步表明，这种几何结构诱导了隐式Sobolev正则化，有效抑制了空白区域的高频振荡。实证上，γ-FM显著提高了高维latent数据集上的向量场平滑度和采样效率，同时表现出对异常值的内在鲁棒性。

---

### 19 Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time

**link**: https://arxiv.org/pdf/2512.24574.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 大型语言模型(LLMs)通常依赖长链推理(CoT)解决复杂任务，但这些推理轨迹往往效率低下或不稳定。本文研究推理轨迹结构，发现与验证和回溯等认知行为相关的特定注意力头，并在推理时对这些头进行干预以引导模型远离低效模式。提出CREST方法，通过离线校准识别认知头并推导转向向量，在推理时旋转隐藏表示以抑制非生产性推理行为，从而提高准确率并降低计算成本。

---

### 20 mHC: Manifold-Constrained Hyper-Connections

**link**: https://arxiv.org/pdf/2512.24880.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 超连接(HC)通过扩展残差流宽度和多样化连接模式提升性能，但破坏了残差连接的恒等映射特性，导致训练不稳定和内存开销问题。本文提出流形约束超连接(mHC)框架，将HC的残差连接空间投影到特定流形以恢复恒等映射特性，并结合基础设施优化确保效率。该方法在大规模训练中表现出显著的性能提升和可扩展性。

---

### 21 CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution

**link**: https://arxiv.org/pdf/2512.23880.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: 当前LLM智能体依赖预定义工具，限制了复杂科学任务的能力和适应性。本文提出CASCADE自演化智能体框架，实现从"LLM+工具使用"到"LLM+技能获取"的转变。该框架通过元技能（如网络搜索、代码提取、内省和知识图谱探索）掌握复杂工具并编码知识，其中包含记忆巩固机制。在SciSkillBench基准测试中，使用GPT-5时成功率达93.3%，展示了在计算分析、自主实验室实验和论文复现中的应用。

---

### 22 LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm

**link**: https://arxiv.org/pdf/2512.24077.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: 传统进化方法在高维代码空间中存在早熟收敛和探索效率低的问题，阻碍了静态大型语言模型向自改进智能体的转变。本文提出LoongFlow，一种自进化智能体框架，通过整合大型语言模型到认知“计划-执行-总结”（PES）范式中，将进化搜索映射为注重推理的过程，并结合混合进化记忆系统来维持长期架构一致性。该记忆系统协同多岛模型、MAP-Elites和自适应玻尔兹曼选择，理论上平衡探索与利用，防止优化停滞。在AlphaEvolve基准和Kaggle竞赛中的评估表明，LoongFlow在进化效率上比领先基线（如OpenEvolve、ShinkaEvolve）高出60%，同时发现更优解，为自主科学发现迈出重要一步。

---

### 23 Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents

**link**: https://arxiv.org/pdf/2512.24461.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: 本文提出一种测试时自适应智能体，该智能体在部分可观测环境下运行的大型语言模型智能体中，通过后验引导的信念精化执行探索性推理，无需基于梯度的更新或额外训练。智能体维持对环境状态的外部结构化信念，通过动作条件观测迭代更新信念，并通过最大化信念空间上的预测信息增益来选择动作。使用轻量级大型语言模型代理估计信息增益，并通过量化后验信念与真实环境配置一致性的新奖励评估世界对齐。实验表明，该方法在与潜在世界状态对齐方面优于推理时扩展基线（如提示增强或检索增强的大型语言模型），且集成开销显著降低。

---

### 24 Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings

**link**: https://arxiv.org/pdf/2512.25055.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: 本研究提出了一个基于大型语言模型（LLM）的建筑能源管理系统（BEMS）AI代理概念框架和原型评估，旨在通过自然语言交互促进智能建筑中的上下文感知能源管理。该框架包括感知（传感）、中央控制（大脑）和行动（执行与用户交互）三个模块，形成一个闭环反馈回路，用于捕获、分析和解释能源数据，以智能响应用户查询并管理连接的设备。通过利用LLM的自主数据分析能力，BEMS AI代理旨在提供关于能源消耗、成本预测和设备调度的上下文感知洞察，从而解决现有能源管理系统的局限性。原型性能通过120个用户查询在四个不同的真实世界住宅能源数据集上进行了评估，涉及延迟、功能、能力、准确性和成本效益等不同评估指标。ANOVA测试证明了该框架的通用性。结果显示，在设备控制（86%）、记忆相关任务（97%）、调度和自动化（74%）以及能源分析（77%）方面的响应准确性表现良好，而更复杂的成本估算任务（49%）则凸显了需要改进的领域。这项基准研究为正式评估基于LLM的BEMS AI代理和确定未来研究方向奠定了基础，强调了响应准确性和计算效率之间的权衡。

---

### 25 iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning

**link**: https://arxiv.org/pdf/2512.24014.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）在显式文本计划的引导下能够进行可靠的逐步推理，但由于LLM的幻觉问题和任务特定问题的高度多样性，生成准确有效的文本计划仍具挑战。为此，本文借鉴人类内隐认知（IC）——一种无需显式语言化、通过从过往经验中学习到的紧凑、广义模式引导决策的潜意识过程，提出了iCLP框架。该框架使LLMs能够自适应生成潜在计划（LPs），即有效推理指令的紧凑编码。iCLP首先从现有的逐步推理轨迹中提取显式计划，然后通过结合码本的向量量化自编码器学习这些计划的离散表示，最后通过在配对的潜在计划和相应推理步骤上微调LLMs，使模型学会在推理过程中进行隐式规划。在数学推理和代码生成任务上的实验结果表明，iCLP使LLMs能够在潜在空间中规划，同时在语言空间中推理，该方法在准确性和效率上均有显著提升，并在保持思维链推理可解释性的同时展现出强大的跨域泛化能力。

---

### 26 Modeling Language as a Sequence of Thoughts

**link**: https://arxiv.org/pdf/2512.25026.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: Transformer语言模型通过将语言建模为标记序列来生成自然文本，但主要依赖表面共现统计，无法形成实体和事件的全局一致潜在表示，这导致了关系方向的脆弱性（如反转诅咒）、语境化错误和数据低效性。受认知科学中人类理解涉及将输入语言流转换为紧凑、事件样表示并在记忆中持续存在的观点启发，本文引入Thought Gestalt（TG）模型，一种递归Transformer，在标记和句子级“思维”状态两个抽象层次上建模语言。TG一次生成一个句子的标记，同时交叉关注先前句子表示的记忆。在TG中，标记和句子表示使用相同的模型参数集，并通过单一的下一个标记交叉熵目标进行训练：通过保留写入记忆的句子表示的计算图，未来标记损失的梯度通过交叉注意力反向流动，以优化生成早期句子向量的参数。扩展实验表明，TG在匹配的GPT-2运行和其他基线模型上持续提高效率，扩展拟合表明GPT-2需要约5-8%更多的数据和33-42%更多的参数才能匹配TG的损失。TG还减少了父子反转诅咒探针上关系方向泛化的错误。

---

### 27 Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models

**link**: https://arxiv.org/pdf/2512.24618.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 本文介绍了Youtu-LLM，这是一个轻量级但功能强大的语言模型，它协调了高计算效率与原生智能体智能。与依赖蒸馏的典型小型模型不同，Youtu-LLM（1.96B）是从头开始预训练的，系统地培养推理和规划能力。关键技术进步如下：（1）支持长上下文的紧凑架构：基于密集的多潜在注意力（MLA）架构，采用新颖的STEM导向词汇表，Youtu-LLM支持128k上下文窗口。这种设计能够在最小内存占用内实现强大的长上下文推理和状态跟踪，使其成为长视野智能体和推理任务的理想选择。（2）有原则的“常识-STEM-智能体”课程：我们策划了大约11T令牌的大规模语料库，并实施了多阶段训练策略。通过逐步将预训练数据分布从一般常识转向复杂的STEM和智能体任务，确保模型获得深层认知能力而非表面对齐。（3）可扩展的智能体中期训练：专门针对智能体中期训练，我们采用多样化的数据构建方案来合成数学、编码和工具使用领域的丰富多样的轨迹。广泛的评估表明，Youtu-LLM为亚2B LLM设定了新的技术水平。在一般基准测试中，它实现了与更大模型的竞争性能，而在智能体特定任务上，它显著超越了现有的SOTA基线，证明轻量级模型可以拥有强大的内在智能体能力。

---

### 28 Emergent World Beliefs: Exploring Transformers in Stochastic Games

**link**: https://arxiv.org/pdf/2512.23722.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 基于Transformer的大型语言模型（LLMs）在多个领域展现出强大的推理能力，从解决编程挑战到参与象棋等策略密集型游戏。先前的研究表明，LLMs能够在完全信息博弈中发展出涌现的世界模型，其内部表征对应于环境的潜在状态。本文将这一研究方向扩展到信息不完全的领域，聚焦扑克这一典型的部分可观测马尔可夫决策过程（POMDP）。我们在扑克手牌历史（PHH）数据上预训练了一个GPT风格的模型，并探测其内部激活。结果表明，该模型在没有明确指令的情况下，既学习了确定性结构（如手牌等级），也学习了随机性特征（如胜率）。此外，通过主要使用非线性探针，我们证明这些表征是可解码的，并且与理论信念状态相关，表明LLMs正在学习德扑这一随机环境的自身表征。

---

### 29 Activation Steering for Masked Diffusion Language Models

**link**: https://arxiv.org/pdf/2512.24143.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 掩码扩散语言模型（MDLMs）通过迭代去噪过程生成文本。由于其掩码并行解码能力以及与自回归大型语言模型相当的性能，它们最近受到关注。然而，MDLMs中有效的推理时控制和引导机制仍未得到充分探索。我们提出了一种适用于MDLMs的激活引导框架，该框架使用对比示例通过单次前向传播计算分层引导向量，无需模拟去噪轨迹。这些方向在每个反向扩散步骤中应用，产生了一种高效的推理时控制机制。在LLaDA-8B-Instruct上的实验表明，该框架能够可靠地调制高级属性，消融实验检验了在Transformer子模块和标记范围（提示与响应）上引导的效果。

---

### 30 Large Emotional World Model

**link**: https://arxiv.org/pdf/2512.24149.pdf
**date**: 2026-01-01
**keywords**: cs.CL
**abs**: 世界模型作为理解世界当前状态并预测其未来动态的工具，在众多领域具有广泛的应用潜力。情感作为世界知识的关键组成部分，显著影响人类决策。尽管现有的大型语言模型（LLMs）已初步展现出捕捉世界知识的能力，但它们主要关注对物理世界规律的建模，缺乏对情感因素的系统性探索。本文首先通过实验表明，移除情感相关信息会降低推理性能，从而证明情感在理解世界中的重要性。受心理理论启发，我们进一步提出了大型情感世界模型（LEWM）。具体而言，我们构建了情感-原因-方式（EWH）数据集，该数据集将情感整合到因果关系中，能够推理行为发生的原因以及情感如何驱动未来的世界状态。基于此数据集，LEWM将情感状态与视觉观察和行为一起显式建模，使世界模型能够同时预测未来状态和情感转变。实验结果表明，LEWM能更准确地预测情感驱动的社会行为，同时在基本任务上保持与通用世界模型相当的性能。

---

### 31 Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions

**link**: https://arxiv.org/pdf/2512.24679.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: 智能故障诊断对机械可靠性至关重要，但现有方法在未知工况下性能显著下降，且大多依赖单模态传感信号，忽视了多模态信息的互补性。为此，本文提出一种具有双解耦的多模态跨域混合融合模型。该模型开发了双解耦框架，用于分离模态不变与模态特定特征以及领域不变与领域特定表示，实现全面的多模态表示学习和鲁棒的领域泛化。设计了跨域混合融合策略，通过跨域随机混合模态信息来增强模态和领域多样性，并引入三模态融合机制自适应整合多模态异质信息。在感应电机故障诊断实验中（包括未知恒定和时变工况），该方法持续优于先进方法，消融研究进一步验证了各组件及多模态融合的有效性。

---

### 32 Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences

**link**: https://arxiv.org/pdf/2512.24829.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: 家庭物体重排机器人系统常依赖从人类演示中推断的潜在偏好模型，这些模型虽预测效果较好，但解释性有限。本文提出沿四个可解释构念明确表述物体排列偏好：空间实用性（物品放置于最适合空间）、习惯便利性（常用物品易获取）、语义连贯性（任务相关物品放在一起）和常识适当性（放置于预期位置）。通过63名参与者的在线研究验证了这些构念的心理独特性和解释力，并将其集成到蒙特卡洛树搜索（MCTS）规划器中，生成的排列与参与者的排列高度一致。该工作提供了紧凑、可解释的物体排列偏好表述，并展示了其在机器人规划中的应用。

---

### 33 GenZ: Foundational models as latent variable generators within traditional statistical models

**link**: https://arxiv.org/pdf/2512.24834.pdf
**date**: 2026-01-01
**keywords**: cs.AI
**abs**: GenZ通过可解释语义特征将基础模型与传统统计模型桥接。大型语言模型虽具备广泛领域知识，但难以捕捉预测任务关键的数据集特定模式。该方法通过迭代对比统计建模误差识别的项目组来发现语义特征描述，而非仅依赖基础模型领域知识。将其表述为广义EM算法，联合优化语义特征描述符和统计模型参数。提示冻结的基础模型基于发现的特征对项目分类，将这些判断视为潜在二元特征的噪声观测，通过学习的统计关系预测实值目标。在房价预测（特征回归）和电影推荐冷启动协同过滤任务中验证：房价预测使用多模态数据发现的语义特征实现12%中位相对误差，显著优于GPT-5（38%误差）；Netflix电影嵌入仅通过语义描述预测协同过滤表示，余弦相似度达0.59，相当于传统方法约4000用户评分的效果，且发现的特征揭示了数据集特定模式。