### 1 ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning

**link**: https://arxiv.org/pdf/2601.03093.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出自适应测试时潜在空间引导（ATLAS），通过外部轻量级潜在验证器在推理时动态控制引导决策，以增强大型语言模型（LLMs）的推理能力。验证器预测推理质量并自适应调整引导强度，实现每个示例和步骤的最小开销调整。实验表明，ATLAS在多个数学推理基准上优于常规解码和固定引导基线，减少令牌使用的同时提高准确性，为高效推理提供可扩展机制。所有源代码将公开。

---

### 2 RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse

**link**: https://arxiv.org/pdf/2601.02790.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出RadioDiff-Flux，一种两阶段潜在扩散框架，用于高效构建无线电地图（RM）。该方法将静态环境建模与动态细化解耦，重用预计算的潜在中点以绕过冗余去噪，显著减少推理时间。实验显示，RadioDiff-Flux在保持保真度的同时实现高达50倍加速，精度损失小于0.15%，适用于未来6G网络的快速、可扩展RM生成。

---

### 3 Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning

**link**: https://arxiv.org/pdf/2601.03032.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出因果流形公平性（CMF）框架，桥接因果推断与几何深度学习，学习对敏感属性反事实干预时局部黎曼几何不变的潜在表示。通过对解码器雅可比矩阵和海森矩阵施加约束，CMF确保潜在空间距离和形状在不同人口统计群体中一致。在合成结构因果模型上验证，CMF有效解耦敏感几何扭曲，保留任务效用，并提供公平性-效用权衡的量化指标。

---

### 4 Threat Detection in Social Media Networks Using Machine Learning Based Network Analysis

**link**: https://arxiv.org/pdf/2601.02581.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出基于机器学习的威胁检测框架，用于社交媒体网络中的恶意行为分类。该框架利用网络流量数据集，通过预处理和探索性数据分析解决数据不平衡问题，并构建人工神经网络模型捕捉复杂非线性趋势。实验在准确率、召回率等指标上表现优异，表明该方法能有效识别大规模网络威胁，补充现有入侵检测系统。

---

### 5 Credit Assignment via Neural Manifold Noise Correlation

**link**: https://arxiv.org/pdf/2601.02636.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出神经流形噪声相关（NMNC），通过限制在神经流形上的扰动进行信用分配。理论和实证表明，训练网络中雅可比行空间与神经流形对齐，流形维度随网络规模缓慢增长。NMNC在卷积网络和循环网络上优于传统噪声相关，产生更接近灵长类视觉系统的表示，为生物信用分配提供机制假设。

---

### 6 Image, Word and Thought: A More Challenging Language Task for the Iterated Learning Model

**link**: https://arxiv.org/pdf/2601.02911.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本文扩展迭代学习模型至七段数码管图像任务，智能体成功学习并传递具有表达性、组合性和稳定性的语言。模型结合监督与无监督学习，在自编码器架构中探索更大意义-信号空间，实现对所有128个字形使用不同代码、信号组件映射意义组件、语言世代间不变。

---

### 7 SimpleMem: Efficient Lifelong Memory for LLM Agents

**link**: https://arxiv.org/pdf/2601.02553.pdf  
**date**: 2026-01-07  
**keywords**: cs.AI  
**abs**: 本文提出SimpleMem，一种基于语义无损压缩的高效记忆框架，用于LLM智能体的长期交互。框架包括语义结构化压缩、递归记忆整合和自适应查询感知检索三阶段，最大化信息密度和token利用率。实验显示，SimpleMem在准确性、检索效率和推理成本上优于基线，平均F1提升26.4%，token消耗减少高达30倍。

---

### 8 Physical Transformer

**link**: https://arxiv.org/pdf/2601.02433.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出物理Transformer，将Transformer计算与几何表示和物理动力学结合。微观层建模注意力头和前馈块为相互作用自旋；中观层通过神经微分流形演化状态；宏观层维护语义工作空间和信息相位图。在数值积分和动力系统上，该方法优于基线，为物理AI提供新路径。

---

### 9 Polynomial Convergence of Riemannian Diffusion Models

**link**: https://arxiv.org/pdf/2601.02499.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文研究黎曼扩散模型，在L2准确分数估计下证明多项式小步长足以保证小采样误差，无需数据分布光滑性或正定性。分析基于流形曲率假设，利用热核对数梯度的Li-Yau估计和扰动热方程参数化展开，为非欧几里得空间扩散模型提供精确分析框架。

---

### 10 hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures

**link**: https://arxiv.org/pdf/2601.02509.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文介绍hdlib 2.0，扩展向量符号架构（VSA）的机器学习能力，包括改进监督分类模型、新增回归模型、聚类模型和基于图的学习模型，并首次实现量子超维计算。该库支持量子驱动算术运算和量子机器学习监督学习模型，增强VSA的信息处理能力。

---

### 11 LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection

**link**: https://arxiv.org/pdf/2601.02511.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出统一框架，整合基于LLM的势函数、变分自编码器增强的动态奖励缩放和带标签传播的主动学习，用于时间序列异常检测。基于LSTM的强化学习智能体利用LLM衍生语义奖励指导探索。在Yahoo-A1和SMD基准上评估，该方法在有限标注预算下实现最先进精度，有效应对数据稀疏环境。

---

### 12 Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion

**link**: https://arxiv.org/pdf/2601.03213.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出强化学习框架，用于文本到图像扩散模型中的机器遗忘，将去噪视为序贯决策过程，并引入时间步感知评论家与噪声步骤奖励。通过训练基于CLIP的奖励预测器，计算策略梯度更新优势估计。实验显示，该方法在多个概念上实现优于或相当的遗忘效果，保持图像质量和提示保真度。

---

### 13 PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters

**link**: https://arxiv.org/pdf/2601.03237.pdf  
**date**: 2026-01-07  
**keywords**: cs.LG  
**abs**: 本文提出PET-TURTLE，通过幂律先验泛化成本函数处理不平衡数据分布，并在标签过程引入稀疏logits优化搜索空间。实验表明，该方法提高不平衡数据源准确性，防止少数聚类过度预测，增强整体聚类效果，适用于潜空间表示应用。

---

### 14 Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models

**link**: https://arxiv.org/pdf/2601.02580.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本研究通过微调大型语言模型模拟学生反应，隐式建模项目参数如难度和区分度。利用Qwen-3模型和低秩适应，生成合成项目特征曲线以估计IRT参数。在六年级英语语言艺术和BEA 2024数据集上评估，该方法与基线相当或更优，特别在建模项目区分度方面有效。

---

### 15 TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs

**link**: https://arxiv.org/pdf/2510.06747.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本文提出TWIST，一种无需训练和无标签的短文本聚类方法，基于迭代向量更新：根据代表性文本构建稀疏向量，通过LLM指导优化。实验表明，该方法在不同数据集和较小LLM上实现与对比学习相当或更优结果，可扩展至大型数据集，降低计算成本，适用于现实场景。

---

### 16 TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents

**link**: https://arxiv.org/pdf/2601.02845.pdf  
**date**: 2026-01-07  
**keywords**: Latent Memory  
**abs**: 本文提出TiMem，一种时间层次记忆框架，通过时间记忆树组织对话，实现从原始观察到角色表征的系统性整合。核心特性包括时间层次组织、语义引导整合和复杂度感知召回。实验在LoCoMo和LongMemEval-S基准上达到最先进准确率，召回记忆长度减少52.20%，流形分析显示角色分离清晰。

---

### 17 RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems

**link**: https://arxiv.org/pdf/2601.02917.pdf  
**date**: 2026-01-07  
**keywords**: Latent Reasoning  
**abs**: 本文介绍检索增强学习匹配（RAL2M），将LLMs重新定位为检索系统中的匹配判断器，消除生成幻觉。通过查询自适应潜在集成策略建模异构模型能力，得出校准共识决策。实验表明，该方法显著优于基线，有效利用“群体智慧”确保合规响应。

---

### 18 Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders

**link**: https://arxiv.org/pdf/2601.02978.pdf  
**date**: 2026-01-07  
**keywords**: Latent Space  
**abs**: 本文提出基于稀疏自编码器的框架，用于检索和引导与高级语言行为相关的语义特征。方法采用对比特征检索管道，结合统计激活分析和生成验证。以大五人格特质为案例，证明能精确双向引导模型行为，性能优于传统激活引导，产生功能忠实性效应。

---

### 19 Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners

**link**: https://arxiv.org/pdf/2601.02996.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本文系统研究大型推理模型（LRMs）在11种语言中的多语言潜在推理。通过截断策略测量逐步潜在预测形成，揭示多语言潜在推理证据：资源丰富语言较强，低资源语言较弱。表征分析显示预测内部演变跨语言高度一致，但以英语为中心，表明潜在推理路径不平衡。

---

### 20 MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory

**link**: https://arxiv.org/pdf/2601.03192.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本文提出MemRL框架，通过对情景记忆进行非参数强化学习，使智能体实现自我进化。框架采用两阶段检索机制，基于语义相关性和学习Q值选择记忆，效用通过环境反馈优化。在多个基准上实验，MemRL显著优于基线，调和稳定性-可塑性困境，实现持续运行时改进。

---

### 21 Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration

**link**: https://arxiv.org/pdf/2601.02906.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本文表明，文字在线性地编码在多语言语音模型激活空间中，修改激活能直接控制输出文字。在测试时添加文字向量可诱导文字变化，应用于语音识别输出文字控制。该方法在所有Whisper模型尺寸上表现出竞争力，支持非常规语言-文字配对。

---

### 22 Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy

**link**: https://arxiv.org/pdf/2601.02989.pdf  
**date**: 2026-01-07  
**keywords**: cs.CL  
**abs**: 本文提出受System-2启发的测试时策略，将大型计数任务分解为小型独立子问题。机制分析识别关键组件：潜在计数在项目表示中计算，通过注意力头转移，并在最后阶段聚合。实验表明，该策略使LLMs突破架构限制，在大规模计数上实现高精度，提供System-2计数机制洞察。