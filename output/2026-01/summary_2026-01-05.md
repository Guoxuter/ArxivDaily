### 1 Memory Bank Compression for Continual Adaptation of Large Language Models

**link**: https://arxiv.org/pdf/2601.00756.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLMs）已成为许多日常应用的支柱，但其知识易过时。持续学习旨在用新信息更新LLMs而不覆盖旧知识。全微调方法成本高且易导致灾难性遗忘。记忆增强方法通过外部记忆库存储信息，但记忆库会随数据流增长而膨胀。本文提出MBC模型，采用码本优化策略压缩记忆库，引入在线重置机制防止码本崩溃，并在注意力层使用键值低秩适应高效利用压缩表示。实验表明，MBC将记忆库大小减少到0.3%，同时保持高保留准确率。

---

### 2 BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics

**link**: https://arxiv.org/pdf/2601.00366.pdf  
**date**: 2026-01-05  
**keywords**: cs.CL  
**abs**: 联合嵌入预测架构（JEPA）是一种自监督训练技术。本文提出BERT-JEPA（BEPA），在BERT风格模型中添加JEPA训练目标，解决[CLS]嵌入空间坍缩问题，将其转化为语言无关空间。新结构提升了跨多语言基准测试的性能。

---

### 3 ECR: Manifold-Guided Semantic Cues for Compact Language Models

**link**: https://arxiv.org/pdf/2601.00543.pdf  
**date**: 2026-01-05  
**keywords**: cs.CL  
**abs**: 紧凑型模型易丢失嵌入空间结构，尤其在模型容量有限或多语言数据下。结构坍塌使下游任务难以构建。现有压缩方法仅对齐表面输出，未保留底层流形结构，导致语义偏移。本文提出ECR方法，通过流形引导语义线索解决此问题。

---

### 4 Fast-weight Product Key Memory

**link**: https://arxiv.org/pdf/2601.00671.pdf  
**date**: 2026-01-05  
**keywords**: cs.CL  
**abs**: 序列建模层面临存储容量与计算效率的权衡。本文提出Fast-weight Product Key Memory（FwPKM），将静态Product Key Memory（PKM）转换为动态“fast-weight”情景记忆。FwPKM通过局部块级梯度下降动态更新参数，支持快速记忆和检索输入序列中的新键值对。实验显示，FwPKM作为情景记忆补充标准模块，显著降低长上下文数据集困惑度，并泛化到128K token上下文。

---

### 5 Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models

**link**: https://arxiv.org/pdf/2601.00003.pdf  
**date**: 2026-01-05  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLMs）通过检索语义相似信息或提升推理能力增强性能，但有效整合两者仍是挑战。本文提出推理感知知识检索方法，使LLMs获取与对话逻辑对齐的信息。采用粗到细策略：先识别知识库中上下文相关子区域，再精炼搜索提取推理相关知识。实验表明，该方法提升检索多样性和响应创造性。

---

### 6 The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition

**link**: https://arxiv.org/pdf/2601.00065.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 开源LLM生态系统依赖模型组合技术（如权重合并和词汇扩展），但分词器移植存在供应链漏洞。通过设计“破坏者token”，在donor模型中惰性，移植到base模型后可重构为高显著性恶意特征。攻击利用系数复用几何特性，创建非对称可实现性缺口，破坏base模型生成同时逃避检测。实验证明攻击对微调与权重合并持久，揭示模块化AI组合风险。

---

### 7 Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI

**link**: https://arxiv.org/pdf/2601.00516.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 自主LLM智能体生成的多步行动计划可能因上下文错位或结构不连贯失败。现有异常检测方法不适合此挑战。本文提出Trajectory Guard，一种暹罗循环自编码器，通过混合损失函数联合学习任务轨迹对齐和序列有效性。在合成扰动和真实世界基准测试中，F1分数达0.88-0.94，推理延迟仅32毫秒，比基线快17-27倍。

---

### 8 GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation

**link**: https://arxiv.org/pdf/2601.00231.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 参数高效微调（PEFT）适配大型语言模型，但现有方法忽略局部损失曲率。本文提出GRIT，一种动态曲率感知LoRA方法，包含三项改进：K-FAC预处理梯度、Fisher引导重投影抑制漂移、谱自适应调整有效秩。在LLaMA模型基准测试中，GRIT减少46%参数，性能匹配或超越LoRA和QLoRA。

---

### 9 FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing

**link**: https://arxiv.org/pdf/2601.00785.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 联邦数据共享需处理非IID客户端异质性和梯度泄漏。本文提出FedHypeVAE，基于超网络驱动差分隐私框架，合成嵌入级数据。使用条件变分自编码器（VAE），以共享超网络生成客户端感知解码器，在差分隐私下优化。局部MMD对齐和Lipschitz正则化增强非IID稳定性，支持领域无关合成。

---

### 10 Unknown Aware AI-Generated Content Attribution

**link**: https://arxiv.org/pdf/2601.00218.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 生成模型来源归因日益重要。本文利用CLIP特征和线性分类器建立目标生成器归因基线，但难以泛化到未见生成器。提出约束优化方法，整合未标记野生数据（含真实图像和未知生成器输出），鼓励野生样本分类为非目标。实验表明，野生数据显著提升对未见过生成器的归因性能。

---

### 11 Imitation from Observations with Trajectory-Level Generative Embeddings

**link**: https://arxiv.org/pdf/2601.00452.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 针对离线观察模仿学习（LfO）中专家演示稀缺和离线数据与专家行为差异问题，提出轨迹级生成嵌入（TGE）方法。在时间扩散模型潜空间中估计专家状态密度，构建密集平滑替代奖励。TGE捕捉长时程动态，弥合分布差距。实验在D4RL基准中优于现有离线LfO方法。

---

### 12 Bayesian Inverse Games with High-Dimensional Multi-Modal Observations

**link**: https://arxiv.org/pdf/2601.00696.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 多智能体交互建模为非合作博弈，但部署需明确所有智能体目标。本文提出近似贝叶斯推理方法用于逆博弈，整合多模态观测数据，生成隐藏目标贝叶斯后验样本。训练结构化变分自编码器嵌入可微纳什博弈求解器。实验表明，框架学习先验和后验分布，优于最大似然估计，实现更安全决策。

---

### 13 The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving

**link**: https://arxiv.org/pdf/2601.00747.pdf  
**date**: 2026-01-05  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLM）管道依赖自举推理循环，优化正确性但易导致推理路径分布坍缩，削弱创造性。引入分布创造性推理（DCR）变分目标，将训练视为轨迹概率测度梯度流。分析多样性衰减定理，提出防止坍缩的设计。DCR为LLM提供兼顾正确性和创造性的原则性方案。