### 1 GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction

**link**: https://arxiv.org/pdf/2601.04550.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 本文提出了一种新颖的图增强时空层次推理网络（GEnSHIN），用于处理交通流预测中的复杂时空依赖关系。该模型集成了三项创新设计：1）注意力增强的图卷积循环单元（GCRU），通过引入Transformer模块增强长期时间依赖建模能力；2）非对称双嵌入图生成机制，利用真实道路网络和数据驱动的潜在非对称拓扑生成更符合实际交通流特征的图结构；3）动态记忆库模块，利用可学习的交通模式原型为每个传感器节点提供个性化交通模式表示，并在解码阶段引入轻量级图更新器以适应路网状态的动态变化。在公开数据集METR-LA上的实验表明，GEnSHIN在平均绝对误差（MAE）、均方根误差（RMSE）和平均绝对百分比误差（MAPE）等多个指标上达到或超越对比模型，尤其在早晚交通高峰时段表现出优异的预测稳定性。消融实验进一步验证了各核心模块的有效性及其对最终性能的贡献。

---

### 2 Meta-probabilistic Modeling

**link**: https://arxiv.org/pdf/2601.04462.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 虽然概率图模型可以发现数据中的潜在结构，但其有效性取决于选择规范良好的模型。在实践中识别此类模型具有挑战性，通常需要通过反复试验进行迭代检查和修订。为此，我们提出元概率建模（MPM），这是一种元学习算法，可直接从多个相关数据集中学习生成模型结构。MPM采用分层架构，其中全局模型规范在数据集之间共享，而局部参数保持特定于数据集。对于学习和推理，我们提出了一种易于处理的VAE启发式代理目标，并通过双层优化对其进行优化：局部变量通过坐标上升进行解析更新，而全局参数则使用基于梯度的方法进行训练。我们在以对象为中心的图像建模和顺序文本建模上评估了MPM，证明它能使生成模型适应数据，同时恢复有意义的潜在表示。

---

### 3 When Models Manipulate Manifolds: The Geometry of a Counting Task

**link**: https://arxiv.org/pdf/2601.04480.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 语言模型尽管只接收标记序列，却能感知文本的视觉属性——我们从机制上研究Claude 3.5 Haiku如何完成这样一项任务：固定宽度文本中的换行。我们发现字符计数通过稀疏特征族离散化的低维弯曲流形来表示，类似于生物的位置细胞。准确的预测源于一系列几何变换：标记长度被累积到字符计数流形中，注意力头扭曲这些流形以估计到行边界的距离，而换行决策通过将估计值正交排列以创建线性决策边界来实现。我们通过因果干预验证了我们的发现，并发现了视觉错觉——即劫持计数机制的字符序列。我们的工作展示了早期层丰富的感官处理、注意力算法的复杂性，以及结合基于特征和几何视角进行可解释性研究的重要性。

---

### 4 Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony

**link**: https://arxiv.org/pdf/2601.04592.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 经典循环神经网络(RNN)将音乐上下文总结为确定性隐藏状态向量，存在信息瓶颈，无法捕捉音乐中固有的模糊性。本文提出密度矩阵RNN(DM-RNN)，这是一种利用密度矩阵的新型理论架构。该架构允许模型维持音乐解释的统计集合(混合状态)，同时捕捉经典概率和量子相干性。论文使用量子通道(CPTP映射)严格定义时间动态，并基于Choi-Jamiolkowski同构详细阐述参数化策略，确保学习到的动态本质上保持物理有效性(CPTP)。引入冯·诺依曼熵量化音乐不确定性，以及量子互信息(QMI)测量声部间的纠缠。DM-RNN为建模复杂、模糊的音乐结构提供了数学严谨的框架。

---

### 5 Do LLMs Benefit from User and Item Embeddings in Recommendation Tasks?

**link**: https://arxiv.org/pdf/2601.04690.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 大型语言模型(LLMs)作为推荐系统展现出潜力，通过生成式方法建模用户偏好。然而现有方法常仅依赖文本语义，或有限地整合协同信号(通常仅使用用户或物品嵌入)。这些方法难以处理表示用户历史的多个物品嵌入，往往退回到文本语义而忽略更丰富的协同信息。本文提出一种简单有效的解决方案：通过独立的轻量级投影模块，将协同过滤学习到的用户和物品嵌入投影到LLM的token空间。微调后的LLM结合这些投影嵌入与文本token生成推荐。初步结果表明，该设计有效利用结构化用户-物品交互数据，优于纯文本LLM基线，为传统推荐系统与现代LLMs的桥接提供实用路径。

---

### 6 Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer

**link**: https://arxiv.org/pdf/2601.04263.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 知识蒸馏通过将知识从大型教师网络转移到小型学生网络，在模型压缩方面已被证明有效。当前时间序列的知识蒸馏主要基于为计算机视觉任务开发的logit和特征对齐技术，这些方法未明确考虑时间数据，存在两个关键不足：一是由于logit和特征的不可解释性，转移的知识如何帮助学生模型学习过程仍不清楚；二是这些方法仅转移有限知识，主要复制教师的预测准确性，导致学生模型的预测分布常与教师有显著差异，阻碍其安全替代教师模型。本文提出通过扩展传统logit转移来传递可解释知识，不仅传达正确预测，还传达教师的正确推理。具体而言，从教师logit中提取称为时间显著性的有用知识，捕捉每个输入时间步对教师预测的重要性。通过时间显著性蒸馏训练学生，鼓励其基于与教师相同的输入特征进行预测。该方法无需额外参数或特定架构假设，实验表明其有效提高基线方法性能，同时实现超出预测准确性的理想特性，为时间序列分析中的可解释知识蒸馏建立新范式。

---

### 7 MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification

**link**: https://arxiv.org/pdf/2601.04264.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 深度学习模型（尤其是循环神经网络及其变体如长短期记忆网络）显著推进了时间序列数据分析，能捕捉复杂序列模式实现实时评估。但高计算复杂度和大模型尺寸对资源受限环境（如可穿戴设备和边缘计算平台）的部署构成挑战。知识蒸馏（KD）通过将知识从大型复杂教师模型转移到小型高效学生模型，在保持高性能的同时降低计算需求。当前为计算机视觉任务设计的KD方法忽视了时间序列模型独特的时间依赖性和记忆保留特性。为此，本文提出记忆差异知识蒸馏（MemKD）框架，利用专门损失函数捕捉教师和学生模型在时间序列数据中子序列上的记忆保留差异，确保学生模型有效模仿教师模型行为。该方法有助于开发紧凑、高性能的循环神经网络，适用于实时时间序列分析任务。大量实验表明，MemKD显著优于最先进的KD方法，将参数大小和内存使用减少约500倍，同时保持与教师模型相当的性能。

---

### 8 Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime

**link**: https://arxiv.org/pdf/2601.04270.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 深度学习优化表现出未被最坏情况梯度边界捕捉的结构。经验上，训练轨迹上的梯度通常在时间上可预测，并且在低维子空间内演化。本文通过一个可测量的框架将这一观察形式化，用于可预测梯度流形。

---

### 9 Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes

**link**: https://arxiv.org/pdf/2601.04299.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 1型糖尿病（T1D）是一种代谢高度异质性的疾病，无法通过糖化血红蛋白（HbA1c）等传统生物标志物充分表征。本研究提出了一种可解释的深度学习框架，该框架整合连续血糖监测（CGM）数据与实验室检查结果，以学习个体代谢状态的多模态时间嵌入。通过Transformer编码器对跨模态的时间依赖性进行建模，并通过高斯混合模型识别潜在代谢表型。模型的可解释性通过Transformer注意力可视化和基于SHAP的特征归因实现。在577名T1D患者中，识别出五种潜在代谢表型，范围从代谢稳定到心脏代谢风险升高。这些表型表现出不同的生化特征，包括血糖控制、脂质代谢、肾脏标志物和促甲状腺激素（TSH）水平的差异。注意力分析强调血糖变异性是主要的时间因素，而SHAP分析确定HbA1c、甘油三酯、胆固醇、肌酐和TSH是表型分化的关键贡献因素。表型归属与高血压、心肌梗死和心力衰竭显示出统计学显著但适度的关联。总体而言，这种可解释的多模态时间嵌入框架揭示了T1D中生理上一致的代谢亚群，并支持超越单一生物标志物的风险分层。

---

### 10 Causally-Aware Information Bottleneck for Domain Adaptation

**link**: https://arxiv.org/pdf/2601.04361.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 我们处理因果系统中的一种常见领域适应场景。在该场景中，目标变量在源域中可观测，但在目标域中完全缺失。我们旨在通过各种偏移下的其余观测变量来推断目标域中的目标变量。我们将此问题构建为学习一种紧凑、机制稳定的表示，该表示保留与预测目标相关的信息，同时丢弃虚假变异。对于线性高斯因果模型，我们推导出闭式高斯信息瓶颈（GIB）解决方案。该解决方案简化为典型相关分析（CCA）风格的投影，并在需要时提供有向无环图（DAG）感知选项。对于非线性或非高斯数据，我们引入变分信息瓶颈（VIB）编码器-预测器。该方法可扩展到高维数据，能够在源域数据上训练并零样本部署到目标域。在合成和真实数据集上，我们的方法始终实现准确的推断，支持在高维因果模型中的实际应用，并提供了一个统一、轻量级的因果领域适应工具包。

---

### 11 Neural-Symbolic Integration with Evolvable Policies

**link**: https://arxiv.org/pdf/2601.04799.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 神经符号（NeSy）人工智能旨在结合神经网络的学习能力与符号系统的可解释推理，但现有框架通常需要预定义符号策略或可微策略，限制了其在缺乏领域知识或策略非可微时的适用性。本文提出一种通过进化过程同时学习非可微符号策略和神经网络权重的框架，将NeSy系统视为种群中的有机体，通过突变（符号规则添加和神经权重变化）和基于适应度的选择收敛于隐藏目标策略。该框架扩展NEUROLOG架构使符号策略可训练，将Valiant的可进化性框架适应NeSy上下文，并采用Machine Coaching语义实现可变符号表示。神经网络通过符号组件的溯因推理训练，无需可微性。实验表明，从空策略和随机神经权重开始的NeSy系统能成功逼近隐藏的非可微目标策略，median正确率接近100%。

---

### 12 Robust Reasoning as a Symmetry-Protected Topological Phase

**link**: https://arxiv.org/pdf/2601.05240.pdf
**date**: 2026-01-09
**keywords**: cs.LG
**abs**: 大型语言模型存在“幻觉”问题——即由语义噪声引发的逻辑不一致。本文提出，当前架构处于“度量相”，其中因果顺序易受自发对称性破缺的影响。在此，我们将鲁棒推理识别为一种有效的对称保护拓扑相，其中逻辑运算在形式上同构于非阿贝尔任意子编织，用鲁棒的拓扑不变量取代了脆弱的几何插值。实验上，我们展示了显著的拓扑相变：当Transformer和RNN表现出无能隙衰减时，我们的完整网络（Holonomic Network）呈现宏观“质量能隙”，在临界噪声阈值以下保持不变的保真度。此外，在代表符号操作的S₁₀（3.6×10⁶个状态）的变量绑定任务中，我们展示了完整泛化能力：拓扑模型在训练范围外（L=50→5000）外推100倍时仍保持完美保真度，与理论上无限的因果视界一致，而Transformer则失去逻辑一致性。消融研究表明，这种保护严格源于非阿贝尔规范对称性。这为逻辑推理提供了新的普适类证据，将因果稳定性与语义流形的拓扑结构联系起来。

---

### 13 Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data

**link**: https://arxiv.org/pdf/2601.05227.pdf
**date**: 2026-01-09
**keywords**: stat.ML
**abs**: 本文提出了一种将随机微分方程（SDEs）与深度生成模型相结合的新框架，以改进涉及结构化和时间数据的机器学习应用中的不确定性量化。这种称为随机潜在微分推理（SLDI）的方法，将Itô SDE嵌入变分自编码器的潜在空间中，能够灵活地进行连续时间的不确定性建模，同时保持严谨的数学基础。SDE的漂移项和扩散项由神经网络参数化，实现了数据驱动的推理，并将经典时间序列模型推广到处理不规则采样和复杂动态结构。

---

### 14 Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions

**link**: https://arxiv.org/pdf/2601.04465.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 本文提出了概念标记（Concept Tokens），这是一种轻量级方法，通过在预训练大型语言模型中添加新的特殊标记，并仅从目标概念的多个自然语言定义中学习其嵌入（将概念的出现替换为新标记）。大型语言模型保持冻结状态，嵌入通过标准语言建模目标进行优化。作者在三个设置中评估了概念标记：首先，在HotpotQA的闭卷问答中研究幻觉现象，发现否定幻觉标记主要通过增加弃权来减少幻觉答案，而肯定则增加幻觉并降低精度；其次，引入了第二语言教学中的重铸教学反馈策略，观察到相同的方向效应，并且与上下文提供完整定义语料相比，概念标记更好地保持了对其他指令的遵从性（例如询问后续问题）；最后，通过埃菲尔铁塔和虚构的“澳大利亚塔”进行定性研究，以说明学习到的嵌入捕获的信息及其局限性。总体而言，概念标记提供了一种从定义中学习的紧凑控制信号，可以引导冻结大型语言模型的行为。

---

### 15 Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data

**link**: https://arxiv.org/pdf/2601.04518.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 深度学习的进步极大地提高了有监督图像分类的性能。然而，标记数据成本高昂，促使研究转向对比学习等无监督学习方法。在现实场景中，完全无标记的数据集很少见，因此半监督学习（SSL）在少量标记数据与大量未标记数据共存的情况下具有高度相关性。一种著名的半监督对比学习方法涉及为未标记数据分配伪标签。本研究旨在通过整合标记和未标记特征嵌入之间的分布匹配来增强基于伪标签的半监督学习，以提高多个数据集的图像分类准确性。

---

### 16 A General Neural Backbone for Mixed-Integer Linear Optimization via Dual Attention

**link**: https://arxiv.org/pdf/2601.04509.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 混合整数线性规划（MILP）作为组合优化的广泛使用建模框架，在许多科学和工程应用中至关重要，但在大规模问题上仍面临计算挑战。近年来深度学习通过将MILP实例表示为变量-约束二分图并应用图神经网络（GNN）来提取潜在结构模式，以提高求解器效率。然而，这种架构受限于局部导向机制，导致表示能力受限，阻碍了MILP的神经方法发展。本文提出一种注意力驱动的神经架构，超越纯图视图学习富有表现力的表示。设计了双注意力机制，对变量和约束执行并行自注意力和交叉注意力，实现全局信息交换和更深层次的表示学习。将此通用骨干网络应用于实例级、元素级和求解状态级的各种下游任务。在广泛使用的基准测试上的大量实验表明，该方法持续优于最先进的基线，突显了基于注意力的神经架构作为学习增强混合整数线性优化的强大基础。

---

### 17 Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment

**link**: https://arxiv.org/pdf/2601.04571.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 近年来，多模态检索已成为一个前景广阔但具有挑战性的研究方向。大多数现有多模态检索研究侧重于捕捉多模态数据中与其配对文本相似的信息，却常常忽略了多模态数据中包含的互补信息。本研究提出了CIEA，一种新颖的多模态检索方法，该方法采用互补信息提取与对齐技术，将文档中的文本和图像转换到一个统一的latent space（潜在空间）中，并设计了一个互补信息提取器，用于识别和保留图像表示中的差异。我们使用两种互补的对比损失来优化CIEA，以确保语义完整性并有效捕捉图像中包含的互补信息。大量实验证明了CIEA的有效性，其性能显著优于分治模型和通用密集检索模型。我们还提供了消融研究、进一步讨论和案例研究，以突出CIEA所取得的进展。为促进社区的进一步研究，我们已发布源代码。

---

### 18 Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning

**link**: https://arxiv.org/pdf/2601.04695.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 我们提出了Tape，一个受控的强化学习基准，旨在隔离潜在规则下的分布外（OOD）失败。

---

### 19 Differential syntactic and semantic encoding in LLMs

**link**: https://arxiv.org/pdf/2601.04765.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 我们研究了句法和语义信息如何在大型语言模型（LLMs）的内层表示中编码，重点关注超大型模型DeepSeek-V3。我们发现，通过平均共享句法结构或意义的句子的隐藏表示向量，可以获得捕获表示中大量句法和语义信息的向量。特别是，从句子向量中减去这些句法和语义“质心”会显著影响它们与句法和语义匹配句子的相似性，这表明句法和语义至少部分是线性编码的。我们还发现，句法和语义的跨层编码模式不同，并且这两种信号在一定程度上可以解耦，表明LLM表示中这两种语言信息的差异化编码。

---

### 20 Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics

**link**: https://arxiv.org/pdf/2601.04854.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 自回归语言模型通常定义在离散令牌序列上，在每个生成步骤都确定一个特定令牌。这种早期离散化迫使通过令牌级采样来解决不确定性，这往往导致不稳定性、重复以及对解码启发式的敏感性。

---

### 21 CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters

**link**: https://arxiv.org/pdf/2601.04885.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 随着大型语言模型（LLMs）服务于全球受众，对齐必须从强制普遍共识转向尊重文化多元性。我们证明，当密集模型被迫拟合冲突的价值分布时，会遭受“均值崩溃”，收敛到无法代表不同群体的通用平均值。我们将此归因于“文化稀疏性”，即梯度干扰阻止密集参数跨越不同的文化模式。为解决此问题，我们提出了CuMA（Cultural Mixture of Adapters）框架，将对齐构建为条件容量分离问题。通过整合人口统计感知路由，CuMA内化潜在文化拓扑结构，将冲突梯度明确分解到专门的专家子空间中。在WorldValuesBench、Community Alignment和PRISM上的广泛评估表明，CuMA实现了最先进的性能，显著优于密集基线和仅语义的MoE。关键的是，我们的分析证实CuMA有效缓解了均值崩溃，保留了文化多样性。我们的代码可在...

---

### 22 Learning Latent Action World Models In The Wild

**link**: https://arxiv.org/pdf/2601.05230.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 能够在现实世界中进行推理和规划的智能体需要具备预测其行为后果的能力。虽然世界模型具备这种能力，但它们通常需要动作标签，而大规模获取动作标签可能较为复杂。这促使了潜在动作模型的学习，即仅从视频中学习动作空间。本文致力于在真实场景视频上学习潜在动作世界模型，扩展了现有专注于简单机器人模拟、视频游戏或操作数据的研究范围。尽管这使得我们能够捕捉更丰富的动作，但也带来了视频多样性引发的挑战，例如环境噪声或视频间缺乏共同的具身性。为解决部分挑战，我们讨论了动作应遵循的属性以及相关的架构选择和评估方法。我们发现，连续但受约束的潜在动作能够捕捉真实场景视频中动作的复杂性，而常见的向量量化方法则无法做到这一点。例如，我们发现来自智能体（如人类进入房间）的环境变化可以在视频间传递。这突显了学习特定于真实场景视频动作的能力。在视频间缺乏共同具身性的情况下，我们主要能够学习在空间上相对于相机定位的潜在动作。尽管如此，我们能够训练一个控制器，将已知动作映射到潜在动作，从而允许将潜在动作用作通用接口，并使用我们的世界模型解决规划任务，性能与动作条件基线相当。我们的分析和实验为将潜在动作模型扩展到现实世界迈出了一步。

---

### 23 Reinforced Efficient Reasoning via Semantically Diverse Exploration

**link**: https://arxiv.org/pdf/2601.05053.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 强化学习与可验证奖励（RLVR）已被证明能有效增强大型语言模型（LLMs）的推理能力。基于蒙特卡洛树搜索（MCTS）的扩展方法通过提供基于树的推理展开，实现了细粒度和段级别的信用分配，从而改进了 vanilla RLVR（如 GRPO）。然而，现有方法仍存在探索多样性有限和推理效率低下的问题。为解决这些挑战，本文提出了一种通过语义多样性探索实现的强化高效推理框架（ROSE）。为鼓励更丰富的推理探索，该方法融合了基于语义熵的分支策略和ε-探索机制：前者利用已采样的推理展开捕捉语义不确定性，并选择具有高语义分歧的分支点来生成新的后续推理路径；后者则从根节点随机启动推理展开，防止搜索过程过度局部化。为提高效率，本文设计了一种长度感知的段级优势估计器，奖励简洁且正确的推理，同时惩罚不必要的长推理链。在多个数学推理基准上使用 Qwen 和 Llama 模型进行的大量实验验证了 ROSE 的有效性和效率。

---

### 24 Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis

**link**: https://arxiv.org/pdf/2601.04709.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 现代云基础设施中的根因分析需要对异构数据源有深入理解，尤其是包含核心故障特征的时间序列性能指标。虽然大型语言模型在文本推理方面表现出卓越能力，但其基于离散token的架构与具有时间依赖性的连续数值序列存在根本不兼容性。当前方法未能充分解决这种模态不匹配问题，限制了语言模型驱动的自动化在事件管理工作流中的潜力。本文提出了一种多模态诊断框架，将时间序列表示与预训练语言模型嵌入空间相协调。我们的方法有三项技术进展：（1）一种语义压缩技术，将时间片段提炼为单token抽象，同时保留模式语义；（2）一种对齐编码器，利用门控交叉注意力将时间序列特征投影到语言模型潜在空间；（3）一种检索增强诊断管道，将对齐的嵌入与历史事件知识合成，实现专家级故障归因。在六个云系统基准上的综合评估表明，我们的框架实现了领先性能，诊断准确率达到48.75%，在涉及复合故障模式的场景中表现出显著改进。结果验证了嵌入空间对齐是使语言模型能够在生产事件响应环境中对多模态遥测数据进行推理的有效策略。

---

### 25 Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning

**link**: https://arxiv.org/pdf/2601.04726.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 大型语言模型（LLMs）越来越多地被部署为能够推理、规划和与环境交互的智能体。为了有效扩展到长时程场景，此类智能体的关键能力是具备能够保留、组织和检索过去经验以支持下游决策的记忆机制。然而，大多数现有方法以扁平方式组织和存储记忆，并依赖简单的基于相似性的检索技术。即使引入了结构化记忆，现有方法也往往难以明确捕捉经验或记忆单元之间的逻辑关系。此外，记忆访问在很大程度上与构建的结构脱节，仍然依赖浅层语义检索，阻碍了智能体对长时程依赖进行逻辑推理。在这项工作中，我们提出了CompassMem，一种受事件分割理论启发的以事件为中心的记忆框架。CompassMem通过将经验增量分割为事件并通过显式逻辑关系将它们链接起来，将记忆组织为事件图。该图作为逻辑地图，使智能体能够在记忆上进行结构化和目标导向的导航，超越表面检索，逐步收集有价值的记忆以支持长时程推理。在LoCoMo和NarrativeQA上的实验表明，CompassMem在多个骨干模型上持续提高了检索和推理性能。

---

### 26 Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models

**link**: https://arxiv.org/pdf/2601.04731.pdf
**date**: 2026-01-09
**keywords**: cs.AI
**abs**: 当前用于大型推理模型的无评论家强化学习方法在正向同质提示（所有滚动轨迹均正确）上训练时效率极低，由于优势估计为零而导致滚动轨迹浪费。我们引入了一种极其简单但功能强大的解决方案——挖掘内在掌握度（Miner），该方案将策略的内在不确定性重新用作自监督奖励信号，无需外部监督、辅助模型或额外推理成本。我们的方法开创了两项关键创新：（1）令牌级焦点信用分配机制，动态放大关键不确定令牌的梯度，同时抑制过度自信的令牌；（2）自适应优势校准，无缝整合内在奖励和可验证奖励。在Qwen3-4B和Qwen3-8B基础模型上的六个推理基准上进行评估，Miner在其他四种算法中实现了最先进的性能，与GRPO相比，Pass@1绝对增益高达4.58，Pass@K绝对增益高达6.66。与其他旨在增强探索的方法相比，进一步揭示了这两项新提出的创新的优越性。这表明，挖掘内在不确定性对于推理模型的高效和可扩展RL训练既是必要的也是充分的。

---

### 27 ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG

**link**: https://arxiv.org/pdf/2601.05038.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 检索增强生成（RAG）有助于大型语言模型保持准确性，但将长文档输入提示会使模型变慢且昂贵。这促使了上下文压缩的发展，包括令牌修剪、摘要和基于嵌入的压缩。虽然研究人员尝试将这些文档“压缩”为更小的摘要或数学嵌入，但存在一个问题：数据压缩得越多，大型语言模型就越难理解。为解决这一挑战，我们提出ArcAligner（自适应递归上下文对齐器），这是一个集成到语言模型层中的轻量级模块，帮助模型更好地利用高度压缩的上下文表示进行下游生成。它使用自适应“门控”系统，仅在信息复杂时才增加额外处理能力，保持系统高效。在知识密集型问答基准测试中，ArcAligner在可比压缩率下持续优于压缩基线，尤其在多跳和长尾设置中表现突出。源代码已公开。

---

### 28 ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models

**link**: https://arxiv.org/pdf/2601.04394.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 本文认为大型语言模型（LLMs）中的事实性和安全性问题源于其潜在激活空间（latent activation space）中的表征错位，而非完全独立的对齐问题。作者提出ARREST框架，通过一个外部网络识别并纠正潜在空间中漂移的特征，在不微调模型参数的情况下将虚假内容转为真实、不安全输出转为安全输出，并支持软硬拒绝及事实修正。实证结果表明，ARREST不仅能有效调节错位，还因对抗训练在生成软拒绝方面比RLHF对齐模型更具通用性。

---

### 29 Learning to Simulate Human Dialogue

**link**: https://arxiv.org/pdf/2601.04436.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 本文通过下一轮对话预测研究人类思维建模，比较不同学习方法。发现优化法官评分奖励会降低真实人类对话的似然性和类人胜率，而最大化观察到的人类响应的对数概率能提升性能。将思维链（chain-of-thought）视为潜在变量（latent variable），推导对数概率的下界，优化此目标在所有评估中效果最佳，表明基于真实人类对话的分布匹配目标下，思考过程有助于提升模型对人类行为的理解。

---

### 30 LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal

**link**: https://arxiv.org/pdf/2601.04768.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 多语言环境下的密集检索通常在混合语言集合中进行搜索，但多语言嵌入会将语言身份与语义一起编码。这种语言信号可能会夸大同语言对的相似度，并排挤其他语言编写的相关证据。我们提出了LANGSAE EDITING，这是一种在池化嵌入上训练的事后稀疏自编码器，能够直接在向量空间中可控地去除语言身份信号。该方法利用跨语言激活统计识别与语言相关的潜在单元（latent units），在推理时抑制这些单元，并在原始维度上重建嵌入，使其无需重新训练基础编码器或重新编码原始文本即可与现有向量数据库兼容。多语言实验表明，该方法在排序质量和跨语言覆盖度方面持续改进，尤其对文字系统不同的语言效果显著。

---

### 31 Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems

**link**: https://arxiv.org/pdf/2601.05171.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 现有长期个性化对话系统难以协调无界交互流与有限上下文约束，常受内存噪声累积、推理退化和角色一致性问题困扰。为此，本文提出Inside Out框架，该框架利用全局维护的PersonaTree作为长期用户画像的载体。通过用初始 schema 约束主干并更新分支和叶子，PersonaTree实现可控增长，在保持一致性的同时实现内存压缩。此外，我们通过基于过程奖励的强化学习训练轻量级MemListener，以生成结构化、可执行且可解释的{ADD, UPDATE, DELETE, NO_OP}操作，从而支持个性化树的动态演化。在响应生成阶段，PersonaTree可直接用于低延迟场景以增强输出；当用户需要更多细节时，触发智能体模式在PersonaTree约束下按需引入细节。实验表明，PersonaTree在抑制上下文噪声和保持角色一致性方面优于全文拼接和各种个性化内存系统。值得注意的是，小型MemListener模型的内存操作决策性能可与甚至超越DeepSeek-R1-0528和Gemini-3-Pro等强大推理模型。

---

### 32 A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction

**link**: https://arxiv.org/pdf/2601.04960.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 本文提出了一个用于情绪智能的统一口语语言模型，通过一种称为注入情绪归因思维（IEAT）的新型数据构建策略进行增强。IEAT将用户的情绪状态及其潜在原因整合到模型的内部推理过程中，使情绪感知推理内化为模型自身能力，而非作为显式监督。该模型采用两阶段渐进式训练策略：第一阶段通过自蒸馏执行语音-文本对齐和情绪属性建模，第二阶段进行端到端跨模态联合优化，以确保文本和口语情绪表达的一致性。在类人对话系统挑战（HumDial）情绪智能基准测试中，所提方法在情绪轨迹建模、情绪推理和共情响应生成方面均取得了顶级性能，无论是基于LLM的评估还是人类评估。

---

### 33 SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment

**link**: https://arxiv.org/pdf/2601.05075.pdf
**date**: 2026-01-09
**keywords**: cs.CL
**abs**: 传统的句子嵌入方法在非生成式预训练模型上采用token级对比学习。近年来，出现了基于生成式大型语言模型（LLMs）的嵌入方法。这些方法要么依赖固定的提示模板，要么涉及模型架构的修改。前者缺乏对模型的进一步优化，导致性能有限；后者改变了模型的内部计算机制，从而损害了其生成能力。本文提出SemPA，一种通过语义偏好对齐提升句子表示同时保留LLMs生成能力的新方法。我们利用句子级直接偏好优化（DPO）在释义生成任务上高效优化LLMs，使模型学会区分语义等效的句子，同时保留固有的生成能力。理论上，我们在Plackett-Luce模型框架下建立了DPO与对比学习之间的形式化联系。实验上，在语义文本相似性任务和各种LLM基准测试中，SemPA在不牺牲LLMs固有生成能力的情况下实现了更好的语义表示。