### 1 ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference
**link**: https://arxiv.org/pdf/2601.21109.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出ChunkWise LoRA，一种动态自适应方法，基于令牌复杂度将序列分割为可变长度块，并为每块分配定制低秩配置。系统引入运行时调度器估计令牌难度、执行自适应分块，并使用秩梯机制选择每块的LoRA秩和缩放。为保持输出一致性，引入边界安全组合模块，并整合基于策略的KV缓存策略。在Wikitext-103和SQuAD等基准上的实验表明，该方法比基线LoRA降低延迟34%、减少内存38%，同时保持或提升BLEU、EM和困惑度等指标。框架与现有Transformer架构兼容，为参数高效LLM部署提供实用方案。

---

### 2 Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space
**link**: https://arxiv.org/pdf/2601.21169.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文引入输出空间搜索（OS-Search），将LLM生成转化为端点搜索。外循环在冻结编码器定义的3D输出空间Z中选择目标z*，基于检索的策略（通过序列级强化学习训练）生成输出，其坐标在标准自回归解码下落在z*附近。这支持在Z空间中进行并行扫描和黑盒优化，无需依赖令牌/程序搜索。在故事生成任务上，扫描Z空间（文本）比提示链方法产生3.1倍更高的LLM评分多样性；在代码生成任务上，对Z空间（代码）进行贝叶斯优化，在匹配推理预算下改进控制器未知目标，同时保持有效性。

---

### 3 Scaling Embeddings Outperforms Scaling Experts in Language Models
**link**: https://arxiv.org/pdf/2601.21204.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文探索嵌入扩展作为稀疏扩展的有效维度，通过综合分析和实验，确定嵌入扩展优于专家扩展的帕累托前沿机制。研究系统表征了关键架构因素，包括参数预算以及与模型宽度和深度的相互作用。通过整合定制系统优化和推测解码，将稀疏性转化为实际推理加速。基于此，提出LongCat-Flash-Lite模型（68.5B参数，约3B激活参数），在嵌入上分配超过30B参数，超越参数相当的MoE基线，并在智能体和编码领域表现出竞争力。

---

### 4 Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models
**link**: https://arxiv.org/pdf/2601.21214.pdf
**date**: 2026-01-30
**keywords**: latent reasoning
**abs**: 本文通过多领域任务研究推理跳数泛化问题，发现错误集中在少数关键错误类型的标记位置，而非均匀分布。分析表明，这些错误源于内部竞争机制：某些注意力头（错误处理头）通过放大错误推理轨迹并抑制正确轨迹打破平衡。移除单个错误处理头可恢复正确预测。基于此，提出推理的测试时校正方法，通过动态识别和停用错误处理头，在不同任务和LLM上一致提高推理跳数泛化能力。

---

### 5 CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding
**link**: https://arxiv.org/pdf/2601.21262.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文提出自回归生成方法CausalEmbed，用于构建多向量嵌入。通过在对比训练中引入迭代边际损失，促使嵌入模型学习紧凑且结构良好的表示。该方法仅使用数十个视觉标记即可实现高效视觉文档检索任务，在各种骨干模型和基准测试中实现30-155倍的标记数量减少，同时保持高度竞争力性能。理论分析和实证结果证明自回归嵌入生成在训练效率和测试时可扩展性方面的优势，为多向量表示引入灵活测试时扩展策略。

---

### 6 TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning
**link**: https://arxiv.org/pdf/2601.21711.pdf
**date**: 2026-01-30
**keywords**: latent reasoning
**abs**: 本文提出TACLer框架，通过多阶段强化学习训练基于模型熟练度逐步增加数据复杂度。框架包含定制课程学习（确定模型缺乏的知识）和混合思考/不思考推理范式（平衡准确性和效率）。实验表明，TACLer降低计算成本，训练计算量比长思考模型减少50%以上，推理token使用量减少42%以上；在基础模型上提高9%以上准确性，在四个数学数据集上持续优于最先进基线。

---

### 7 Enhancing Language Models for Robust Greenwashing Detection
**link**: https://arxiv.org/pdf/2601.21722.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文提出参数高效框架，通过结合对比学习和序数排序目标结构化LLM潜在空间，以捕捉具体行动和模糊声明之间的等级差异。方法整合门控特征调制过滤披露噪声，并利用MetaGradNorm稳定多目标优化。跨类别设置实验表明，该方法比标准基线具有更优鲁棒性，同时揭示了表征刚性和泛化之间的权衡。

---

### 8 Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention
**link**: https://arxiv.org/pdf/2601.21768.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文介绍Zonkey分层扩散模型，通过从原始字符到文档级表示的全可训练管道解决固定分词器限制。核心是可微分分词器，学习概率性序列开始决策，实现无需显式监督形成语言学意义分割。可微性由概率注意力机制实现，结合位置特定存在概率模拟软掩码。序列通过概率衰减支持变长输出。分层级别将序列压缩为更高层次抽象，通过去噪扩散混合模型在潜在空间去噪。Stitcher确保跨段重叠不变性。在Wikipedia上训练后，Zonkey能从噪声生成连贯变长文本，展示新兴层次结构。

---

### 9 Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation
**link**: https://arxiv.org/pdf/2601.21797.pdf
**date**: 2026-01-30
**keywords**: LLM Memory
**abs**: 本文提出对抗性记忆适应机制（AMA），通过模拟任务执行使记忆构建和更新与任务目标对齐。具体过程：挑战代理基于原始对话生成问答对；使用构建记忆回答问题；评估代理评估响应并错误分析；适应代理分析错误案例并执行双级更新。通过此过程，记忆系统在离线阶段接收任务感知监督信号，增强对下游任务适应性。AMA可集成到现有记忆系统中，在长对话基准LoCoMo上实验证明其有效性。

---

### 10 LMK > CLS: Landmark Pooling for Dense Embeddings
**link**: https://arxiv.org/pdf/2601.21525.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文引入Landmark (LMK)池化，将序列划分为块，在块间插入landmark令牌，并通过对landmark令牌嵌入平均池化形成最终表示。该机制在不牺牲局部特征下改善长上下文外推能力，代价是少量特殊令牌。实验证明，LMK池化在短上下文检索任务上与现有方法相当，在长上下文任务上显著改进，成为现有池化方法的实用替代方案。

---

### 11 inversedMixup: Data Augmentation via Inverting Mixed Embeddings
**link**: https://arxiv.org/pdf/2601.21543.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出inversedMixup框架，结合Mixup可控性和基于LLM生成可解释性。采用三阶段训练过程，将任务模型输出嵌入空间与LLM输入嵌入空间对齐。对齐后，inversedMixup可将混合嵌入重建为人类可解释增强句子。此外，首次提供文本Mixup流形入侵现象实证证据，并引入简单缓解策略。实验证明方法在少样本和全监督场景下的有效性和通用性。

---

### 12 ILRR: Inference-Time Steering Method for Masked Diffusion Language Models
**link**: https://arxiv.org/pdf/2601.21647.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文针对掩码扩散语言模型提出迭代潜在表示优化（ILRR）方法，通过动态对齐生成序列与参考序列内部激活实现推理时引导。方法能捕获高层语义属性并支持灵活属性控制（如情感调节），扩展空间调制引导机制可利用短参考序列引导长文本生成。实验表明，ILRR在LLaDA和MDLM架构上以微小计算开销实现10%-60%属性准确率提升，同时保持高生成质量。

---

### 13 FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning
**link**: https://arxiv.org/pdf/2601.21682.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出FIT框架，通过数据过滤、重要性感知更新和目标层归因机制实现稳定持续遗忘。方法在处理大量删除请求时平衡遗忘效果与效用保留，并构建PCH基准及Forget Degree与Retain Utility评估指标。实验表明FIT在四个开源LLM上优于现有方法，且能抵抗再学习和量化恢复攻击。

---

### 14 Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling
**link**: https://arxiv.org/pdf/2601.21684.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出回收搜索经验（RSE）策略，解决测试时扩展中LLM推理计算冗余问题。通过将搜索轨迹提炼为共享经验库，实现正向回收（复用中间结论）和负向回收（剪枝失败模式），将孤立搜索过程转化为累积学习过程。理论分析证明其效率优势，实验在HMMT24、HMMT25、IMO-Bench和HLE数据集上表明，RSE以相当计算成本持续优于强基线，实现最先进扩展效率。

---

### 15 Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents
**link**: https://arxiv.org/pdf/2601.21699.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出DAVID-GRPO框架，通过最小监督稳定早期学习、基于证据召回分配检索信用、重采样近失轨迹改善探索。在仅使用四个RTX 3090 GPU训练的1.5B参数模型上，在六个多跳QA基准上超越为大规模设置设计的现有RL方法。结果表明，通过适当归纳偏置，小型智能体可在低训练成本下实现高推理准确率。

---

### 16 A Separable Architecture for Continuous Token Representation in Language Models
**link**: https://arxiv.org/pdf/2601.22040.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出Leviathan架构，使用连续嵌入生成器替代传统离散查找表。在Pile数据集上，Leviathan在等参数设置下优于标准LLaMA风格架构。通过经验幂律拟合，Leviathan表现出显著优越有效参数容量，在所研究范围内相当于1.47至2.11倍更多参数的密集模型。

---

### 17 VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning
**link**: https://arxiv.org/pdf/2601.22069.pdf
**date**: 2026-01-30
**keywords**: cs.CL
**abs**: 本文提出VTC-R1，将视觉-文本压缩集成到推理过程中的新高效推理范式。方法不处理冗长文本轨迹，而是将中间推理段渲染为紧凑图像，作为“光学内存”迭代反馈到视觉语言模型中。基于OpenR1-Math-220K构建训练数据集，实现3.4倍令牌压缩，并微调代表性视觉语言模型。在MATH500、AIME25、AMC23和GPQA-DIAMOND等基准上，VTC-R1持续优于标准长上下文推理，同时显著提高推理效率，实现2.7倍端到端延迟加速。

---

### 18 Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking
**link**: https://arxiv.org/pdf/2601.21619.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文形式化并量化过扩展诅咒，展示其普遍性和严重性，并分析触发机制。提出轻量级方法T2来打破过扩展诅咒，利用潜在表示在解码前估计每个样本最优并行度。实验表明，T2在保持相当性能同时显著降低成本，实现更高效并行思维。

---

### 19 Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities
**link**: https://arxiv.org/pdf/2601.21702.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文考虑表示误导（RM）类LLM遗忘方法，通过线性表示假设视角重新审视RM。假设除遗忘外，机器遗忘能引发与高层概念对应可控副行为和更强副能力。假设在广泛任务中得到实证验证，包括行为控制（如控制未学习模型真实性、情感）和能力增强（如提高上下文学习能力）。研究结果表明，此现象若滥用可能是风险，也可用于开发更强能力模型。

---

### 20 Embodied Task Planning via Graph-Informed Action Generation with Large Language Model
**link**: https://arxiv.org/pdf/2601.21841.pdf
**date**: 2026-01-30
**keywords**: cs.CL,LLM Memory,latent space
**abs**: 本文提出GiG框架，使用图中图架构构建具身智能体记忆。方法采用图神经网络将环境状态编码为嵌入，在经验记忆库中组织为动作连接执行轨迹图。通过对图嵌入聚类，框架能够检索结构感知先验，使智能体基于相关结构模式决策。此外，引入有界前瞻模块，利用符号转换逻辑通过接地动作投影增强规划能力。在三个具身规划基准上评估，GiF优于最先进基线，计算成本相当或更低。

---

### 21 SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching
**link**: https://arxiv.org/pdf/2601.21927.pdf
**date**: 2026-01-30
**keywords**: cs.CL,LLM Memory
**abs**: 本文提出SONIC框架，将历史片段压缩为紧凑且语义丰富Nexus令牌。通过集成动态预算训练，SONIC灵活适应不同内存约束。实验表明，在80%和50%压缩率下，SONIC在四个多轮基准上持续优于H2O和StreamingLLM等基线。在MTBench101基准上，SONIC相比最先进基线平均得分提升35.55%，同时提高部署效率，整体推理加速50.1%。

---

### 22 Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts
**link**: https://arxiv.org/pdf/2601.22156.pdf
**date**: 2026-01-30
**keywords**: cs.CL,LLM Memory
**abs**: 本文提出HALO流水线，将Transformer模型蒸馏为RNN-注意力混合模型，并设计HypeNet架构，通过新颖位置编码方案和架构修改实现优异长度泛化。将Qwen3系列转换为HypeNet后，在保持相当性能同时实现更优长上下文性能和效率，转换仅需2.3B tokens。

---

### 23 Do Reasoning Models Enhance Embedding Models?
**link**: https://arxiv.org/pdf/2601.21192.pdf
**date**: 2026-01-30
**keywords**: cs.AI,latent space
**abs**: 本文评估经强化学习与可验证奖励训练的推理模型作为嵌入初始化时是否提升语义表示。评估发现零效应：相同训练流程下，RLVR调优骨干与基础模型嵌入性能无一致优势。通过分层表示相似性分析揭示，RLVR引起潜流形局部几何重组和坐标基漂移，但保留全局流形几何和线性读出，导致后续对比学习使两类模型强对齐。

---

### 24 Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization
**link**: https://arxiv.org/pdf/2601.21358.pdf
**date**: 2026-01-30
**keywords**: cs.AI,latent reasoning
**abs**: 本文提出PLaT框架，将潜推理重构为规划，通过解耦推理与语言化，将推理建模为潜规划状态确定性轨迹，解码器在必要时将其转化为文本。解耦使模型动态决定推理终止时机。数学基准实验显示，PLaT贪婪准确率低于基线，但推理多样性可扩展性更优，表明学习更广泛解空间，为推理时搜索提供基础。

---

### 25 System 1&2 Synergy via Dynamic Model Interpolation
**link**: https://arxiv.org/pdf/2601.21414.pdf
**date**: 2026-01-30
**keywords**: latent reasoning,cs.AI
**abs**: 本文将重点转向能力控制，通过动态参数插值利用现有Instruct和Thinking检查点，无需额外训练。初步研究表明，线性插值产生凸的、单调帕累托前沿。基于此，提出DAMI框架，估计特定查询推理强度λ(q)以配置认知深度。对于训练估计，开发编码准确性和效率标准偏好学习方法；对于零样本部署，引入利用模型间认知差异基于置信度方法。实验表明，DAMI比Thinking模型实现更高准确性，同时保持高效。

---

### 26 From Consistency to Complementarity: Aligned and Disentangled Multi-modal Learning for Time Series Understanding and Reasoning
**link**: https://arxiv.org/pdf/2601.21436.pdf
**date**: 2026-01-30
**keywords**: latent space,cs.LG
**abs**: 本文提出MADI多模态LLM，特点包括（1）补丁级对齐，强制异质模态间物理基础细粒度对应；（2）离散分离交互，将模态公共语义分离为紧凑离散潜在变量，自适应协同净化模态独特信息；（3）关键令牌突出，强调信息丰富、查询相关信号以实现稳健推理。在合成和现实世界基准上实验表明，MADI始终优于通用LLM和时间序列专用MLLM。

---

### 27 The Path of Least Resistance: Guiding LLM Reasoning Trajectories with Prefix Consensus
**link**: https://arxiv.org/pdf/2601.21494.pdf
**date**: 2026-01-30
**keywords**: latent reasoning,cs.AI
**abs**: 本文引入PoLR方法，利用前缀一致性实现计算高效推理。PoLR对推理轨迹短前缀聚类，识别主导聚类，并扩展该聚类中所有路径，保留自一致性准确性优势，同时大幅减少令牌使用和延迟。通过互信息和熵构建理论分析解释早期推理步骤编码强信号。实证上，PoLR在多个基准上匹配或超过自一致性，减少高达60%令牌使用和50%延迟，并与自适应推理方法互补。

---

### 28 The Effectiveness of Style Vectors for Steering Large Language Models: A Human Evaluation
**link**: https://arxiv.org/pdf/2601.21505.pdf
**date**: 2026-01-30
**keywords**: latent space,cs.AI
**abs**: 本研究首次对LLM输出情感基调进行激活转向人类评估，收集190名参与者超过7,000个评分。发现人类和模型质量评分间强一致性（平均r=0.776），表明自动评分可替代感知质量。中等转向强度（λ≈0.15）可靠放大目标情感，对厌恶和恐惧影响最强，惊讶影响最小。从Alpaca升级到LlaMA-3产生更一致转向效果，评分者间信度高（ICC=0.71-0.87）。

---

### 29 ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory
**link**: https://arxiv.org/pdf/2601.21545.pdf
**date**: 2026-01-30
**keywords**: LLM Memory,cs.AI
**abs**: 本文提出ShardMemo分层内存服务，具有A级每智能体工作状态、B级分片证据和C级版本化技能库。B级实施路由前范围：结构化资格约束在路由或ANN搜索前屏蔽不合格分片。将分片探测转换为对合格分片屏蔽混合专家路由，通过Top-Bprobe或自适应Top-P探测最多Bprobe个分片，并使用成本感知选通；路由器通过证据到分片监督训练。在LoCoMo上，ShardMemo在问题类别上比最强基线提高+5.11至+6.82 F1。

---

### 30 Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves
**link**: https://arxiv.org/pdf/2601.21582.pdf
**date**: 2026-01-30
**keywords**: latent reasoning,cs.AI
**abs**: 本文引入深度循环注意力混合（Dreamer）框架，结合序列注意力、深度注意力和稀疏专家注意力。通过沿深度注意力缓解隐藏大小瓶颈，解耦缩放维度。在语言推理基准上，模型需比匹配基线少2到8倍训练令牌达到相同准确性，相同训练令牌下优于约2倍大模型。提供跨深度知识使用见解，如显示比SOTA MoE大2到11倍专家选择多样性。

---

### 31 Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement
**link**: https://arxiv.org/pdf/2601.21113.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文介绍自我改进、可选缓存Planner-Auditor框架，通过将生成与确定性验证及目标重放解耦，提高临床出院规划安全性和可靠性。

---

### 32 Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving
**link**: https://arxiv.org/pdf/2601.21164.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出训练MLLM解释器生成图形几何描述，并利用现成LLM推理。选择条件声明语言作为几何描述，MLLM解释器通过链式思维增强监督微调和GRPO微调以生成CDL。设计CDL匹配奖励促进有效GRPO训练。构建新数据集Formalgeo7k-Rec-CoT。实验表明，方法在多个基准上优于领先开源和闭源MLLM。

---

### 33 Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation
**link**: https://arxiv.org/pdf/2601.21335.pdf
**date**: 2026-01-30
**keywords**: cs.AI,cs.IR
**abs**: 本文提出因果神经符号推理模型（CNRE），将用户行为链中内生逻辑用于显式推理，并通过因果推断解决观察性多行为数据中混淆变量问题。CNRE通过层次偏好传播捕获异质跨行为依赖，基于偏好强度建模内生逻辑规则，自适应调度神经逻辑推理路径，生成可解释因果中介变量。实验表明，CNRE显著优于基线模型，并提供多层次可解释性。

---

### 34 EHR-RAG: Bridging Long-Horizon Structured Electronic Health Records and Large Language Models via Enhanced Retrieval-Augmented Generation
**link**: https://arxiv.org/pdf/2601.21340.pdf
**date**: 2026-01-30
**keywords**: cs.AI,cs.CL
**abs**: 本文提出EHR-RAG框架，解决长程结构化EHR数据与LLM结合时上下文限制和时间依赖丢失问题。框架包含事件与时间感知混合EHR检索、自适应迭代检索和双路径证据检索与推理。在四个长程EHR预测任务上实验表明，EHR-RAG持续优于最强LLM基线，平均Macro-F1提升10.76%。

---

### 35 Exploring Reasoning Reward Model for Agents
**link**: https://arxiv.org/pdf/2601.22154.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文引入智能体推理奖励模型（Agent-RRM），为智能体轨迹生成结构化反馈，包括显式推理轨迹、聚焦性批评和整体评估分数。利用这些信号，研究三种整合策略：Reagent-C、Reagent-R和Reagent-U。在12个基准上评估表明，Reagent-U实现显著性能提升，验证推理奖励模型和训练方案有效性。

---

### 36 Discovering Hidden Gems in Model Repositories
**link**: https://arxiv.org/pdf/2601.22157.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文研究公共代码库中模型集中问题，通过对2000多个模型评估发现“隐藏瑰宝”——不受欢迎但性能显著优于流行模型的微调版本。将模型发现表述为多臂老虎机问题，通过共享查询集和激进淘汰策略加速序贯减半搜索算法。该方法仅需每个候选模型50次查询检索顶级模型，提高发现速度50多倍。

---

### 37 QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation
**link**: https://arxiv.org/pdf/2601.21049.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出QUARK框架，用于非忠实查询下鲁棒检索。QUARK通过恢复假设显式建模查询不确定性，并引入查询锚定聚合以稳健组合信号。原始查询作为语义锚点，恢复假设提供受控辅助证据。在受控模拟和BEIR基准上实验表明，QUARK优于基础检索器Recall、MRR和nDCG。

---

### 38 Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report
**link**: https://arxiv.org/pdf/2601.21051.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出Foundation-Sec-8B-Reasoning，首个开源网络安全原生推理模型。模型基于Foundation-Sec-8B基础模型，通过监督微调和可验证奖励强化学习两阶段训练构建。训练利用涵盖网络安全分析、指令遵循和数学推理专有数据。评估表明，模型在网络安全任务上性能与显著更大模型竞争力，同时保持强大通用能力。

---

### 39 Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models
**link**: https://arxiv.org/pdf/2601.21183.pdf
**date**: 2026-01-30
**keywords**: latent reasoning
**abs**: 本文引入“谄媚锚点”——能因果地将模型锁定在用户同意状态的句子。通过分析反事实展开，表明可可靠检测和量化这些锚点。线性探针以84.6%平衡准确率区分谄媚锚点，基于激活回归器可预测承诺强度（R²=0.74）。发现谄媚行为在推理过程中逐渐形成，为干预提供窗口。

---

### 40 Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification
**link**: https://arxiv.org/pdf/2601.21210.pdf
**date**: 2026-01-30
**keywords**: latent reasoning
**abs**: 本文提出DoVerifier符号验证器，使用do-演算和概率论规则检查LLM生成因果表达式是否可从给定因果图推导。在合成数据和因果QA基准上评估表明，DoVerifier更准确捕捉因果推理轨迹语义正确性。

---

### 41 MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning
**link**: https://arxiv.org/pdf/2601.21468.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出MemOCR多模态记忆智能体，通过视觉布局分配自适应信息密度内存空间。具体而言，MemOCR维护结构化富文本记忆并将其渲染为图像，在视觉上优先关键证据，压缩辅助细节。在预算感知目标下使用强化学习训练MemOCR，使智能体接触多样化压缩级别。在长上下文多跳和单跳问答基准上，MemOCR优于基于文本基线。

---

### 42 ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management
**link**: https://arxiv.org/pdf/2601.21473.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出ScaleSim系统，支持高效内存LLM服务。ScaleSim支持主动预取和基于优先级驱逐，通过模块化接口支持智能体特定内存。在仿真基准上实现比SGLang高达1.74倍加速。

---

### 43 Chain Of Thought Compression: A Theoritical Analysis
**link**: https://arxiv.org/pdf/2601.21576.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文首次对CoT压缩学习难度进行理论分析，通过引入Order-r交互证明高阶逻辑依赖学习信号指数衰减，导致不可约问题。引入NatBool-DAG基准强制不可约逻辑推理。提出ALiCoT框架，通过潜在标记分布与中间推理状态对齐克服信号衰减。实验表明，ALiCoT实现54.4倍加速，保持与显式CoT相当性能。

---

### 44 Beyond Imitation: Reinforcement Learning for Active Latent Planning
**link**: https://arxiv.org/pdf/2601.21598.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文强调在潜在标记表示空间上主动规划重要性，提出主动潜在规划方法（ATP-Latent）。方法将潜在标记监督过程建模为条件变分自编码器，结合辅助一致性奖励进行强化学习。实验表明，ATP-Latent在四个基准上相比先进基线实现+4.1%准确率和-3.3%令牌消耗。

---

### 45 RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems
**link**: https://arxiv.org/pdf/2601.21609.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出RecNet自进化偏好传播框架。在前向阶段，集中式偏好路由机制整合偏好更新并动态传播；在后向阶段，反馈驱动传播优化机制模拟多智能体强化学习框架，实现传播策略持续自进化。实验证明RecNet在为推荐系统建模偏好传播方面有效性。

---

### 46 The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR
**link**: https://arxiv.org/pdf/2601.22128.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文引入SMB-Structure结构化EHR世界模型，将联合嵌入预测架构与下token预测结合。SFT将模型锚定在token空间重建未来患者状态，JEPA仅从初始患者表示中在潜在空间预测未来。在MSK和INSPECT数据集上验证表明，SMB-Structure学习嵌入捕捉自回归基线无法恢复疾病动态。

---

### 47 LOCUS: Low-Dimensional Model Embeddings for Efficient Model Exploration, Comparison, and Selection
**link**: https://arxiv.org/pdf/2601.21082.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出LOCUS方法，通过注意力机制生成低维向量嵌入紧凑表示语言模型能力。方法无需重新训练整合新模型，训练正确性预测器实现路由精度。实验表明LOCUS所需查询评估样本比基线少4.8倍，学习嵌入空间具有几何意义。

---

### 48 Rethinking Refinement: Correcting Generative Bias without Noise Injection
**link**: https://arxiv.org/pdf/2601.21182.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出双阶段流优化（BFR）框架，通过潜在空间对齐和数据空间优化两阶段进行确定性修正。实验显示，BFR在保持多样性同时显著提升保真度和覆盖率，尤其在MNIST上仅需单次函数评估将FID从3.95降至1.46。

---

### 49 Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences
**link**: https://arxiv.org/pdf/2601.21246.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出基于峰值感知条件生成模型AI鉴别框架。框架通过峰值感知机制突出GC-MS数据特征峰，并将化学和溶剂信息编码到潜在向量中，使条件生成对抗网络生成符合实验条件合成信号。实验表明，方法在保持峰数量多样性同时减少误报。

---

### 50 E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory
**link**: https://arxiv.org/pdf/2601.21714.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出E-mem框架，从记忆预处理转向情景语境重构。采用异构层次结构，多个辅助智能体维护未压缩记忆语境，中央主智能体协调全局规划。辅助智能体在激活片段内进行局部推理，聚合前提取语境感知证据。在LoCoMo基准上，E-mem F1分数超过54%，令牌成本降低超70%。

---

### 51 Language-based Trial and Error Falls Behind in the Era of Experience
**link**: https://arxiv.org/pdf/2601.21754.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文提出SCOUT框架，将探索与利用解耦。采用轻量级“侦察器”探测环境动态，收集轨迹通过监督微调引导LLM，随后通过多轮强化学习激活潜在世界知识。实验表明，SCOUT使Qwen2.5-3B-Instruct模型达到0.86平均分数，节省约60% GPU小时消耗。

---

### 52 Textual Equilibrium Propagation for Deep Compound AI Systems
**link**: https://arxiv.org/pdf/2601.21064.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入文本平衡传播（TEP），包括自由阶段（局部LLM评论家迭代优化提示）和推动阶段（应用近端提示编辑）。在长程QA基准和多智能体工具使用数据集上，TEP持续优于TextGrad等全局传播方法。

---

### 53 Zenith: Scaling up Ranking Models for Billion-scale Livestreaming Recommendation
**link**: https://arxiv.org/pdf/2601.21285.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出Zenith排序架构，通过Token Fusion和Token Boost模块处理高维Prime Tokens。在TikTok Live上部署，A/B测试显示CTR AUC和Logloss分别达+1.05%/-1.10%，用户优质观看会话和时长分别增长+9.93%和+8.11%。

---

### 54 L2R: Low-Rank and Lipschitz-Controlled Routing for Mixture-of-Experts
**link**: https://arxiv.org/pdf/2601.21349.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文提出低秩与Lipschitz控制路由（L2R）框架，通过共享低秩潜在路由空间进行专家分配，并引入饱和内积评分显式控制路由函数Lipschitz行为。实验表明，L2R提升路由稳定性、专家专业化和整体模型性能。

---

### 55 Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning
**link**: https://arxiv.org/pdf/2601.21418.pdf
**date**: 2026-01-30
**keywords**: latent reasoning
**abs**: 本文提出难度感知策略优化（DiPO）框架，鼓励模型自发建模任务复杂度并整合到强化学习中。提出基于模型自推理难度建模方法，开发难度信号增强奖励函数。实验表明，DiPO使模型自发调整推理开销，显著减少冗余tokens。

---

### 56 SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation
**link**: https://arxiv.org/pdf/2601.21452.pdf
**date**: 2026-01-30
**keywords**: LLM Rec
**abs**: 本文旨在高效复用开源LLM架构，无需构建单独tokenization词汇表。研究发现OneRec梯度有界策略优化存在“对称保守性”问题。

---

### 57 HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing
**link**: https://arxiv.org/pdf/2601.21459.pdf
**date**: 2026-01-30
**keywords**: latent reasoning
**abs**: 本文提出HER框架，引入双层思维机制区分角色第一人称思考与LLM第三人称思考。通过逆向工程构建推理增强角色扮演数据，并构建人类对齐原则和奖励模型。实验验证方法有效性，模型显著优于基线。

---

### 58 Generative Modeling of Discrete Data Using Geometric Latent Subspaces
**link**: https://arxiv.org/pdf/2601.21831.pdf
**date**: 2026-01-30
**keywords**: stat.ML
**abs**: 本文介绍在分类分布乘积流形指数参数空间中使用潜在子空间作为学习离散数据生成模型工具。为参数域配备黎曼几何，使空间和距离通过等距映射关联，实现一致流匹配。实证结果表明，降低潜在维度足以表示生成建模数据。

---

### 59 Latent Adversarial Regularization for Offline Preference Optimization
**link**: https://arxiv.org/pdf/2601.22083.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文利用潜空间正则化进行语言模型偏好优化，提出GANPO，通过惩罚策略模型和参考模型内部表示差异实现潜空间正则化。将GANPO集成为现有离线偏好优化目标正则化器，实验表明潜空间正则化带来一致改进。

---

### 60 Latent-IMH: Efficient Bayesian Inference for Inverse Problems with Approximate Operators
**link**: https://arxiv.org/pdf/2601.20888.pdf
**date**: 2026-01-30
**keywords**: latent space
**abs**: 本文提出Latent-IMH方法，基于Metropolis-Hastings独立性采样器。方法首先使用近似算子生成中间潜变量，再利用精确算子优化。理论通过KL散度和混合时间界分析性能。数值实验表明，计算效率优于NUTS等方法。

---

### 61 Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening
**link**: https://arxiv.org/pdf/2601.21590.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出无需训练算法，能自回归锐化基础模型生成分布。在数学、问答和代码任务上评估表明，方法匹配或超越单样本GRPO性能，推理延迟比基于MCMC采样降低10倍以上。

---

### 62 Dynamics Reveals Structure: Challenging the Linear Propagation Assumption
**link**: https://arxiv.org/pdf/2601.21601.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文研究线性传播假设几何限制，采用关系代数研究否定、逆和组合操作。证明保证方向无关一阶传播需要张量分解；组合可简化为合取，任何在线性特征上定义良好合取必须是双线性。结果表明知识编辑失败等问题源于LPA固有结构限制。

---

### 63 Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification
**link**: https://arxiv.org/pdf/2601.21615.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出TTReFT框架，通过不确定性引导节点选择、低秩表示干预和干预感知掩码自编码器实现测试时表示微调。理论上建立分布外设置保证。实验表明TTReFT实现一致优越性能。

---

### 64 Gauge-invariant representation holonomy
**link**: https://arxiv.org/pdf/2601.21653.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入表示完整相关性，规范不变统计量测量路径依赖性。估计器通过全局白化固定规范，使用共享子空间和仅旋转普罗克拉斯提斯对齐邻域。证明对正交变换不变性，建立仿射层线性零假设。实证表明完整相关性随环路半径增加，能区分相似模型，并与鲁棒性相关。

---

### 65 XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision
**link**: https://arxiv.org/pdf/2601.21688.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文介绍XFACTORS方法，通过对比监督实现解纠缠信息瓶颈。

---

### 66 Curriculum Learning for LLM Pretraining: An Analysis of Learning Dynamics
**link**: https://arxiv.org/pdf/2601.21698.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文在三种课程下训练Pythia模型，并与随机排序比较。不同排序下训练遵循共享潜在阶段序列，课程主要改变阶段内数据曝光。较小模型中随机排序表现出更高梯度噪声和输出头谱饱和；更大规模下课程增益缩小。分析形式化难度pacing与优化稳定性联系。

---

### 67 Temporal Sepsis Modeling: a Fully Interpretable Relational Way
**link**: https://arxiv.org/pdf/2601.21747.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出机器学习框架：关系方法。电子病历时间数据以关系数据模式表示，应用命题化技术构建可解释特征。使用选择性朴素贝叶斯分类器分类。解释有单变量、全局、局部和反事实四个方面。

---

### 68 Effective LoRA Adapter Routing using Task Representations
**link**: https://arxiv.org/pdf/2601.21795.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出LORAUTER路由框架，使用任务表示而非适配器特征选择和组合LoRA适配器。LORAUTER通过任务嵌入路由查询，实现高效路由。实验表明LORAUTER优于基线路由方法，在未见任务上实现最先进结果。

---

### 69 Robust Multimodal Representation Learning in Healthcare
**link**: https://arxiv.org/pdf/2601.21941.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出双流特征去相关框架，通过潜在混杂因素引入结构因果分析处理偏差。方法采用带双流神经网络因果偏差去相关框架，利用广义交叉熵损失和互信息最小化去相关。实验证明一致性能改进。

---

### 70 Low-Rank Plus Sparse Matrix Transfer Learning under Growing Representations and Ambient Dimensions
**link**: https://arxiv.org/pdf/2601.21873.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文研究在环境维度和内在表示同时增长情况下，结构化矩阵估计迁移学习，其中一个估计良好源任务嵌入为更高维目标任务子空间。

---

### 71 Is Parameter Isolation Better for Prompt-Based Continual Learning?
**link**: https://arxiv.org/pdf/2601.20894.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出提示共享框架：构建全局提示池，引入任务感知门控路由机制稀疏激活提示子集，实现任务特定特征表示动态解耦与协同优化；引入历史感知调制器保护频繁使用提示免受过度更新。实验表明方法在有效性和效率上优于静态分配策略。

---

### 72 Clustering in Deep Stochastic Transformers
**link**: https://arxiv.org/pdf/2601.21942.pdf
**date**: 2026-01-30
**keywords**: stat.ML
**abs**: 本文分析带层归一化深度Transformer中token动态，考虑随机初始化内在随机性。在扩散缩放和token-wise RMS归一化下，证明当Transformer层数趋于无穷时，离散token动态收敛到球面上相互作用粒子系统。对于两个token，证明由相互作用强度和维度控制相变。

---

### 73 The Powers of Precision: Structure-Informed Detection in Complex Systems -- From Customer Churn to Seizure Onset
**link**: https://arxiv.org/pdf/2601.21170.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出机器学习方法用于涌现现象早期检测，从经验协方差或精度矩阵单参数估计族中学习最优特征表示。证明该估计族结构一致性，并在癫痫检测和客户流失预测上展示可靠性。

---

### 74 Missing-Data-Induced Phase Transitions in Spectral PLS for Multimodal Learning
**link**: https://arxiv.org/pdf/2601.21294.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文在比例高维尖峰模型中研究独立条目完全随机缺失掩蔽下PLS-SVD。经过适当归一化后，掩蔽互协方差表现为尖峰矩形随机矩阵，有效信号强度被√ρ衰减。PLS-SVD表现出BBP型相变：低于临界信噪比阈值时主导奇异向量渐近无信息；高于阈值时非平凡对齐潜在共享方向。

---

### 75 TRACE: Trajectory Recovery for Continuous Mechanism Evolution in Causal Representation Learning
**link**: https://arxiv.org/pdf/2601.21135.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文将过渡机制建模为有限原子机制凸组合，由时变混合系数控制。证明潜在因果变量和连续混合轨迹联合可识别性，提出TRACE框架，通过混合专家模型学习原子机制，测试时恢复机制轨迹。实验表明TRACE恢复混合轨迹相关性达0.99。

---

### 76 Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting
**link**: https://arxiv.org/pdf/2601.21151.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出PARADIS模型，通过功能分解将网络行为归纳偏置为平流、扩散和反应模块。平流通过神经半拉格朗日算子实现，基于球面上可微插值进行轨迹传输。PARADIS以较低训练成本达到SOTA预报精度。

---

### 77 Task-Awareness Improves LLM Generations and Uncertainty
**link**: https://arxiv.org/pdf/2601.21500.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文通过直接在任务相关潜在结构中建模LLM输出解决自然语言响应潜在结构问题。通过为结构配备dissimilarity度量，计算贝叶斯最优响应。这些响应通过在潜在空间中组合单个响应新合成。实验表明贝叶斯最优响应优于标准解码方法，并通过诱导贝叶斯风险量化不确定性。

---

### 78 Revisiting Diffusion Model Predictions Through Dimensionality
**link**: https://arxiv.org/pdf/2601.21419.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文基于广义预测公式提供理论框架，推导数据几何结构与最优预测目标解析关系，严谨证明环境维度显著超过数据内在维度时x-预测更优。提出k-Diff框架，通过数据驱动方法直接从数据学习最优预测参数k。实验表明k-Diff在不同架构和数据规模上优于固定目标基线。

---

### 79 ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation
**link**: https://arxiv.org/pdf/2601.21420.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出ConceptMoE，动态将语义相似token合并为概念表示，实现隐式token级计算分配。可学习分块模块通过测量token间相似度确定最优边界，序列按目标比率R压缩。MoE架构支持受控评估：重新分配节省计算资源匹配基线激活FLOPs和总参数。实验表明ConceptMoE在语言和视觉-语言任务上优于标准MoE，在持续训练中增益达+5.5分。此外，ConceptMoE将注意力计算减少高达R²倍，KV缓存减少R倍。

---

### 80 Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models
**link**: https://arxiv.org/pdf/2601.21794.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出知识向量弱化（KVW）方法，识别在遗忘集上模型输出生成过程被激活知识向量，并逐渐削弱其贡献。在MLLMU和CLEAR基准上实验表明，KVW实现稳定遗忘-保留权衡，显著提高计算效率。

---

### 81 LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics
**link**: https://arxiv.org/pdf/2601.21681.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出LLM4Fluid框架，通过增强物理信息解纠缠机制降阶建模，将高维流场压缩到紧凑潜在空间。预训练LLM作为时间处理器，利用时间序列提示自回归预测物理序列动态。提出模态对齐策略解决表示不匹配问题。实验表明LLM4Fluid作为稳健可泛化神经求解器，实现最先进精度。

---

### 82 Don't be so Stief! Learning KV Cache low-rank approximation over the Stiefel manifold
**link**: https://arxiv.org/pdf/2601.21686.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文研究与LLM内存管理相关键值缓存压缩问题。

---

### 83 IGETR: Hybrid Reasoning via Graph Structure and Knowledge-Guided Editing for Interpretable Temporal Knowledge Graph Reasoning
**link**: https://arxiv.org/pdf/2601.21978.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文针对TKGR任务提出IGETR混合推理框架，结合GNN结构化时序建模能力与LLM上下文理解能力。通过三阶段pipeline实现可解释推理：利用时序GNN识别可靠图证据；引入LLM引导路径编辑修正逻辑语义不一致；整合优化推理路径生成准确预测。实验验证方法有效性。

---

### 84 Investigation into using stochastic embedding representations for evaluating the trustworthiness of the Fréchet Inception Distance
**link**: https://arxiv.org/pdf/2601.21979.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文研究利用随机嵌入表示评估FID可信度方法。通过蒙特卡洛dropout计算FID预测方差及特征嵌入模型潜在表示不确定性，发现方差大小与测试输入分布外程度相关。方法为检测异常样本和提升FID评估可靠性提供新视角。

---

### 85 Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models
**link**: https://arxiv.org/pdf/2601.21851.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出DiDAE框架，将冻结基础模型与解纠缠字典学习结合，为基础模型高效生成反事实样本。DiDAE首先在解纠缠字典可解释方向上编辑基础模型嵌入，然后通过扩散自编码器解码。当与反事实知识蒸馏结合时，DiDAE-CFKD在缓解捷径学习方面实现最先进性能。

---

### 86 Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning
**link**: https://arxiv.org/pdf/2601.21894.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文研究代码结构复杂性对LLM推理影响，考察解决方案驱动复杂性和问题驱动复杂性。使用圈复杂度和逻辑代码行数构建受控微调数据集，评估开源权重LLMs。结果表明结构属性强烈决定代码有用性，83%实验中将微调数据限制在特定结构复杂性范围优于结构多样代码训练。

---

### 87 Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models
**link**: https://arxiv.org/pdf/2601.21944.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出“清晰度”衡量标准，捕捉下游性能与概念表示稀疏性和精确性相互作用。考虑基于VLM和属性预测器CBMs，及三种稀疏诱导策略。实验揭示灵活性和可解释性关键权衡。

---

### 88 FlexCausal: Flexible Causal Disentanglement via Structural Flow Priors and Manifold-Aware Interventions
**link**: https://arxiv.org/pdf/2601.21567.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出FlexCausal框架，基于块对角协方差VAE，利用基于因子流先验建模外生噪声复杂密度。通过整合监督对齐目标和反事实一致性约束，确保学习潜在子空间与真实因果关系结构对应。引入流形感知相对干预策略确保高保真生成。实验表明FlexCausal显著优于其他方法。

---

### 89 Learning the Mechanism of Catastrophic Forgetting: A Perspective from Gradient Similarity
**link**: https://arxiv.org/pdf/2601.21577.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文建立基于梯度理论框架解释灾难性遗忘，证明强负梯度相似性是遗忘根本原因。利用梯度相似性识别冲突神经元和协作神经元。提出协作神经学习（CNL），通过冻结冲突神经元仅更新协作神经元，理论上消除灾难性遗忘。实验表明CNL实现零遗忘或减少遗忘。

---

### 90 Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps
**link**: https://arxiv.org/pdf/2601.21624.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本综述按来源、生命周期和可见性组织训练记忆机制。引入种子配对函数空间因果估计量；可移植扰动原语；带审计工件报告清单。结论提出可移植、因果、不确定性感知测量协议。

---

### 91 From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning
**link**: https://arxiv.org/pdf/2601.22028.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入CLReg对比表示正则化器，识别遗忘特征并将其推离保留特征，减少遗忘-保留干扰。提供将表示塑造与纠缠减少关联理论见解。实验表明CLReg减少纠缠，促进主流遗忘方法。

---

### 92 Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space
**link**: https://arxiv.org/pdf/2601.22036.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入交叉融合距离（CFD），原则性度量方法隔离改变融合几何结构，对保留融合变化鲁棒。理论上刻画CFD不变性和敏感性属性，并在实验中验证。CFD与下游泛化性能下降相关性比常用方法更紧密。

---

### 93 Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference
**link**: https://arxiv.org/pdf/2601.22132.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出LLM引导框架，仅向LLM请求短前缀并提供给SLM。在数学和编码任务上效果显著：提示占LLM完整响应10-30%显著提高SLM准确性。开发两阶段预测器联合决定提示需求。实验表明引导框架降低推理成本42-94%，比路由和级联基线实现高达2.8倍成本降低。

---

### 94 Rate-Distortion Optimization for Transformer Inference
**link**: https://arxiv.org/pdf/2601.22002.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出基于率失真理论有损压缩框架，学习紧凑编码权衡比特率和准确性。实验表明编解码器实现显著节省。描述并分析Transformer率失真性能，为理解表示编码性能提供统一视角。推导率和熵之间差距边界，开发PAC风格边界估计此差距。实验证明率由这些边界驱动。

---

### 95 Exploring Diverse Generation Paths via Inference-time Stiefel Activation Steering
**link**: https://arxiv.org/pdf/2601.22010.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入STARS方法，将激活引导转化为探索引擎。在每个token处，STARS收集并发生成运行隐藏激活，并在Stiefel流形上联合优化多个加性引导方向。STARS最大化被引导激活几何体积，Stiefel流形诱导引导干预正交性。设计轻量级单步更新保证低延迟。在测试用例生成和科学发现基准上，STARS优于标准采样方法。

---

### 96 Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability
**link**: https://arxiv.org/pdf/2601.22012.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入机制框架，将灾难性遗忘从几何角度解释为个体特征编码转换结果。对一个易于处理模型分析形式化观点，实证测试并强调深度有害影响。展示如何通过Crosscoders将框架用于实际模型分析，呈现视觉Transformer案例研究。

---

### 97 The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR
**link**: https://arxiv.org/pdf/2601.22128.pdf
**date**: 2026-01-30
**keywords**: cs.AI
**abs**: 本文引入SMB-Structure结构化EHR世界模型，将联合嵌入预测架构与下token预测结合。SFT将模型锚定在token空间重建未来患者状态，JEPA仅从初始患者表示中在潜在空间预测未来。在MSK和INSPECT数据集上验证表明，SMB-Structure学习嵌入捕捉自回归基线无法恢复疾病动态。

---

### 98 From Logits to Latents: Contrastive Representation Shaping for LLM Unlearning
**link**: https://arxiv.org/pdf/2601.22028.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入CLReg对比表示正则化器，识别遗忘特征并将其推离保留特征，减少遗忘-保留干扰。提供将表示塑造与纠缠减少关联理论见解。实验表明CLReg减少纠缠，促进主流遗忘方法。

---

### 99 Cross-Fusion Distance: A Novel Metric for Measuring Fusion and Separability Between Data Groups in Representation Space
**link**: https://arxiv.org/pdf/2601.22036.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文引入交叉融合距离（CFD），原则性度量方法隔离改变融合几何结构，对保留融合变化鲁棒。理论上刻画CFD不变性和敏感性属性，并在实验中验证。CFD与下游泛化性能下降相关性比常用方法更紧密。

---

### 100 Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference
**link**: https://arxiv.org/pdf/2601.22132.pdf
**date**: 2026-01-30
**keywords**: cs.LG
**abs**: 本文提出LLM引导框架，仅向LLM请求短前缀并提供给SLM。在数学和编码任务上效果显著：提示占LLM完整响应10-30%显著提高SLM准确性。开发两阶段预测器联合决定提示需求。实验表明引导框架降低推理成本42-94%，比路由和级联基线实现高达2.8倍成本降低。