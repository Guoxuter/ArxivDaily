### 1 Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning

**link**: https://arxiv.org/pdf/2602.18232.pdf  
**date**: 2026-02-23  
**keywords**: cs.CL  
**abs**: 现有针对大型语言模型（LLM）推理的测试时扩展方法通常假设均匀分配更多推理计算资源能提高正确性，但先前研究表明推理不确定性具有高度局部性：一小部分低置信度标记不成比例地导致推理错误和不必要的输出扩展。基于此观察，本文提出“Thinking by Subtraction”，一种基于置信度的对比解码方法（Confidence-Driven Contrastive Decoding, CCD），通过目标性的标记级干预提高推理可靠性。该方法在解码过程中检测低置信度标记并进行选择性干预，构建对比参考（将高置信度标记替换为最小占位符），并在低置信度位置减去该参考分布以优化预测。实验表明，CCD在数学推理基准上显著提高了准确性，同时大幅减少了输出长度，且仅带来最小的KV缓存开销。作为无训练方法，CCD通过目标性低置信度干预增强了推理可靠性，避免了计算冗余。  

---

### 2 CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications

**link**: https://arxiv.org/pdf/2602.17949.pdf  
**date**: 2026-02-23  
**keywords**: cs.CL, latent space  
**abs**: 背景：临床命名实体识别工具通常将自由文本映射到统一医学语言系统（UMLS）的概念唯一标识符（CUI）。然而，对于许多下游任务，具有临床意义的单元并非单个CUI，而是包含相关同义词、子类型和超类型的概念集。构建此类概念集劳动密集、执行不一致，且现有工具支持不足，尤其对于直接基于UMLS CUI运行的NLP管道。方法：提出CUICurate，一种基于图的检索增强生成（GraphRAG）框架，用于自动化UMLS概念集构建。构建UMLS知识图谱（KG）并进行嵌入以实现语义检索。针对每个目标概念，从KG中检索候选CUI，随后使用大型语言模型（LLM）进行过滤和分类步骤，比较了两种LLM（GPT-5和GPT-5-mini）。该框架在五个词汇异质的临床概念上 against 手动构建的基准和黄金标准概念集进行了评估。结果：在所有概念上，CUICurate生成的概念集比手动基准显著更大、更完整，同时匹配人类精度。两种LLM的比较发现，GPT-5-mini在过滤过程中实现了更高的召回率，而GPT-5产生的分类更接近临床医生的判断。输出在重复运行中稳定且计算成本低。结论：CUICurate提供了一种可扩展且可重复的方法来支持UMLS概念集构建，大幅减少了手动工作量。通过整合基于图的检索与LLM推理，该框架生成了聚焦的候选概念集，可适应不同表型分析和分析需求的临床NLP管道。  

---

### 3 Bayesian Optimality of In-Context Learning with Selective State Spaces

**link**: https://arxiv.org/pdf/2602.17744.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 该论文提出贝叶斯最优序列预测作为理解上下文学习（ICL）的新原理。不同于将Transformer视为执行隐式梯度下降的解释，作者将ICL形式化为对潜在序列任务的元学习。对于由线性高斯状态空间模型（LG-SSMs）控制的任务，证明元训练的选择性SSM渐近实现贝叶斯最优预测器，收敛到后验预测均值。进一步建立了与梯度下降的统计分离，构造了具有时间相关噪声的任务，其中最优贝叶斯预测器严格优于任何经验风险最小化（ERM）估计器。由于Transformer可视为执行隐式ERM，这表明选择性SSMs由于更高的统计效率而实现更低的渐近风险。实验在合成LG-SSM任务和字符级马尔可夫基准上证实，选择性SSMs更快收敛到贝叶斯最优风险，在结构化噪声设置中具有更长上下文时表现出更高的样本效率，并且比线性Transformer更稳健地跟踪潜在状态。该研究将ICL从“隐式优化”重新定义为“最优推理”，解释了选择性SSMs的效率，并为架构设计提供了原则性基础。  

---

### 4 QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration

**link**: https://arxiv.org/pdf/2602.17784.pdf  
**date**: 2026-02-23  
**keywords**: cs.CL  
**abs**: 矿产远景 mapping 需要整合文本矿床模型和地理空间数据以识别潜在矿化区域，传统过程手动且知识密集。本文提出QueryPlot，一个语义检索与映射框架，利用现代NLP技术整合大规模地质文本语料与地质图数据。系统将用户自然语言查询与区域描述通过预训练嵌入模型编码，计算语义相似度分数来排名并空间可视化区域为连续证据层。支持矿床特征的组合查询，聚合多相似性层用于多标准分析。钨矽卡岩矿床案例研究表明，基于嵌入的检索实现高召回率，生成的远景区域与专家定义区域高度一致，相似性分数作为特征加入监督学习可提升分类性能。  

---

### 5 Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations

**link**: https://arxiv.org/pdf/2602.17881.pdf  
**date**: 2026-02-23  
**keywords**: cs.CL  
**abs**: 转向向量（steering vectors）是通过在推理时向激活添加学习偏差来控制语言模型行为的轻量级方法。尽管平均有效，但效果大小因样本而异，对许多目标行为不可靠。本研究调查为何转向可靠性因行为而异及受训练数据影响。发现：1）训练激活差异的余弦相似度越高，转向越可靠；2）正负极激活沿转向方向分离更好的行为数据集更易可靠转向；3）不同提示变体训练的转向向量方向不同，但性能相似且数据集间 efficacy 相关。研究表明，当潜在目标行为表示不能被线性转向方向有效近似时，转向向量不可靠。这些发现提供了诊断转向不可靠性的实用方法，并推动开发考虑非线性潜在表示的更鲁棒转向方法。  

---

### 6 Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions

**link**: https://arxiv.org/pdf/2602.17907.pdf  
**date**: 2026-02-23  
**keywords**: cs.CL  
**abs**: 传统神经主题模型通常通过重构文档的词袋（BoW）表示优化，忽略上下文信息且受数据稀疏性影响。本文提出一种利用语言模型（LMs）构建语义接地软标签目标的新方法：通过特定提示条件下的下一个token概率投影到预定义词汇表，获得上下文丰富的监督信号。通过训练主题模型使用LM隐藏状态重构软标签，该方法生成与语料潜在主题结构更对齐的高质量主题。三个数据集实验表明，该方法在主题连贯性、纯度上显著优于现有基线，并引入检索基度量，在识别语义相似文档上表现突出，证明其对检索应用的有效性。  

---

### 7 Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers

**link**: https://arxiv.org/pdf/2602.17993.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 人类通过一系列步骤解决复杂问题，每个步骤的结果为下一步提供信息。本文认为Transformer的推理能力受限于计算路径的固定最大步数。为此，提出Turbo Connection（TurboConn）架构，通过将每个token t的高层隐藏状态路由到token t+1的低层，克服固定深度约束。在预训练LLM上微调该方法，在GSM8K、Parity和多步算术等基准测试中准确率提升0.9%至10%以上，并表明这些反向连接的密度至关重要；密集交互显著优于仅传递单个隐藏状态或向量的“稀疏”替代方案。TurboConn可集成到预训练LLM中以克服特定任务的性能瓶颈，例如微调后的Qwen-3-1.7B在Parity上仅达到53.78%，添加该架构修改后准确率达100%，且无需从头重新训练模型或复杂的课程学习。结果为计算路径深度是推理能力的关键因素提供了强有力的实证证据，并提供了一种不显著影响生成延迟的LLM增强机制。  

---

### 8 VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning

**link**: https://arxiv.org/pdf/2602.18429.pdf  
**date**: 2026-02-23  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）在数学和编码等领域的推理任务中取得了显著进展，但在需要丰富社会文化知识和多样化本地语境的任务中表现不佳，尤其是涉及印度文化的任务。现有文化基准存在手工制作、单跳事实回忆问题和难以扩展等问题。为此，本文提出VIRAASAT，一种新颖的半自动多跳方法，用于生成印度文化特定的多跳问答数据集。该数据集利用包含700多个专家策划的文化 artifacts 的知识图谱，涵盖印度文化的13个关键属性，生成了3200多个需要链式文化推理的多跳问题。作者还提出了Symbolic Chain-of-Manipulation（SCoM）框架，通过训练模型模拟内部原子知识图谱操作，以桥接现有CoT方法在低概率事实推理上的局限性。实验表明，SCoM在监督微调中比标准CoT基线高出20%。  

---

### 9 Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering

**link**: https://arxiv.org/pdf/2602.17691.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 量化语言模型面临一个基本困境：低采样温度导致输出重复、模式崩溃，而高温度（T>2.0）则导致轨迹发散和语义不连贯。本文提出HELIX几何框架，通过将隐藏状态轨迹束缚到预计算的真实性流形（truthfulness manifold）上，实现输出熵与幻觉的解耦。HELIX计算统一真实分数（UTS），结合 token 级语义熵和与流形的马氏距离。当UTS指示轨迹发散时，分级导向向量将激活重定向到结构连贯的区域，同时仅影响0.2-2.5%的 token。该方法利用潜空间（隐藏状态流形）进行推理控制，属于潜空间和潜推理相关研究。  

---

### 10 ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization

**link**: https://arxiv.org/pdf/2602.17867.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 理解大型语言模型（LLM）激活空间中学习到的方向所编码的特征，需要识别能强烈激活这些方向的输入。特征可视化通过优化输入以最大程度激活目标方向，为昂贵的数据集搜索方法提供了替代方案，但由于文本的离散性，在LLM中尚未得到充分探索。现有提示优化技术在该领域（易陷入局部最小值）表现不佳。为此，本文提出ADAPT，一种结合束搜索初始化和自适应梯度引导突变的混合方法，专为这些失败模式设计。在Gemma 2 2B的稀疏自编码器潜变量上进行评估，提出基于数据集激活统计的指标，结果表明ADAPT在各层和潜变量类型上一致优于先前方法。该研究证实LLM的特征可视化是可行的，但需要针对该领域定制设计假设。此研究直接涉及LLM的激活空间（潜空间）的特征可视化与分析。  

---

### 11 On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction

**link**: https://arxiv.org/pdf/2602.18301.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 近期研究表明，冻结的大型语言模型（LLMs）可以通过两个学习到的原型标记（proto-tokens）在单次前向传播中重建数百个标记，为超越自回归范式提供了可能。本文研究这些原型标记编码了哪些信息以及它们在重建和受控约束下的行为。通过一系列实验，旨在分离两个原型标记中的语义和句法内容，分析e-标记的稳定性属性，并可视化重建过程中对e-标记的注意力模式。最后，使用教师嵌入测试了两种用于“施加”语义结构到e-标记的正则化方案，包括基于锚点的损失和关系蒸馏目标。结果表明，在标准优化下，m-标记比e-标记更强烈地捕获语义信息；基于锚点的约束与重建准确性存在显著权衡；关系蒸馏可以将批次级语义关系转移到原型标记空间，而不会牺牲重建质量，支持未来将原型标记作为中间表示的非自回归seq2seq系统的可行性。  

---

### 12 El Agente Gráfico: Structured Execution Graphs for Scientific Agents

**link**: https://arxiv.org/pdf/2602.17902.pdf  
**date**: 2026-02-23  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLMs）越来越多地用于自动化科学工作流，但其与异构计算工具的集成仍然是临时且脆弱的。当前的智能体方法通常依赖非结构化文本来管理上下文和协调执行，生成大量可能掩盖决策来源并阻碍可审计性的信息。本文提出El Agente Gráfico，这是一个单智能体框架，将LLM驱动的决策嵌入到类型安全的执行环境和用于外部持久化的动态知识图谱中。该设计的核心是科学概念的结构化抽象和对象-图谱映射器，将计算状态表示为类型化的Python对象，存储在内存中或持久化到外部知识图谱中。这种设计通过类型化符号标识符而非原始文本来实现上下文管理，确保一致性、支持来源跟踪并实现高效的工具编排。通过在先前多智能体系统评估的大学级量子化学任务套件上开发自动化基准框架进行评估，表明单个智能体在与可靠执行引擎耦合时，能够稳健地执行复杂、多步骤和并行计算。进一步将该范式扩展到另外两类大型应用：构象集合生成和金属有机框架设计，其中知识图谱同时作为内存和推理基板。这些结果共同说明了抽象和类型安全如何为超越以提示为中心的设计的智能体科学自动化提供可扩展的基础。  

---

### 13 MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance

**link**: https://arxiv.org/pdf/2602.17930.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 强化学习（RL）智能体在稀疏或延迟奖励环境中常因缺乏先验结构而面临高样本复杂度问题。大型语言模型（LLMs）可提供子目标分解、合理轨迹和抽象先验知识以促进早期学习，但过度依赖LLM监督会带来可扩展性限制和对不可靠信号的依赖。本文提出MIRA（记忆集成强化学习智能体），它整合了结构化、可进化的记忆图来指导早期训练。该图存储决策相关信息（包括轨迹段和子目标结构），由智能体的高回报经验和LLM输出共同构建。此设计将LLM查询转化为持久记忆，而非需要持续实时监督。从记忆图中导出效用信号，通过软调整优势估计来影响策略更新，且不修改底层奖励函数。随着训练推进，智能体策略逐渐超越初始LLM衍生先验，效用项衰减以保留标准收敛保证。理论分析表明，基于效用的塑造可改善稀疏奖励环境中的早期学习。实验显示，MIRA优于RL基线，实现了与依赖频繁LLM监督方法相当的回报，同时显著减少在线LLM查询次数。  

---

### 14 Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning

**link**: https://arxiv.org/pdf/2602.17931.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 在稀疏或延迟奖励环境中，强化学习（RL）因需要大量交互来学习而导致样本复杂度高。这一局限性促使人们使用大型语言模型（LLMs）进行子目标发现和轨迹指导。尽管LLMs可支持探索，但频繁依赖LLM调用引发了可扩展性和可靠性担忧。本文通过构建记忆图来解决这些挑战，该图编码来自LLM指导和智能体自身成功滚动的子目标与轨迹。从该图中导出效用函数，用于评估智能体轨迹与先前成功策略的对齐程度。此效用塑造优势函数，为评论家提供额外指导而不改变奖励。我们的方法主要依赖离线输入和偶尔的在线查询，避免依赖持续LLM监督。基准环境中的初步实验表明，与基线RL方法相比，该方法提高了样本效率并加速了早期学习，最终回报与需要频繁LLM交互的方法相当。  

---

### 15 Neural Prior Estimation: Learning Class Priors from Latent Representations

**link**: https://arxiv.org/pdf/2602.17853.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 类不平衡会导致深度神经网络产生系统性偏差，因其会施加倾斜的有效类先验。本文提出神经先验估计器（NPE）框架，从潜在表示中学习特征条件对数先验估计。NPE通过单向逻辑损失联合训练先验估计模块与主干网络，在神经崩溃机制下可解析恢复类对数先验（至多加性常数），提供无需显式类计数或分布特定超参数的理论自适应信号。将学习到的估计整合到对数调整中形成NPE-LA，实现偏差感知预测。在长尾CIFAR和不平衡语义分割基准（STARE、ADE20K）上的实验表明，该方法持续改进性能，尤其对代表性不足的类效果显著，为学习先验估计和不平衡感知预测提供了轻量级且理论合理的方案。  

---

### 16 Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization

**link**: https://arxiv.org/pdf/2602.17679.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 贝叶斯优化（BO）是优化黑盒制造过程的强大方法，但其在处理高维多阶段系统时性能常受限制，这类系统中可观察中间输出。标准BO将过程视为黑盒，忽略中间观测和底层过程结构。部分可观测高斯过程网络（POGPN）将过程建模为有向无环图（DAG）。然而，当观测是高维状态空间时间序列时，使用中间观测具有挑战性。过程专家知识可用于从高维状态空间数据中提取低维潜在特征。我们提出POGPN-JPSS框架，将POGPN与联合参数和状态空间（JPSS）建模相结合，以利用中间提取的信息。我们在多阶段生物乙醇生产过程的高维模拟中证明了POGPN-JPSS的有效性。结果表明，POGPN-JPSS显著优于最先进方法，达到所需性能阈值的速度快两倍，且可靠性更高。快速优化直接转化为时间和资源的大量节省。这凸显了将专家知识与结构化概率模型相结合对快速过程成熟的重要性。  

---

### 17 BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs

**link**: https://arxiv.org/pdf/2602.17680.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 现有蛋白质语言模型（PLMs）往往难以适应多任务，且在不同生物背景下泛化能力较差。相比之下，通用大型语言模型（LLMs）缺乏解释蛋白质序列的能力，在领域特定知识方面存在不足，限制了其有效的生物语义推理能力。为结合两者优势，我们提出BioBridge，一种用于蛋白质理解的领域自适应持续预训练框架。该框架采用领域增量持续预训练（DICP），将蛋白质领域知识和通用推理语料同时注入LLM，有效缓解灾难性遗忘。通过PLM-Projector-LLM管道实现跨模态对齐，将蛋白质序列嵌入映射到语言模型的语义空间。最终采用端到端优化统一支持各种任务，包括蛋白质属性预测和知识问答。我们提出的BioBridge在多个蛋白质基准测试（如EC和BindingDB）上表现出与主流PLMs相当的性能。在MMLU和RACE等一般理解任务上也达到了与LLMs相当的结果。这展示了其结合领域特定适应性和通用语言能力的创新优势。  

---

### 18 Neurosymbolic Language Reasoning as Satisfiability Modulo Theory

**link**: https://arxiv.org/pdf/2602.18095.pdf  
**date**: 2026-02-23  
**keywords**: cs.AI  
**abs**: 自然语言理解需要交错的文本和逻辑推理，但大型语言模型往往难以可靠地执行此类推理。现有神经符号系统将LLM与求解器相结合，但仅限于可完全形式化的任务（如数学或程序合成），无法处理仅有部分逻辑结构的自然文档。本文引入Logitext，一种神经符号语言，将文档表示为自然语言文本约束（NLTC），使部分逻辑结构显式化。作者开发了一种将基于LLM的约束评估与可满足性模理论（SMT）求解相结合的算法，实现联合文本-逻辑推理。在新的内容审核基准以及LegalBench和Super-Natural Instructions上的实验表明，Logitext提高了准确性和覆盖率。这项工作首次将基于LLM的推理视为SMT理论，将神经符号方法扩展到可完全形式化领域之外。  

---

### 19 SOMtime the World Ain't Fair: Violating Fairness Using Self-Organizing Maps

**link**: https://arxiv.org/pdf/2602.18201.pdf  
**date**: 2026-02-23  
**keywords**: cs.AI  
**abs**: 人们普遍认为，当敏感属性未被纳入训练时，无监督表示对这些属性是中立的。本文证明这一假设是错误的。通过基于高容量自组织映射（SOM）的拓扑保持表示方法SOMtime，作者表明年龄和收入等敏感属性在纯无监督嵌入中会成为主导的潜在轴。在两个大规模真实世界数据集（五个国家的世界价值观调查和人口普查收入数据集）上，SOMtime恢复了与被隐瞒敏感属性对齐的单调排序，Spearman相关系数高达0.85，而PCA和UMAP通常低于0.23（仅有一个例外达到0.31），t-SNE和自编码器最多达到0.34。此外，SOMtime嵌入的无监督分割产生了人口统计学上倾斜的簇，表明在没有任何监督任务的情况下存在下游公平性风险。这些发现证实，“通过无知实现公平”在序数敏感属性的表示层面失败，公平性审计必须扩展到机器学习管道的无监督组件。  

---

### 20 Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO

**link**: https://arxiv.org/pdf/2602.17686.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 将大型语言模型的思维链（CoT）推理提炼到紧凑的学生模型中面临一个基本挑战：教师的推理过程往往过于冗长，导致较小的模型难以忠实地复现。现有方法要么将推理压缩为单步，从而失去了CoT的可解释性价值。本文提出了一个三阶段课程学习框架，通过渐进式技能获取来解决这种能力不匹配问题。首先，通过掩码打乱重构建立结构理解。其次，在掩码补全任务上应用组相对策略优化（GRPO），使模型能够自主发现准确性和简洁性之间的平衡。第三，识别持续的失败案例，并通过目标重写引导学生内化教师知识，同样使用GRPO进行优化。在GSM8K上的实验表明，该方法使Qwen2.5-3B-Base的准确率提高了11.29%，同时输出长度减少了27.4%，优于指令调优变体和先前的提炼方法。  

---

### 21 Agentic Unlearning: When LLM Agent Meets Machine Unlearning

**link**: https://arxiv.org/pdf/2602.17692.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 本文引入了“智能体遗忘”（agentic unlearning），旨在从具有闭环交互的智能体的模型参数和持久记忆中移除指定信息。现有遗忘方法仅针对参数，留下两个关键缺口：（i）参数-记忆回流，即检索会重新激活参数残余或记忆伪影重新引入敏感内容；（ii）缺乏覆盖参数和记忆路径的统一策略。作者提出了同步回流遗忘（SBU）框架，该框架联合跨参数和记忆路径进行遗忘。记忆路径执行基于依赖闭合的遗忘，修剪孤立实体同时逻辑上使共享伪影失效。参数路径采用随机参考对齐，引导模型输出朝向高熵先验。这些路径通过同步双更新协议集成，形成闭环机制，其中记忆遗忘和参数抑制相互强化，防止跨路径再污染。在医疗QA基准上的实验表明，SBU减少了两个路径中目标私人信息的痕迹，同时对保留数据的性能下降有限。  

---

### 22 Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework

**link**: https://arxiv.org/pdf/2602.18055.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 双模态大型语言模型（Dual-to-Dual MLLMs）指的是能够通过文本和图像模态实现统一的多模态理解与生成的模型。尽管这类模型展现出强大的即时学习和泛化能力，但在终身进化方面仍存在不足，这严重影响了它们对动态现实场景的持续适应。其中一个挑战是学习新任务不可避免地会破坏已学知识。除了传统的灾难性遗忘外，双模态大型语言模型还面临幻觉、不遵循指令以及跨模态知识迁移失败等其他挑战。然而，目前尚未建立针对双模态大型语言模型的标准化持续学习框架，导致这些挑战尚未得到充分探索。为此，本文提出了Continual-NExT，一个针对双模态大型语言模型的持续学习框架，并设计了专门的评估指标。为了提高双模态大型语言模型的持续学习能力，我们提出了一种高效的MAGE（通用LoRA与专家LoRA的混合与聚合）方法，以进一步促进跨模态知识迁移并减轻遗忘。大量实验表明，MAGE优于其他持续学习方法，并实现了最先进的性能。  

---

### 23 Variational Distributional Neuron

**link**: https://arxiv.org/pdf/2602.18250.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 本文提出了变分分布神经元的概念验证，这是一种作为VAE构建块的计算单元，明确携带先验、摊销后验和局部ELBO。该单元不再是确定性标量而是分布：计算不再是传播值，而是在约束下收缩连续的可能性空间。每个神经元参数化后验，传播重参数化样本，并通过局部ELBO的KL项进行正则化——因此，激活是分布性的。这种“收缩”可通过局部约束测试，并通过内部措施监控。单元携带的上下文信息量及其时间持久性由不同约束局部调节。该提案解决了一个结构性张力：在顺序生成中，因果关系主要在符号空间中组织，即使存在潜变量，它们通常仍为辅助性，而有效动态由高度确定性的解码器承载。同时，概率潜模型捕获变异因素和不确定性，但这种不确定性通常由全局或参数机制承担，而单元继续传播标量——因此核心问题：如果不确定性是计算的内在属性，为何计算单元不明确携带它？为此，作者提出两个方向：(i)概率约束的组合，必须稳定、可解释且可控；(ii)粒度：如果推理是约束下分布的协商，基本单元应保持确定性还是变为分布性？作者分析了“崩溃”模式和“活神经元”的条件，然后通过单元上潜变量的自回归先验将贡献扩展到时间维度。  

---

### 24 A Probabilistic Framework for LLM-Based Model Discovery

**link**: https://arxiv.org/pdf/2602.18266.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 自动化从观测数据中发现机制模拟器模型的方法为加速科学进展提供了有前景的途径。此类方法通常采用智能体式迭代工作流，通过模仿人类发现过程反复提出和修改候选模型。然而，现有的基于LLM的方法通常通过手工制作的启发式程序实现此类工作流，缺乏明确的概率公式。本文将模型发现重新定义为概率推理，即从未知的机制模型分布中采样以解释数据。这一视角提供了在单一推理框架内统一推理模型提议、改进和选择的方法。作为这一观点的具体实例，作者引入ModelSMC，一种基于序贯蒙特卡洛采样的算法。ModelSMC将候选模型表示为粒子，由LLM迭代提议和改进，并使用基于似然的标准加权。在现实世界科学系统上的实验表明，该公式能发现具有可解释机制的模型，并改进后验预测检查。更广泛地说，这一视角为理解和开发基于LLM的模型发现方法提供了概率透镜。  

---

### 25 Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds

**link**: https://arxiv.org/pdf/2602.17798.pdf  
**date**: 2026-02-23  
**keywords**: cs.LG  
**abs**: 混合专家模型依赖学习到的路由器将令牌分配给专家，但标准的softmax门控没有提供控制稀疏性和利用率权衡的原则性机制。本文提出Grassmannian MoE（GrMoE），这是一种在子空间的Grassmann流形上运行的路由框架，其中门控权重来自矩阵Bingham分布的浓度参数。这种构造产生了一个单一的、可解释的旋钮——浓度矩阵Λ，它可以连续控制路由熵，用平滑的、几何上有原则的稀疏性机制取代离散的top-k选择。作者进一步开发了用于后验路由分布的摊销变分推理程序，实现了自然抵抗专家崩溃的不确定性感知专家分配。在合成路由任务、350M参数（8个专家）、1.3B参数（16个专家）和2.7B参数（32个专家）的MoE语言模型上，GrMoE在所有种子上实现了0%的路由崩溃，具有相当或更好的困惑度，负载平衡提高了15-30%，并且浓度和有效稀疏性之间存在平滑的单调关系，支持无需重新训练的事后稀疏性调整。令牌级分析显示，专家学习到与语言专业化相关的异质浓度值，提供了可解释的路由行为。