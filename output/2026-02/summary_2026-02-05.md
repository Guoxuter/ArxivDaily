### 1 Scaling In-Context Online Learning Capability of LLMs via Cross-Episode Meta-RL

**link**: https://arxiv.org/pdf/2602.04089.pdf  
**date**: 2026-02-05  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLM）在任务相关信息预先可用时表现良好，但在需要交互获取信息、处理延迟反馈和平衡信息收集与利用的在线决策任务中存在不足。本文提出ORBIT框架，通过多任务、多轮次元强化学习训练LLM从上下文交互中学习。元训练后，较小的开源模型（如Qwen3-14B）在新环境中展现出显著提升的上下文在线学习能力，匹配GPT-5.2性能，并优于标准RL微调。模型规模扩展与性能提升一致，突显了推理时学习决策智能体的潜力。  

---

### 2 GeoIB: Geometry-Aware Information Bottleneck via Statistical-Manifold Compression

**link**: https://arxiv.org/pdf/2602.03906.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 信息瓶颈（IB）在深度学习中广泛应用，但通常依赖变分界或神经互信息估计器，而非直接控制互信息I(X;Z)，导致压缩控制间接且优化脆弱。本文提出GeoIB方法，通过统计流形压缩实现几何感知信息瓶颈，解决上述问题，提升模型鲁棒性和效率。  

---

### 3 Transformers perform adaptive partial pooling

**link**: https://arxiv.org/pdf/2602.03980.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 研究发现，Transformer（如GPT2）在训练过程中，对当前上下文外观察的依赖逐渐减少（池化程度降低），且池化受上下文频率、类型数量和变异性影响，类似于层次回归。这些特征在理性和经验上均合理，可能与LLM的记忆机制相关，揭示了模型在泛化过程中如何利用相似上下文信息。  

---

### 4 When Chains of Thought Don't Matter: Causal Bypass in Large Language Models

**link**: https://arxiv.org/pdf/2602.03994.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 思维链（CoT）提示被认为能提高模型透明度和推理能力，但实验显示模型答案常与CoT内容因果独立，即使CoT被操纵检测器标记。研究提出诊断框架，结合可解释行为模块和因果探针，测量CoT介导的影响。结果显示，许多QA任务存在近乎完全绕过，而逻辑问题显示更强中介效应，暴露了LLM潜在推理机制的缺陷。  

---

### 5 Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search

**link**: https://arxiv.org/pdf/2602.04248.pdf  
**date**: 2026-02-05  
**keywords**: cs.AI  
**abs**: 现有蒙特卡洛树搜索（MCTS）方法在推理后丢弃经验，无法模拟人类经验积累。本文提出Empirical-MCTS框架，通过成对经验进化元提示（PE-EMP）和记忆优化代理，将无状态搜索转为连续学习过程。在AIME25、ARC-AGI-2和MathArena Apex等基准上，该方法显著优于无状态MCTS和独立经验驱动代理，强调结构化搜索与经验结合对复杂推理任务的关键性。  

---

### 6 From Assumptions to Actions: Turning LLM Reasoning into Uncertainty-Aware Planning for Embodied Agents

**link**: https://arxiv.org/pdf/2602.04326.pdf  
**date**: 2026-02-05  
**keywords**: cs.AI  
**abs**: 具身智能体在部分可观测环境中需处理不确定性，但现有方法依赖频繁通信，成本高。本文引入PCE框架，将LLM推理轨迹转为结构化决策树，节点编码环境假设，叶节点映射到动作，并通过可能性、收益和成本评分路径。在C-WAH和TDW-MAT基准上，PCE在成功率和效率上优于通信基线，且用户研究显示其通信模式更高效可信。  

---

### 7 Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents

**link**: https://arxiv.org/pdf/2602.04813.pdf  
**date**: 2026-02-05  
**keywords**: cs.AI  
**abs**: 本文提出七维分类法（认知能力、知识管理、交互模式等）评估医疗健康领域LLM智能体。基于49项研究的分析显示，外部知识整合普遍实现（76%），但事件触发激活（92%未实现）和漂移检测（98%未实现）缺失。多智能体设计主导架构（82%），而治疗规划等行动导向领域存在显著差距（59%未实现），揭示能力不对称性。  

---

### 8 Fluid Representations in Reasoning Models

**link**: https://arxiv.org/pdf/2602.04843.pdf  
**date**: 2026-02-05  
**keywords**: cs.AI  
**abs**: 推理语言模型在抽象问题上优于非推理模型，但内部机制不明。在Mystery Blocksworld任务上，QwQ-32B模型在推理中逐步改进动作和概念的内部表示，发展出聚焦结构（而非动作名称）的抽象编码。实验证明，注入精炼表示可提升准确性，符号表示可替代模糊编码，表明“流体推理表示”是性能驱动因素。  

---

### 9 Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra

**link**: https://arxiv.org/pdf/2602.03875.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 本文介绍一种可逆深度学习模型，使用条件可逆神经网络实现分子结构和13C NMR光谱的双向转换。模型通过i-RevNet风格块构建，训练后可从结构预测分箱光谱编码，并在推理时反转生成结构候选。在过滤子集上，模型展示数值可逆性和有意义的结构信号，支持不确定性感知的候选生成。  

---

### 10 Language Models Struggle to Use Representations Learned In-Context

**link**: https://arxiv.org/pdf/2602.04212.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLM）在部署时适应新上下文能力不足。研究探讨LLM是否能利用上下文诱导的表示完成下游任务，基于Park等人（2024）的上下文表示学习证明，但LLM在应用这些表示时存在挑战，突显模型在动态环境中的泛化局限。  

---

### 11 Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning

**link**: https://arxiv.org/pdf/2602.04265.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 强化学习（RLVR）增强LLM推理时面临熵崩溃和探索不足问题。本文提出T2T奖励框架，受人类学习启发，分两阶段：错误时激励“增厚”（更长轨迹）拓宽搜索；正确后“变薄”（长度惩罚）减少冗余。在数学基准上，T2T显著优于GRPO和基线，在Qwen和Deepseek模型上实现更优性能。  

---

### 12 Contextual Drag: How Errors in the Context Affect LLM Reasoning

**link**: https://arxiv.org/pdf/2602.04288.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 研究发现“上下文拖累”现象：上下文中的失败尝试使后续生成偏向结构相似错误。在11个模型和8个任务上评估，性能下降10-20%，迭代自我优化可能恶化。树编辑距离分析显示错误模式继承，外部反馈和自验证无法消除该效应。缓解策略如微调和去噪仅部分有效，表明这是推理架构的持续失败模式。  

---

### 13 LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding

**link**: https://arxiv.org/pdf/2602.04541.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL, LLM Memory  
**abs**: 长上下文LLM推理因键值缓存扩展面临内存和延迟瓶颈。现有方法跨层共享关键令牌，但忽略注意力头多样性。LycheeDecode提出混合头注意力机制，将头分为动态检索头和稀疏计算头，重用关键令牌。在Llama3和Qwen3上，该方法在长上下文理解和推理基准上实现与全注意力相当的质量，128K上下文下加速达2.7倍。  

---

### 14 Trust The Typical

**link**: https://arxiv.org/pdf/2602.04581.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL, latent space  
**abs**: 当前LLM安全方法依赖枚举威胁的脆弱策略。本文提出T3框架，将安全视为分布外（OOD）检测问题，学习可接受提示的分布并标记显著偏离为威胁。在18个基准上，T3实现最先进性能，误报率降低40倍，且单一模型可迁移到不同领域和14种语言，生产部署开销低于6%。  

---

### 15 A Hitchhiker's Guide to Poisson Gradient Estimation

**link**: https://arxiv.org/pdf/2602.03896.pdf  
**date**: 2026-02-05  
**keywords**: stat.ML  
**abs**: 泊松分布潜变量模型在神经科学中应用广泛，但离散样本微分困难。本文系统比较指数到达时间（EAT）模拟和Gumbel-SoftMax（GSM）松弛方法，改进EAT以保障无偏一阶矩并减少二阶矩偏差。在泊松潜变量变分自编码器和广义线性模型任务上，改进EAT性能更优，鲁棒性更高。  

---

### 16 Learning Multi-type heterogeneous interacting particle systems

**link**: https://arxiv.org/pdf/2602.03954.pdf  
**date**: 2026-02-05  
**keywords**: stat.ML  
**abs**: 本文提出从多轨迹数据中联合推断网络拓扑、多类型相互作用核和类型分配的框架。通过三阶段方法（低秩嵌入、聚类、矩阵分解）解决非凸混合整数优化问题。理论提供误差界，合成数据实验验证方法能准确重建动态（如捕食者-猎物系统）并对噪声鲁棒。  

---

### 17 Statistical Guarantees for Reasoning Probes on Looped Boolean Circuits

**link**: https://arxiv.org/pdf/2602.03970.pdf  
**date**: 2026-02-05  
**keywords**: stat.ML  
**abs**: 本文分析循环布尔电路推理模型中探针的统计行为，模型由完美ν元树计算图构成，输出反馈到输入。在可实现的转导设置中，研究部分可观测性导致的泛化问题，为推理探针的不确定性提供理论基础。  

---

### 18 Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning

**link**: https://arxiv.org/pdf/2602.04380.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 现有组策略优化（如GRPO）仅使用KL散度正则化。本文提出基于组的镜像策略优化（GBMPO）框架，扩展至Bregman散度，包括手动设计（如概率L2）和神经镜像映射。在GSM8K和MBPP任务上，手动ProbL2-GRPO准确率86.7%（比基线高5.5%），神经镜像映射pass@1达60.1-60.8%，表明散度选择是关键设计维度。  

---

### 19 History-Guided Iterative Visual Reasoning with Self-Correction

**link**: https://arxiv.org/pdf/2602.04413.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 自一致性方法提高多模态LLM（MLLM）可靠性，但未重用历史信息。受人类纠错启发，提出H-GIVR框架：MLLM多次观察图像，将先前答案作为参考以动态纠正错误。在五个数据集上，H-GIVR显著提升跨模态推理准确率（如ScienceQA上Llama3.2-vision提升107%），计算成本低。  

---

### 20 EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL

**link**: https://arxiv.org/pdf/2602.04417.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 本文提出两种技术改进LLM策略梯度：用指数移动平均（EMA）替换固定锚定策略，引入Top-k KL估计器（无偏且灵活）。推导了EMA锚定的稳定性条件。在数学推理和智能体任务上，EMA-PG使Qwen-1.5B在OlympiadBench达53.9%准确率，Qwen-3B在搜索引擎问答上平均提升33.3%。  

---

### 21 SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization

**link**: https://arxiv.org/pdf/2602.04811.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 自我进化要求智能体内化新经验解决未来问题。SE-Bench引入混淆NumPy API的诊断环境，评估知识内化。研究发现：（1）开放书训练抑制记忆保留，“闭卷训练”强制知识压缩；（2）标准RL无法完全内化知识；（3）自博弈结合SFT可行，但RL不行，为自我进化提供严格平台。  

---

### 22 Subliminal Effects in Your Data: A General Mechanism via Log-Linearity

**link**: https://arxiv.org/pdf/2602.04863.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 训练现代LLM涉及多种算法和数据集组合，数据集可传递无法从单点观察的信号。受LLM线性结构研究启发，本文揭示一种通用机制，通过该机制隐藏子文本在通用数据集中出现，为数据集效应提供基本解释。  

---

### 23 Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning

**link**: https://arxiv.org/pdf/2602.04872.pdf  
**date**: 2026-02-05  
**keywords**: stat.ML  
**abs**: 多模态上下文学习理论基础不足。本文引入数学框架，证明单层线性自注意力无法均匀恢复贝叶斯最优预测器。提出新颖线性化交叉注意力机制，在层数和上下文长度较大时，通过梯度流优化可证明贝叶斯最优，强调深度和交叉注意力对多模态分布的有效性。  

---

### 24 Group Contrastive Learning for Weakly Paired Multimodal Data

**link**: https://arxiv.org/pdf/2602.04021.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 提出GROOVE方法，用于高内容扰动数据，其中跨模态样本通过共享扰动标签弱配对。核心贡献GroupCLIP损失桥接CLIP和SupCon，解决弱配对设置下的对比学习空白。结合动态反向翻译自编码器，鼓励跨模态纠缠表示。评估框架显示GROOVE在模拟和单细胞数据上优于基线，强调组级约束的重要性。  

---

### 25 Identifying Intervenable and Interpretable Features via Orthogonality Regularization

**link**: https://arxiv.org/pdf/2602.04718.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 通过稀疏自编码器微调语言模型，将解码器矩阵分解为几乎正交的特征。正交性惩罚减少特征干扰和叠加，保持性能。嵌入特征解释距离随正交性惩罚增加，提升可解释性。结合因果机制原理，正交性促进适合干预的模块化表示，实验证明可进行孤立干预。  

---

### 26 From Data to Behavior: Predicting Unintended Model Behaviors Before Training

**link**: https://arxiv.org/pdf/2602.04735.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: LLM可能从良性数据中习得意外偏见，现有方法难在训练前检测风险。提出Data2Behavior任务和操纵数据特征（MDF）方法，通过平均表示总结数据并注入基础模型前向传播，揭示潜在偏见。MDF消耗20% GPU资源，在Qwen3和Gemma模型上可靠预测意外行为。  

---

### 27 When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?

**link**: https://arxiv.org/pdf/2602.04755.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: LLM在时间问答中常忽略不确定性而生成误导答案。本文首次实证研究训练LLM弃权能力，将弃权视为可教授技能，结合思维链监督和弃权感知奖励的强化学习。实验显示RL在推理上显著收益：Qwen2.5-1.5B在TimeQA上超GPT-4o，但SFT导致过度自信，隐式推理线索益处有限。  

---

### 28 Representation Geometry as a Diagnostic for Out-of-Distribution Robustness

**link**: https://arxiv.org/pdf/2602.03951.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 分布偏移下的鲁棒泛化难监控。提出基于几何的诊断框架，从分布内嵌入构建类条件互k近邻图，提取全局谱复杂度和局部平滑度度量。在多种架构和基准上，较低谱复杂度和较高曲率一致预测更强OOD准确率，表示几何实现可解释、无标签的鲁棒性诊断。  

---

### 29 Non-linear PCA via Evolution Strategies: a Novel Objective Function

**link**: https://arxiv.org/pdf/2602.03967.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 主成分分析（PCA）因线性限制难以捕捉复杂数据结构。本文提出鲁棒非线性PCA框架，将PCA可解释性与神经网络灵活性结合。通过进化策略优化变量变换，引入细粒度目标函数最大化个体方差贡献。处理分类和序数变量无维度爆炸，实验显示在解释方差上显著优于线性和核PCA。  

---

### 30 SEIS: Subspace-based Equivariance and Invariance Scores for Neural Representations

**link**: https://arxiv.org/pdf/2602.04054.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 评估神经表示对几何变换的响应至关重要。现有方法通过比较输出评估鲁棒性，但对内部组织洞察有限。引入SEIS（基于子空间的等变性和不变性分数），分析层级特征表示的子空间度量，区分等变性和不变性。合成验证正确恢复变换，应用于分类网络揭示从早期层等变性到深层不变性的过渡。  

---

### 31 Online Vector Quantized Attention

**link**: https://arxiv.org/pdf/2602.03922.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 标准序列混合层在效率和性能间难以平衡。提出在线向量量化（OVQ）注意力，需线性计算和恒定内存，但使用稀疏内存更新增加状态大小。基于高斯混合回归的理论基础，在合成长上下文任务和语言建模上测试，OVQ-注意力优于线性基线，64k序列长度下与自注意力相当。  

---

### 32 CoRe: Context-Robust Remasking for Diffusion Language Models

**link**: https://arxiv.org/pdf/2602.04096.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 掩码扩散模型解码受上下文刚性限制，初始不一致误导生成。提出CoRe框架，通过探测令牌对上下文扰动的敏感性识别不稳定令牌，将修订形式化为鲁棒优化目标。在LLaDA-8B-Base上，CoRe在推理和代码基准上持续改进，MBPP提升达9.2个百分点。  

---

### 33 Multi-scale hypergraph meets LLMs: Aligning large language models for time series analysis

**link**: https://arxiv.org/pdf/2602.04369.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 现有方法未充分考虑自然语言和时间序列的多尺度结构。提出MSH-LLM，使用超边机制增强时间序列语义空间的多尺度信息，引入跨模态对齐模块对齐不同尺度模态，混合提示机制提供上下文。在27个数据集上，MSH-LLM实现最先进性能。  

---

### 34 Learning to Reason in 13 Parameters

**link**: https://arxiv.org/pdf/2602.04118.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 语言模型可通过强化学习学习推理。提出TinyLoRA方法，将低秩适配器缩至一个参数大小，仅用13个bf16参数在GSM8K上训练Qwen2.5到91%准确率。在更困难基准上，参数减少1000倍仍恢复90%性能提升，且仅RL能实现此性能。  

---

### 35 LORE: Jointly Learning the Intrinsic Dimensionality and Relative Similarity Structure From Ordinal Data

**link**: https://arxiv.org/pdf/2602.04192.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 从主观感知序数数据学习内在维度具挑战性。提出LORE框架，从噪声三元组比较中联合学习内在维度和序数嵌入。使用非凸Schatten-p准范数正则化，实现自动恢复。实验显示LORE学习紧凑、可解释的低维嵌入，恢复感知几何结构，为心理物理学和机器学习提供新方向。  

---

### 36 From Sparse Sensors to Continuous Fields: STRIDE for Spatiotemporal Reconstruction

**link**: https://arxiv.org/pdf/2602.04201.pdf  
**date**: 2026-02-05  
**keywords**: cs.LG  
**abs**: 从稀疏传感器测量重建高维时空场是学习PDE动力学的核心挑战。提出STRIDE框架：时序编码器映射测量到潜在状态，调制隐式神经表示解码器在任意位置重建场。使用FMMNN骨干，优于基于正弦的INR。理论证明在稳定延迟可观条件下，重建算子可分解。在混沌动力学和波传播基准上，STRIDE优于基线，支持超分辨率和噪声鲁棒。  

---

### 37 MTS-JEPA: Multi-Resolution Joint-Embedding Predictive Architecture for Time-Series Anomaly Prediction

**link**: https://arxiv.org/pdf/2602.04643.pdf  
**date**: 2026-02-05  
**keywords**: latent space  
**abs**: 多变量时间序列异常预测对主动风险缓解至关重要。提出MTS-JEPA架构，集成多分辨率预测目标和软码本瓶颈，解耦瞬态冲击与长期趋势，利用码本捕捉离散状态转换。标准基准上实证评估证实方法防止退化解，并在预警协议下实现最先进性能。  

---

### 38 Decomposing Query-Key Feature Interactions Using Contrastive Covariances

**link**: https://arxiv.org/pdf/2602.04752.pdf  
**date**: 2026-02-05  
**keywords**: latent space  
**abs**: 注意力头在Transformer中核心，但缺乏理解模型关注特定令牌的工具。研究查询-键（QK）空间，提出对比协方差方法分解QK空间为低秩、可解释组件。当特征在子空间中对齐时产生高注意力分数。在简化设置和LLM上验证，识别类别语义和绑定特征子空间，展示注意力分数归因。  

---

### 39 Generative Modeling via Drifting

**link**: https://arxiv.org/pdf/2602.04770.pdf  
**date**: 2026-02-05  
**keywords**: latent space  
**abs**: 生成建模可表述为学习映射f使前推分布匹配数据分布。提出漂移模型新范式，训练中演化前推分布，支持一步推理。引入漂移场控制样本移动，分布匹配时达到平衡。在ImageNet 256x256上，一步生成器FID达1.54，为高质量一步生成开辟机会。  

---

### 40 CoLT: Reasoning with Chain of Latent Tool Calls

**link**: https://arxiv.org/pdf/2602.04246.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 思维链（CoT）加速低效，潜在推理方法需模型结构增强和训练。提出CoLT框架，将潜在推理实现为“工具调用”：生成种子token包含推理信息，触发工具时外部模型解包为完整步骤。在数学数据集上，CoLT比基线潜在模型准确率更高、推理长度更短，兼容RL和不同解码器。  

---

### 41 Model-Dowser: Data-Free Importance Probing to Mitigate Catastrophic Forgetting in Multimodal Large Language Models

**link**: https://arxiv.org/pdf/2602.04509.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 微调多模态LLM（MLLM）导致预训练任务泛化能力下降。提出Model-Dowser方法，通过权重大小、输入激活和输出敏感性联合计算参数重要性分数，微调时选择性保留高重要性参数。在LLaVA和NVILA上，该方法有效缓解灾难性遗忘，优于现有方法且资源高效，可扩展至十亿参数模型。  

---

### 42 Textual Planning with Explicit Latent Transitions

**link**: https://arxiv.org/pdf/2602.04557.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: LLM规划受逐token生成限制。提出EmbedPlan，用轻量级转换模型替代自回归状态生成，将状态和动作编码为向量，预测下一个状态嵌入并通过最近邻检索。在九个规划领域评估，域内性能近完美，但跨域迁移是瓶颈。  

---

### 43 Mapping the Web of Science, a large-scale graph and text-based dataset with LLM embeddings

**link**: https://arxiv.org/pdf/2602.04630.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 本文通过嵌入方法研究约5600万篇科学出版物的Web of Science数据集，利用LLM嵌入处理文本特征，揭示文本自组织结构。展示结合文本语义和图结构特征在分类和预测任务中的潜力，为大规模数据分析提供实用框架。  

---

### 44 Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models

**link**: https://arxiv.org/pdf/2602.04355.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 工作记忆是智能核心，但视觉是否可替代文本在MLLM工作记忆中的作用不明。在空间n-back任务上评估Qwen2.5和Qwen2.5-VL，文本条件下准确率和d'值显著高于视觉条件。Trial-wise对数概率证据显示模型响应与近期性锁定一致，网格大小改变干扰模式，支持多模态工作记忆的计算敏感性解释。  

---

### 45 Fine-Grained Activation Steering: Steering Less, Achieving More

**link**: https://arxiv.org/pdf/2602.04428.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 激活引导修改LLM行为，但块级激活异质导致引导粗糙。将块激活分解为原子单元（AU）级激活，每个AU对应权重矩阵切片。提出AUSteer方法，通过对比样本识别判别性AU，分配自适应引导强度。实验显示AUSteer优于基线，引导更少激活，实现“引导更少，实现更多”。  

---

### 46 Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough

**link**: https://arxiv.org/pdf/2602.04489.pdf  
**date**: 2026-02-05  
**keywords**: cs.CL  
**abs**: 以花园路径句测试，提出跨眼动追踪、自定步速阅读等四种范式的联合潜在过程混合模型。模型区分花园路径概率、成本和再分析成本，考虑注意力不集中试验。在人类阅读数据上，混合模型比基于GPT-2 surprisal的无混合模型拟合更好，提供对LLM处理句法的深入见解。  

---

### 47 The Key to State Reduction in Linear Attention: A Rank-based Perspective

**link**: https://arxiv.org/pdf/2602.04852.pdf  
**date**: 2026-02-05  
**keywords**: latent space  
**abs**: 线性注意力为softmax注意力提供高效替代，但训练后状态常低秩。理论分析揭示低秩通过放大查询噪声影响检索误差。提出硬件感知方法，通过结构化剪枝键和查询矩阵减小状态大小。基于秩揭示QR分解提出新型剪枝方法，实验显示移除50%通道仅轻微增加困惑度。  

---

### 48 Rethinking Weight Tying: Pseudo-Inverse Tying for Stable LM Training and Updates

**link**: https://arxiv.org/pdf/2602.04556.pdf  
**date**: 2026-02-05  
**keywords**: LLM Memory  
**abs**: 权重绑定减少参数，但训练中令牌接口可能漂移。提出伪逆绑定（PIT），将嵌入和非嵌入同步为耦合投影，确保接口伪逆一致。维护正交共享内存，引入对称正定隐藏空间变换。在设备端模型上，PIT提高训练稳定性、语义一致性和减少副作用。  

---

### 49 Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging

**link**: https://arxiv.org/pdf/2602.04731.pdf  
**date**: 2026-02-05  
**keywords**: LLM Rec  
**abs**: 基于LLM的检索器在RAG中表现优，但适应生物医学领域未被充分探索。提出合成-训练-合并（STM）框架，通过合成难负例、提示优化和模型合并增强仅解码器LLM。在MTEB医疗任务上，STM将专家性能提高达23.5%，合并模型优于单一专家和基线，保留通用能力。  

---

### 50 Reinforced Attention Learning

**link**: https://arxiv.org/pdf/2602.04884.pdf  
**date**: 2026-02-05  
**keywords**: latent reasoning  
**abs**: 强化学习（RL）后训练通过测试时扩展提升LLM推理能力，但扩展到多模态LLM（MLLM）时对感知能力提升有限。本文未提供完整摘要，需参考原文。  

---

### 51 Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models

**link**: https://arxiv.org/pdf/2602.04649.pdf  
**date**: 2026-02-05  
**keywords**: latent reasoning  
**abs**: 生成式奖励模型（GenRM）和LLM-as-a-Judge优先结果准确性，但错误推理产生正确判断导致欺骗性对齐。引入“推理一致性”指标量化推理与人类判断对齐。提出混合信号训练GenRM，结合推理一致性和结果准确性。在RM-Bench和JudgeBench上实现最先进性能，RLHF中提升创意写作任务7%，避免欺骗性对齐。  

---

### 52 CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation

**link**: https://arxiv.org/pdf/2602.04856.pdf  
**date**: 2026-02-05  
**keywords**: latent reasoning  
**abs**: LLM评估假设拒绝响应即安全，但虚假新闻生成中，思维链（CoT）推理内部可能传播不安全叙事。引入安全分析框架解构CoT生成，通过雅可比矩阵谱度量评估注意力头作用。提出稳定性、几何结构和能量度量，实验显示激活思考模式时生成风险上升，关键路由决策集中在中层深度。