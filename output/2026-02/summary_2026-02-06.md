### 1 Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models

**link**: https://arxiv.org/pdf/2602.04931.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 本文表明，仅解码器大型语言模型（LLMs）在计算过程中表现出从上下文处理到预测形成阶段的深度转变，并伴随着表征几何结构的重组。通过结合几何分析与机制干预的统一框架，作者证明深层表征实现了一种结构化几何编码，能够对令牌预测进行选择性因果控制。具体而言，表征几何的角度组织参数化了预测分布的相似性，而表征范数则编码了不决定预测的上下文特定信息。这些结果共同为LLMs中上下文转化为预测的动态过程提供了机制-几何解释。

---

### 2 Privileged Information Distillation for Language Models

**link**: https://arxiv.org/pdf/2602.04942.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 训练时的特权信息（PI）能使语言模型在原本无法完成的困难长 horizon 任务中取得成功，这使其成为强化学习在复杂环境中的强大工具。然而，将利用PI学到的能力迁移到推理时必须在无PI条件下行动的策略，仍是一个基本挑战。本文在多轮智能体环境中研究前沿模型的蒸馏问题，其中闭源系统通常隐藏内部推理过程，仅暴露动作轨迹，这打破了标准蒸馏流程（因为成功行为可观测但推理过程不可见）。为此，作者提出π-Distill——一种联合教师-学生目标，使用同一模型同时训练PI条件的教师和无条件的学生。此外，还提出反向KL惩罚的强化学习（RL）方法On-Policy Self-Distillation（OPSD）。实验表明，这两种算法能有效利用仅动作PI蒸馏前沿智能体，在多个智能体基准、模型和PI形式上，π-Distill（有时OPSD）优于假设可获取完整思维链监督的行业标准实践（监督微调后接RL）。作者还通过 extensive 分析表征了实现PI有效学习的关键因素。

---

### 3 Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability

**link**: https://arxiv.org/pdf/2602.04902.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 该研究将Transformer视为物理电路，通过引入动量注意力（Momentum Attention）扩展其计算图，整合守恒定律和时变AC动力学。提出辛增强方法，利用运动学差分算子实现查询和键的辛剪切，发现辛滤波对偶性（物理剪切等效于高通滤波器），从而绕过拓扑深度约束实现单层次归纳。通过5100+对照实验验证，125M动量模型在归纳任务上表现优异，同时跟踪350M基线模型的验证损失在2.9%以内。该框架连接生成式AI、哈密顿物理和信号处理，为机制可解释性提供新分析工具。

---

### 4 Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering

**link**: https://arxiv.org/pdf/2602.04903.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 特征引导通过直接操纵内部表示控制LLM行为，但其实际应用效果和性能权衡尚未明确。该研究评估Goodfire的Auto Steer方法与提示工程基线，在14个引导查询（含无害和安全相关行为）和171个MMLU问题上测试Llama-8B和Llama-70B。结果显示，Auto Steer成功修改目标行为（Llama-8B得分3.33 vs 提示工程2.98，Llama-70B 3.57 vs 3.10），但导致性能显著下降：Llama-8B的MMLU准确率从66%降至46%，Llama-70B从87%降至73%，连贯性也大幅降低。研究指出当前特征引导方法存在能力-行为权衡，实际部署需谨慎。

---

### 5 Laplacian Representations for Decision-Time Planning

**link**: https://arxiv.org/pdf/2602.05031.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 本文探讨了模型基强化学习中基于学习模型的规划挑战。在决策时规划中，状态表示需支持局部成本计算并保留长horizon结构，至关重要。研究表明，拉普拉斯表示通过捕获多时间尺度的状态空间距离，为规划提供了有效的latent space。该表示保留有意义的距离，能自然将长horizon问题分解为子目标，并减轻长预测horizon中出现的累积误差。基于这些特性，作者引入分层规划算法ALPS，在OGBench的离线目标条件RL任务上证明其优于常用基线。

---

### 6 Causal Representation Meets Stochastic Modeling under Generic Geometry

**link**: https://arxiv.org/pdf/2602.05033.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 本文致力于从观测中学习有意义的因果表示，这对促进机器学习应用及推动气候科学、生物学、物理学等领域的科学发现至关重要。该过程涉及从低层次观测中解耦高层次潜变量（latent variables）及其因果关系。先前可识别性研究多关注观测为独立同分布或遵循潜离散时间过程的情况，而许多现实场景需识别连续时间随机过程（如多元点过程）的潜变量。为此，作者开发了连续时间潜随机点过程的可识别因果表示学习，通过分析参数空间几何结构研究其可识别性，并提出MUTATE框架（一种带时间自适应转换模块的可识别变分自编码器）以推断随机动力学。在模拟和实证研究中，MUTATE能有效解答科学问题，如基因组学中的突变积累及神经元对时变动力学的尖峰触发机制。

---

### 7 Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution

**link**: https://arxiv.org/pdf/2602.04918.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLMs）经常优先考虑冲突的上下文信息而非预先存在的参数化记忆，这种现象通常被称为谄媚或顺从。然而，这种行为的机制实现仍然模糊不清，具体而言，模型如何通过顺从解决这些知识冲突，以及这种抑制是源于信号幅度稀释还是残差流中的方向几何改变。为解决这一问题，我们在Qwen-4B、Llama-3.1-8B和GLM-4-9B模型上进行了分层几何分析，将反事实上下文引起的残差流更新分解为径向（基于范数）和角度（基于余弦）分量。我们的实证结果否定了“流形稀释”假设的普遍性，因为三个架构中的两个尽管在事实查询上表现出显著的性能下降，但仍保持了稳定的残差范数。相反，我们观察到顺从始终以“正交干扰”为特征，即冲突上下文注入一个与真实方向准正交的引导向量，有效地旋转隐藏状态表示。这表明模型不会“遗忘”或抑制内部事实的幅度，而是采用几何位移机制绕过正确的解嵌入向量，有效地模拟采纳同时保留原始结构幅度。这些发现挑战了用于检测幻觉的标量置信度指标，并强调了区分真正知识整合与表面上下文模仿需要向量监测的必要性。

---

### 8 CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning

**link**: https://arxiv.org/pdf/2602.04920.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 多模态机器学习模仿人脑整合各种模态的能力，发展迅速。大多数先前的多模态模型在完美配对的多模态输入上训练以达到最佳性能。然而，在实际部署中，模态的存在高度可变且不可预测，导致预训练模型在动态缺失模态情况下性能显著下降且无法保持稳健性。在本文中，我们提出了一种新颖的循环信息学习框架（CyIN），以弥合完整和不完整多模态学习之间的差距。具体而言，我们首先通过在各种模态之间循环采用令牌级和标签级信息瓶颈（IB）来构建信息潜空间。通过变分近似捕获任务相关特征，信息瓶颈潜变量被净化以实现更高效的跨模态交互和多模态融合。此外，为了补充不完整多模态输入导致的缺失信息，我们提出跨模态循环翻译，通过正向和反向传播过程利用剩余模态重建缺失模态。借助提取和重建的信息潜变量，CyIN成功地在一个统一模型中联合优化完整和不完整的多模态学习。在4个多模态数据集上的大量实验表明，我们的方法在完整和各种不完整场景中均表现出优越性能。

---

### 9 Internalizing LLM Reasoning via Discovery and Replay of Latent Actions

**link**: https://arxiv.org/pdf/2602.04925.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 将思维链过程内化到隐藏状态中已成为扩展测试时计算的高效范式。然而，现有的激活引导方法依赖静态控制向量，无法适应复杂推理任务的非平稳演化。为解决这一限制，我们提出STIR（Self-Distilled Tools for Internal Reasoning）框架，将推理增强重新表述为动态潜轨迹控制问题。STIR引入协同三阶段管道：（1）差分内在动作诱导收集潜推理成功案例以明确引导原语；（2）稀疏控制基构建精心挑选紧凑、几何多样的工具库；（3）价值调制轨迹干预通过基于锚点的门控动态注入上下文特定脉冲。在四个代表性模型的六个算术和逻辑基准上的大量实验表明，STIR将平均准确率提高了1.9%至7.5%，同时减少了高达35%的平均令牌消耗。这些发现表明，显式思维链的优势可以通过动态潜轨迹控制实现，将推理过程内化以绕过显式生成，同时实现更高的保真度。

---

### 10 Adaptive Exploration for Latent-State Bandits

**link**: https://arxiv.org/pdf/2602.05139.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 多臂老虎机问题是序贯决策的核心框架，但经典算法在具有隐藏、时变状态的环境中常失效，这些状态会混淆奖励估计和最优动作选择。本文通过引入一类无状态模型的老虎机算法来解决未观测混杂因素带来的挑战（如奖励估计偏差和状态信息有限），这些算法利用滞后上下文特征和协调探测策略，隐式跟踪潜在状态并区分状态依赖的奖励模式。所提方法及其自适应变体无需显式状态建模即可学习最优策略，兼具计算效率和对非平稳奖励的鲁棒适应性。不同设置下的实证结果表明其性能优于经典方法，并为实际应用提供了算法选择建议。

---

### 11 EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization

**link**: https://arxiv.org/pdf/2602.05165.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 基于可验证奖励的强化学习（RLVR）已被证明能有效增强大型语言模型（LLMs）的推理能力。然而，如组相对策略优化（GRPO）等主流方法面临关键稳定性挑战：在计算约束下（小组规模）估计器方差高，且在所有响应均产生相同零奖励的饱和失败状态中梯度信号消失。为此，本文提出经验贝叶斯策略优化（EBPO），通过借鉴策略积累的全局统计信息来正则化局部基于组的基线。EBPO不孤立估计基线，而是采用收缩估计器，动态平衡局部组统计与通过Welford在线算法更新的全局先验。理论上，EBPO保证比GRPO更低的均方误差、有界熵衰减，并在失败场景中提供非消失的惩罚信号。实证上，EBPO在AIME和OlympiadBench等多种基准上持续优于GRPO和其他已建立的基线，尤其在小组规模下表现出优异的训练稳定性，并从难度分层课程学习中显著受益。

---

### 12 Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction

**link**: https://arxiv.org/pdf/2602.05269.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLMs）在边缘设备上的部署受到“内存墙”的根本限制——这是一种硬件限制，其中内存带宽而非计算成为瓶颈。最近的1.58位量化技术（如BitNet b1.58）显著减少了内存占用，但与FP16基线相比，通常会导致20-25%的困惑度下降。在这项工作中，我们引入混合门控流（HGF），这是一种双流架构，将1.58位三元主干与由自适应门控制的可学习低秩FP16校正路径相结合。

---

### 13 Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities

**link**: https://arxiv.org/pdf/2602.05281.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 强化学习与可验证奖励（RLVR）已成为增强大型语言模型（LLMs）推理能力的不可或缺的范式。然而，标准的策略优化方法（如组相对策略优化GRPO）往往收敛到低熵策略，导致严重的模式崩溃和有限的输出多样性。我们从采样概率动态的角度分析此问题，发现标准目标不成比例地强化最高似然路径，从而抑制有效的替代推理链。为解决这一问题，我们提出一种新的优势重加权机制（ARM），旨在平衡所有正确响应的置信水平。通过将提示困惑度和答案置信度纳入优势估计，我们的方法动态重塑奖励信号，以减弱过度自信推理路径的梯度更新，同时将概率质量重新分配给未充分探索的正确解决方案。实证结果表明，我们的方法显著提高了生成多样性和响应熵，同时保持了有竞争力的准确性，有效实现了推理任务中探索与利用之间的优越权衡。在Qwen2.5和DeepSeek模型上的数学和编码基准测试结果显示，ProGRPO显著缓解了熵崩溃。具体而言，在Qwen2.5-7B上，我们的方法在Pass@1上优于GRPO 5.7%，尤其在Pass@32上优于13.9%，突出了其生成多样化正确推理路径的卓越能力。

---

### 14 Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning

**link**: https://arxiv.org/pdf/2602.05183.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 本文针对基于LLM的多智能体强化学习训练过程中的行为可解释性问题，提出了数据中心的解释框架。通过应用预训练的稀疏自编码器（SAE）和LLM总结器方法，分析了Full-Press Diplomacy环境中的大规模强化学习训练。引入Meta-Autointerp方法将SAE特征分组为关于训练动态的可解释假设，发现了角色扮演模式、退化输出、语言切换等细粒度行为，以及高层战略行为和环境特定漏洞。实验验证90%的SAE元特征具有显著性，并发现奖励黑客行为。用户研究表明部分SAE衍生假设对下游任务具有预测价值，通过增强未训练智能体的系统提示，分数提升14.2%。该研究展示了SAE和LLM总结器在LLM行为解释中的互补作用，为确保训练过程中LLM行为的可信赖性提供了数据中心解释的实用起点。

---

### 15 Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs

**link**: https://arxiv.org/pdf/2602.05191.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 针对长上下文LLM推理中注意力计算的瓶颈问题，本文提出Double-P分层Top-P稀疏注意力框架。现有Top-P方法难以同时优化精度、选择开销和稀疏注意力成本，而Double-P通过两级优化解决该问题：首先在簇级别使用大小加权质心进行粗粒度Top-P估计，然后通过第二阶段Top-P自适应精化计算，仅在需要时分配令牌级注意力。在长上下文基准测试中，Double-P实现近零精度损失，将注意力计算开销降低1.8倍，端到端解码速度提升1.3倍，为长上下文LLM的高效推理提供了新方案。

---

### 16 Disentangled Representation Learning via Flow Matching

**link**: https://arxiv.org/pdf/2602.05214.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 本文提出基于流匹配的解纠缠表示学习框架，将解纠缠问题转化为在紧凑潜在空间中学习因子条件流。为实现显式语义对齐，引入非重叠（正交）正则化器抑制因子间干扰，减少因子间信息泄漏。该方法通过流匹配在潜在空间构建因子条件流，克服了现有扩散方法依赖归纳偏置导致语义对齐不足的问题。在多个数据集上的实验表明，该框架在解纠缠分数、可控性和样本保真度方面持续优于代表性基线方法。

---

### 17 Mechanisms of AI Protein Folding in ESMFold

**link**: https://arxiv.org/pdf/2602.06020.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 本文研究了AI蛋白质折叠模型ESMFold的工作机制，通过追踪其折叠β发夹（一种常见结构基序）的过程展开。借助对模型潜变量的反事实干预，作者识别出折叠主干中的两个计算阶段：第一阶段，早期模块初始化成对生化信号（如残基身份及相关生化特征从序列表示流入成对表示）；第二阶段，晚期模块发展成对空间特征（距离和接触信息在成对表示中积累）。研究表明，ESMFold的结构决策机制可被定位、通过可解释表示追踪，并能以强烈的因果效应进行操控。

---

### 18 CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs

**link**: https://arxiv.org/pdf/2602.05258.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 旋转位置嵌入（RoPE）是大型语言模型（LLMs）中上下文扩展的关键组件。尽管已有多种方法将RoPE适配到更长的上下文，但其指导原则通常分为两类：（1）分布外（OOD）缓解，通过缩放RoPE频率以适应未见过的位置；（2）语义建模，即RoPE计算的注意力分数应始终优先考虑语义相似的标记。本文通过一种极简干预方法CoPE（对RoPE的低频成分进行软裁剪）统一了这些看似不同的目标。CoPE不仅消除了OOD异常值并优化了语义信号，还防止了硬裁剪导致的频谱泄漏。大量实验表明，将软裁剪策略应用于RoPE可显著提升性能，上下文长度可扩展至256k，验证了理论分析并确立CoPE为长度泛化的新最先进方法。

---

### 19 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents

**link**: https://arxiv.org/pdf/2602.05810.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 自主智能体通过反思和迭代改进实现自我提升，重用成功的任务轨迹作为上下文示例辅助后续推理。然而，任务切换常导致上下文不匹配，现有方法要么丢弃轨迹，要么使用启发式操作，导致不可忽视的微调成本或性能不稳定。为解决此问题，本文揭示了上下文-轨迹相关性，即上下文变化与轨迹变化高度平行。基于此，提出Bifrost方法，利用上下文差异精确引导已解决轨迹适应目标任务，减轻上下文偏移引起的错位。轨迹适应在表示层面使用智能体隐藏状态进行，确保轨迹转换在共享空间中准确对齐目标上下文。在多个基准测试中，Bifrost持续优于现有轨迹重用和微调自改进方法，表明智能体可有效利用过去经验，即使存在显著上下文变化。

---

### 20 DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders

**link**: https://arxiv.org/pdf/2602.05859.pdf  
**date**: 2026-02-06  
**keywords**: cs.LG  
**abs**: 稀疏自编码器（SAEs）是自回归大语言模型（LLMs）机制可解释性的标准工具。随着扩散语言模型（DLMs）成为LLMs的替代方案，本文提出DLM-Scope，首个基于SAE的DLMs可解释性框架。研究发现，在DLMs早期层插入SAEs可降低交叉熵损失，SAE特征支持更有效的扩散时间干预，优于LLM引导。此外，SAEs可为DLM解码顺序提供信号，且在训练后阶段稳定。该工作为DLMs的机制可解释性奠定基础，展示了SAEs在DLM相关任务中的潜力。

---

### 21 Optimal scaling laws in learning hierarchical multi-index models

**link**: https://arxiv.org/pdf/2602.05846.pdf  
**date**: 2026-02-06  
**keywords**: stat.ML  
**abs**: 本文研究了在表示受限的情况下，两层神经网络在分层多索引目标上训练时的缩放定律理论。作者推导了子空间恢复和预测误差的精确信息论缩放定律，揭示了目标的层次特征如何通过一系列相变被顺序学习。进一步表明，这些最优速率可通过简单的目标无关谱估计器实现，该估计器可解释为第一层权重梯度下降的小学习率极限。一旦识别出适配的表示，读出层可通过高效过程实现统计最优学习。因此，本文为浅层神经网络在这类分层目标上训练时的缩放定律、平台现象和谱结构提供了统一且严格的解释。

---

### 22 Beyond Cosine Similarity

**link**: https://arxiv.org/pdf/2602.05266.pdf  
**date**: 2026-02-06  
**keywords**: latent space  
**abs**: 余弦相似度作为向量空间中语义相似性的标准度量，受限于柯西-施瓦茨不等式，仅能捕捉线性关系，无法建模现实世界语义空间的复杂非线性结构。本文通过推导比经典柯西-施瓦茨界更紧的点积上界，提出了一种新的相似度度量recos，它通过排序向量分量来归一化点积。recos将完美相似性的条件从严格线性相关放宽为序数一致性，从而捕捉更广泛的关系类型。在11种嵌入模型（包括静态、上下文和通用类型）上的实验表明，recos持续优于传统余弦相似度，在标准语义文本相似性（STS）基准上与人类判断的相关性更高。该工作确立了recos作为一种数学上合理且经验上更优的替代方案，为复杂嵌入空间中的语义分析提供了更高的准确性。

---

### 23 Aspect-Aware MOOC Recommendation in a Heterogeneous Network

**link**: https://arxiv.org/pdf/2602.05297.pdf  
**date**: 2026-02-06  
**keywords**: LLM Rec  
**abs**: MOOC推荐系统旨在帮助学习者导航和选择偏好的学习内容。传统方法如协同过滤和基于内容的过滤存在数据稀疏和过度特化问题。图基方法虽被提出，但依赖手动预定义元路径，仅捕捉表面结构关系且需大量领域专家工作。为此，本文提出AMR（Aspect-aware MOOC Recommendation）框架，通过在每个元路径中嵌入节点语义内容来建模路径特定的多个方面。AMR通过双向游走自动发现元路径，使用基于bi-LSTM的编码器导出方面感知路径表示，并将这些表示作为学习者-学习者和KC-KC子图中的边特征，实现细粒度语义感知的KC推荐。在大规模MOOCCube和PEEK数据集上的实验表明，AMR在HR@K和nDCG@K等关键指标上持续优于最先进的图神经网络基线，进一步分析证实其有效捕捉了丰富的路径特定方面信息，推荐准确性高于仅依赖预定义元路径的方法。

---

### 24 Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space

**link**: https://arxiv.org/pdf/2602.05971.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 本文将语义表征构建为结构化的动态知识空间，提出通过嵌入空间轨迹来表征人类概念生成过程中的语义导航。研究使用不同的Transformer文本嵌入模型，基于累积嵌入构建参与者特定的语义轨迹，并提取几何和动态指标（如距离、熵、速度、加速度等），以捕捉语义导航的标量和方向特征。该框架在四种跨语言数据集（神经退行性疾病、脏话流畅性、意大利语和德语属性列举任务）上进行了评估，能够有效区分临床组和概念类型，且相比传统语言预处理方法减少了人工干预。研究还发现累积嵌入对较长轨迹更有效，而不同嵌入模型虽训练流程不同但结果相似。此工作通过将语义导航构建为嵌入空间中的结构化轨迹，架起了认知建模与学习表征之间的桥梁，可应用于临床研究、跨语言分析和人工认知评估。

---

### 25 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory

**link**: https://arxiv.org/pdf/2602.06025.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 针对大型语言模型（LLM）智能体在超出单上下文窗口场景下的内存挑战，本文提出BudgetMem框架，一种运行时智能体内存系统，支持显式的查询感知性能-成本控制。该框架将内存处理结构化为一组内存模块，每个模块提供低/中/高三种预算层级，并通过轻量级路由器在模块间进行预算层级路由，以平衡任务性能和内存构建成本。路由器实现为基于强化学习训练的紧凑神经策略。研究探索了三种预算层级实现策略：实现复杂度、推理行为和容量（模块模型大小）。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明，当优先考虑性能（高预算设置）时，BudgetMem超越了强基线模型；在预算受限情况下，能提供更优的精度-成本边界。分析还揭示了不同层级策略的优缺点，明确了各策略在不同预算条件下的最优权衡场景。

---

### 26 ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation

**link**: https://arxiv.org/pdf/2602.05472.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLMs）在追求专家级推理能力时受到持续的“奖励瓶颈”限制：传统强化学习（RL）依赖于标量奖励，这种奖励在扩展时成本高昂、跨领域脆弱且无法洞察解决方案的底层逻辑。这种对外部、贫乏信号的依赖阻碍了模型深入理解推理原理。本文介绍ALIVE（Adversarial Learning with Instructive Verbal Evaluation），一种无需人工干预的对齐框架，它超越标量奖励优化，转向内在推理能力的获取。基于“认知协同”原则，ALIVE将问题提出、解决和判断统一在单个策略模型中，以内化正确性逻辑。通过将对抗性学习与指导性语言反馈相结合，ALIVE使模型能够直接从原始语料中内化评估标准，有效将外部批评转化为内生推理能力。在数学推理、代码生成和一般逻辑推理基准上的实证评估表明，ALIVE持续缓解奖励信号的局限性。在相同数据和计算资源下，它实现了准确率提升、显著改善的跨领域泛化能力和更高的自我修正率。这些结果表明，推理三元组（提出、解决、判断）促进了能力增长的自我维持轨迹，使ALIVE成为无需人工监督的通用推理对齐的可扩展基础。

---

### 27 Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better

**link**: https://arxiv.org/pdf/2602.05393.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 随着大型语言模型（LLMs）通过扩大模型和数据规模取得显著的实证成功，预训练变得日益关键但计算成本高昂，阻碍了快速发展。尽管已有许多经过大量计算资源训练的预训练LLM，但一个基本的现实问题仍未得到充分探索：我们能否利用现有的小型预训练模型来加速大型模型的训练？本文提出了一种“迟至早训练”（LET）范式，使LLM能够在早期步骤和早期层中明确学习后期知识。其核心思想是在LLM训练早期，利用预训练模型（即处于训练后期阶段）的深层表示来指导目标LLM的早期层。我们确定了驱动LET有效性的两个关键机制：迟至早步骤学习和迟至早层学习。这些机制显著加速了训练收敛，同时稳健地增强了语言建模能力和下游任务性能，实现了更快的训练和更优的性能。在1.4B和7B参数模型上的大量实验证明了LET的效率和有效性。值得注意的是，在Pile数据集上训练1.4B LLM时，我们的方法相比标准训练实现了高达1.6倍的加速，下游任务准确率提升近5%，即使使用参数数量仅为目标模型1/10的预训练模型也是如此。

---

### 28 Optimal Bayesian Stopping for Efficient Inference of Consistent LLM Answers

**link**: https://arxiv.org/pdf/2602.05395.pdf  
**date**: 2026-02-06  
**keywords**: stat.ML  
**abs**: 提高LLM准确性（尤其是在数学和推理问题中）的一种简单策略是采样多个响应并提交最一致的答案。本文利用贝叶斯先验信息来节省采样成本，一旦达到足够的一致性就停止采样。尽管精确的后验分布在计算上难以处理，但我们进一步引入了一种高效的“L-聚合”停止策略，该策略仅跟踪L-1个最频繁的答案计数。理论上，我们证明L=3就足够了：这种粗略的近似足以实现渐近最优性，并且严格优于无先验基线，同时具有快速的后验计算。实证上，该方法使用更少的样本识别出最一致（即众数）的LLM答案，并且在将LLM调用次数减少高达50%（即节省LLM推理成本）的同时，能够达到相似的答案准确性。

---

### 29 Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning

**link**: https://arxiv.org/pdf/2602.05464.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 条件表示学习旨在为定制任务提取特定标准的特征。最近的研究通过将通用特征投影到由LLM生成的文本基张成的条件特征子空间上来获取条件表示。然而，此类方法面临两个关键限制：对子空间基的敏感性和对跨子空间干扰的脆弱性。为解决这些挑战，我们提出OD-CRL，这是一个集成了自适应正交基优化（AOBO）和零空间去噪投影（NSDP）的新框架。具体而言，AOBO通过带有曲率基截断的奇异值分解构建正交语义基；NSDP通过将嵌入投影到无关子空间的零空间来抑制非目标语义干扰。在定制聚类、定制分类和定制检索任务上进行的大量实验表明，OD-CRL实现了新的最先进性能，并具有优异的泛化能力。

---

### 30 Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities

**link**: https://arxiv.org/pdf/2602.05532.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 检测大型语言模型中的错位具有挑战性，因为模型在训练过程中可能学会隐藏不当行为。标准审计技术存在不足：黑盒方法通常无法区分错位输出与良性输出，而机制可解释性无法随模型能力扩展。我们引入分裂人格训练（SPT），将第二个“诚实人格”微调为在正常操作中保持非活动状态的LoRA参数。在主模型响应后，我们激活LoRA适配器并插入触发字符串，使诚实人格能够在访问主模型潜在状态的同时审查响应。我们在Anthropic审计游戏模型有机体上测试了我们的方法，这是一个基准，其中Llama-3.3-70B被训练来利用奖励漏洞同时隐藏这种行为。SPT实现了96%的总体准确率，而Anthropic报告的准确率接近0%。诚实人格揭示了外部观察者无法获取的潜在知识，例如受损模型所训练的虚构偏见。

---

### 31 Multi-Task GRPO: Reliable LLM Reasoning Across Tasks

**link**: https://arxiv.org/pdf/2602.05547.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 基于强化学习（RL）的GRPO后训练被广泛用于在单个推理任务上改进大型语言模型。然而，现实世界部署需要跨不同任务的可靠性能。GRPO的直接多任务适应通常会导致不平衡结果，部分任务主导优化而其他任务停滞。此外，任务在提示产生零优势（从而零梯度）的频率上差异很大，这进一步扭曲了它们对优化信号的有效贡献。为解决这些问题，我们提出了一种新的多任务GRPO（MT-GRPO）算法：（i）动态调整任务权重以明确优化最差任务性能并促进跨任务的平衡进展；（ii）引入比率保持采样器以确保任务明智的策略梯度反映调整后的权重。在3任务和9任务设置上的实验表明，MT-GRPO在最差任务准确率上始终优于基线。特别是，MT-GRPO相比标准GRPO和DAPO在最差任务性能上分别实现了16-28%和6%的绝对提升，同时保持了有竞争力的平均准确率。此外，在3任务设置中，MT-GRPO需要少50%的训练步骤即可达到50%的最差任务准确率，证明在实现跨任务可靠性能方面效率显著提高。

---

### 32 Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation

**link**: https://arxiv.org/pdf/2602.05544.pdf  
**date**: 2026-02-06  
**keywords**: LLM Rec, cs.AI  
**abs**: 大型语言模型（LLMs）在可解释推荐系统中展现出潜力，但忽视了协同信号，而现有方法将推荐和解释视为独立任务，导致内存占用问题。本文提出RGCF-XRec，这是一种混合框架，将推理引导的协同过滤（CF）知识引入语言模型，以单步方式提供可解释的序列推荐。理论基础和实证结果表明，RGCF-XRec相比领先的CF感知LLM方法具有三个关键优势：（1）通过上下文提示进行推理引导的CF知识增强，以发现潜在偏好和可解释的推理路径；（2）基于连贯性、完整性、相关性和一致性四个维度的高效评分机制，以减轻噪声CF推理轨迹并保留高质量解释；（3）编码协同和语义信号的统一表示学习网络，使结构化提示能够调节LLM以实现可解释的序列推荐。RGCF-XRec在Amazon数据集（Sports、Toys、Beauty，包含642,503个用户-项目交互）上表现出一致的改进，在Sports和Toys中HR@10分别提高7.38%和4.59%，ROUGE-L分别提高8.02%和3.49%。它缩小了冷启动和热启动性能差距，在冷启动场景中整体提升14.5%，热启动场景中提升11.9%，并在Beauty和Toys的零样本HR@5中分别提高18.54%和23.16%，突出了有效的泛化性和鲁棒性。此外，RGCF-XRec使用轻量级LLaMA 3.2-3B骨干实现训练效率，确保了实际应用的可扩展性。

---

### 33 Graph-based Agent Memory: Taxonomy, Techniques, and Applications

**link**: https://arxiv.org/pdf/2602.05665.pdf  
**date**: 2026-02-06  
**keywords**: LLM Memory, cs.AI  
**abs**: 记忆是基于大型语言模型（LLM）的智能体完成长程复杂任务（如多轮对话、游戏、科学发现）的核心模块，能够实现知识积累、迭代推理和自我进化。在各种范式中，图结构因其内在的关系依赖建模、层次化信息组织和高效检索能力，成为智能体记忆的有力结构。本文从图结构视角对智能体记忆进行了全面综述：首先，介绍了智能体记忆的分类法，包括短期与长期记忆、知识与经验记忆、非结构化与结构化记忆，并从实现角度探讨了图基记忆；其次，根据智能体记忆的生命周期，系统分析了图基智能体记忆的关键技术，涵盖将数据转换为记忆内容的记忆提取、高效组织数据的存储、从记忆中检索相关内容以支持推理的检索，以及更新记忆内容的进化；最后，总结了支持自进化智能体记忆开发和评估的开源库与基准测试，并探讨了多样化的应用场景，同时指出了关键挑战和未来研究方向。本综述旨在为推进更高效、可靠的图基智能体记忆系统发展提供可操作的见解。

---

### 34 Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions

**link**: https://arxiv.org/pdf/2602.05709.pdf  
**date**: 2026-02-06  
**keywords**: latent space, cs.AI  
**abs**: 低秩适应（LoRA）通过两个低秩矩阵的乘积来近似预训练权重矩阵的更新，但标准LoRA遵循显式秩范式，增加模型容量需要向低秩矩阵添加更多行或列（即基向量），导致参数显著增长。本文发现这些基向量存在显著的参数冗余，并且可以通过轻量级非线性函数紧凑表示。因此，提出生成式低秩适配器（GenLoRA），将显式基向量存储替换为非线性基向量生成：为每个低秩矩阵维护一个潜向量，并采用一组轻量级径向基函数（RBFs）来合成基向量。每个RBF所需的参数远少于显式基向量，从而使GenLoRA具有更高的参数效率。多数据集和架构上的广泛实验表明，GenLoRA在更小的参数预算下实现了更高的有效LoRA秩，从而获得更优的微调性能。

---

### 35 NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking

**link**: https://arxiv.org/pdf/2602.05805.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 大型语言模型在推理计算中越来越多地采样多条思维链轨迹或搜索合并的检查点，这将瓶颈从生成转移到选择，且通常缺乏对目标分布的监督。本文表明基于熵的探索代理与准确性呈倒U形关系，过多探索可能冗余并导致过度思考。提出NEX，一种白盒无标签无监督评分框架，将推理视为交替的E阶段（探索）和X阶段（利用）。NEX通过稀疏激活缓存中每个标记新激活的MLP神经元峰值检测E阶段，然后使用粘性两态HMM推断E-X阶段，并根据E阶段引入的神经元在后续X跨度中是否被重用来对其进行评分。这些信号产生可解释的神经元权重和单一的Good-Mass Fraction分数，用于在无任务答案的情况下对候选响应和合并变体进行排名。在推理基准和Qwen3合并系列上，基于少量无标签激活集计算的NEX可预测下游准确性并识别更好的变体；通过人类注释验证了E-X信号，并通过“有效vs冗余”神经元转移提供了因果证据。

---

### 36 TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning

**link**: https://arxiv.org/pdf/2602.05818.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 时间知识图谱问答（TKGQA）旨在通过利用时间知识库回答时间敏感问题。尽管大型语言模型（LLMs）在TKGQA中显示出巨大潜力，但当前的提示策略在两个主要方面限制了其有效性：一是在复杂时间约束下容易产生推理幻觉；二是静态提示限制了模型的自主性和泛化能力，因为它缺乏通过与时间知识图谱（TKGs）环境动态交互进行的优化。为解决这些限制，本文提出TKG-Thinker，一种具备自主规划和自适应检索能力的智能体，用于在TKGs上进行推理。具体而言，TKG-Thinker通过双训练策略（首先使用思维链数据进行监督微调（SFT）以灌输核心规划能力，然后通过强化学习（RL）阶段利用多维奖励在复杂时间约束下优化推理策略），通过与TKGs的动态多轮交互进行深入的时间推理。实验结果表明，在三个开源LLMs的基准数据集上，TKG-Thinker实现了最先进的性能，并在复杂TKGQA设置中表现出强大的泛化能力。

---

### 37 A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges

**link**: https://arxiv.org/pdf/2602.05883.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLMs）已迅速成为研究人员和从业者的常用工具。提示、温度或少样本示例等概念现已广为人知，并且LLMs越来越多地用于建模与仿真（M&S）工作流。然而，看似简单的实践可能会引入微妙的问题、不必要的复杂性，甚至可能导致较差的结果。添加更多数据可能适得其反（例如，通过模型崩溃降低性能或无意中消除现有防护措施），在未事先评估模型已有的知识的情况下花费时间微调模型可能是不必要的，将温度设置为0不足以使LLMs具有确定性，提供大量M&S数据作为输入可能过多（LLMs无法关注所有内容），但简单的简化可能会丢失信息。本文旨在提供关于如何使用LLMs的全面实用指南，重点关注M&S应用。讨论常见的混淆来源，包括非确定性、知识增强（包括RAG和LoRA）、M&S数据的分解以及超参数设置。强调原则性的设计选择、诊断策略和实证评估，旨在帮助建模者就何时、如何以及是否依赖LLMs做出明智的决策。

---

### 38 Transport and Merge: Cross-Architecture Merging for Large Language Models

**link**: https://arxiv.org/pdf/2602.05495.pdf  
**date**: 2026-02-06  
**keywords**: latent space  
**abs**: 本文提出一种基于最优传输（OT）的跨架构合并框架，通过对齐激活来推断异构模型间的神经元对应关系。该框架利用传输计划指导权重空间融合，实现仅使用少量输入就能将知识从高资源大语言模型有效转移到低资源小模型。在低资源语言和特定领域的实验中，目标模型性能得到持续提升。

---

### 39 Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories

**link**: https://arxiv.org/pdf/2602.05085.pdf  
**date**: 2026-02-06  
**keywords**: LLM Memory  
**abs**: 本文提出Locas，一种局部支持的参数化记忆机制，其设计与现代Transformer的FFN块共享，可灵活永久化为模型参数并支持高效持续学习。Locas有两种变体：传统两层MLP设计（具有理论保证）和与SOTA LLM共享的GLU-FFN结构。研究表明，通过重用模型参数、激活和/或梯度对这种低秩旁置FFN式记忆进行原则性初始化，对快速收敛、泛化提升和防止灾难性遗忘至关重要。在PG-19整书语言建模和LoCoMo长上下文对话问答任务上验证了其有效性，仅需最少0.02%的额外参数即可存储过去上下文信息，同时最小化对模型现有知识的灾难性遗忘。

---

### 40 CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System

**link**: https://arxiv.org/pdf/2602.05004.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 大型语言模型正在交互式环境中实现语言条件化智能体，但高度协作的任务通常面临两个同时存在的约束：亚秒级实时协调和在严格的在线token预算下的持续多轮次适应。现有方法要么依赖频繁的轮次内推理，导致延迟和时序抖动，要么通过非结构化文本提供轮次后改进，难以编译为可靠的低成本执行方案。我们提出CoWork-X，这是一种主动协同进化框架，受快慢记忆分离的启发，将同伴协作视为跨轮次的闭环优化问题。CoWork-X实例化了一个技能智能体（Skill-Agent），该智能体通过基于层次任务网络（HTN）的技能检索从结构化、可解释且可组合的技能库中执行任务，以及一个轮次后协同优化器（Co-Optimizer），该优化器在明确的预算约束和漂移正则化下执行补丁式技能整合。在类似Overcooked-AI的具有挑战性的实时协作基准测试中，实验表明CoWork-X实现了稳定的累积性能提升，同时稳步降低了在线延迟和token使用量。

---

### 41 DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching

**link**: https://arxiv.org/pdf/2602.06039.pdf  
**date**: 2026-02-06  
**keywords**: cs.AI  
**abs**: 由提示大型语言模型构建的多智能体系统可以改进多轮推理，但大多数现有流程依赖固定的、跨轨迹的通信模式，这些模式与迭代问题解决中依赖阶段的需求匹配不佳。我们引入DyTopo，这是一种管理器引导的多智能体框架，可在每一轮重建稀疏有向通信图。根据管理器的轮次目标，每个智能体输出轻量级自然语言查询（需求）和关键（提供）描述符；DyTopo对这些描述符进行嵌入并执行语义匹配，仅沿诱导边路由私有消息。在代码生成和数学推理基准测试以及四个LLM骨干模型上，DyTopo始终优于最强基线（平均+6.2）。除准确性外，DyTopo通过演化图产生可解释的协调轨迹，能够定性检查通信路径如何在各轮次中重新配置。

---

### 42 Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs

**link**: https://arxiv.org/pdf/2602.05444.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）中的安全对齐机制通常表现为潜在内部状态，这掩盖了模型的固有能力。基于这一观察，我们从因果角度将安全机制建模为未观察到的混杂因素。随后，我们提出了因果前门调整攻击（CFA²）来对LLM进行越狱攻击，该框架利用Pearl的前门准则切断混杂关联以实现稳健的越狱。具体而言，我们采用稀疏自编码器（SAEs）来物理剥离与防御相关的特征，从而隔离核心任务意图。我们进一步将计算昂贵的边缘化过程简化为低推理复杂度的确定性干预。实验表明，CFA²实现了最先进的攻击成功率，同时提供了越狱过程的机制解释。

---

### 43 Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training

**link**: https://arxiv.org/pdf/2602.05940.pdf  
**date**: 2026-02-06  
**keywords**: cs.CL  
**abs**: 长推理模型在多语言环境中常常面临困境：它们倾向于对非英语问题用英语进行推理；当被限制使用问题语言推理时，准确率会大幅下降。这种困境是由多语言问题理解和多语言推理能力有限共同导致的。为解决这两个问题，我们提出了TRIT（翻译-推理集成训练），这是一种将翻译训练集成到多语言推理中的自改进框架。在没有外部反馈或额外多语言数据的情况下，该方法能联合增强多语言问题理解和响应生成能力。在MMATH数据集上，该方法平均优于多个基线7个百分点，同时提高了答案正确性和语言一致性。进一步分析表明，集成翻译训练使跨语言问题对齐提升了10个百分点以上，并提高了数学问题和通用领域文本的翻译质量，在FLORES-200上COMET分数最高提升8.4分。

---

### 44 LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards

**link**: https://arxiv.org/pdf/2602.05758.pdf  
**date**: 2026-02-06  
**keywords**: latent reasoning  
**abs**: 强化学习已成为LLM推理的关键驱动力，在长上下文场景（如长对话理解和结构化数据分析）中尤为重要，其挑战不仅在于处理tokens，还包括执行严格的演绎。现有方法依赖稀疏的结果奖励，效果有限，因为这种粗略信号不足以有效引导复杂的长上下文推理。为此，本文提出LongR框架，通过整合动态“思考与阅读”机制（交织推理与文档咨询）和基于相对信息增益的上下文密度奖励（量化相关文档的效用）来增强长上下文性能。实验表明，LongR在LongBench v2上实现9%的增益，并在RULER和InfiniteBench上持续改进，证明其在处理广泛上下文时的稳健效率。

---

### 45 Reinforcement World Model Learning for LLM-based Agents

**link**: https://arxiv.org/pdf/2602.05842.pdf  
**date**: 2026-02-06  
**keywords**: latent space  
**abs**: 基于LLM的代理在语言任务中表现强劲，但在代理环境中难以预测行动后果和适应环境动态，凸显了LLM代理世界建模能力的必要性。本文提出强化世界模型学习（RWML），一种自监督方法，通过sim-to-real差距奖励为LLM代理学习文本状态上的action-conditioned世界模型。该方法使模型生成的模拟下一状态与环境观察到的实际状态在预训练嵌入空间中对齐，鼓励内部世界模拟与实际环境动态的一致性。与优先考虑token级保真度的下一状态token预测不同，RWML提供更稳健的训练信号，且实证表明比LLM-as-a-judge更不易受奖励黑客攻击。在ALFWorld和τ² Bench上的评估显示，RWML显著优于基础模型，结合任务成功奖励时，分别比直接任务成功奖励RL高出6.9和5.7个百分点，同时匹配专家数据训练的性能。

---

### 46 OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions

**link**: https://arxiv.org/pdf/2602.05843.pdf  
**date**: 2026-02-06  
**keywords**: latent reasoning  
**abs**: 大型语言模型的快速发展推动了能够导航复杂环境的自主代理的发展。然而，现有评估主要采用演绎范式，代理基于明确提供的规则和静态目标执行任务，通常在有限的规划范围内。关键的是，这忽视了代理从经验中自主发现潜在转换规律的归纳必要性，而这是实现代理远见和维持战略连贯性的基石。为弥合这一差距，本文引入OdysseyArena，将代理评估重新聚焦于长视野、主动和归纳交互。我们形式化并实例化四个原语，将抽象转换动态转化为具体的交互环境。在此基础上，建立OdysseyArena-Lite用于标准化基准测试，提供120个任务以衡量代理的归纳效率和长视野发现能力。进一步，引入OdysseyArena-Challenge以压力测试代理在极端交互视野（如>200步）下的稳定性。对15+领先LLM的广泛实验表明，即使前沿模型在归纳场景中也存在缺陷，识别出在复杂环境中追求自主发现的关键瓶颈。

---

### 47 Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models

**link**: https://arxiv.org/pdf/2602.05897.pdf  
**date**: 2026-02-06  
**keywords**: latent reasoning  
**abs**: 随着大型语言模型变得更小更高效，小型推理模型（SRMs）对于在资源受限环境中启用思维链（CoT）推理至关重要。然而，它们容易出现忠实性幻觉，尤其是在中间推理步骤中。现有的基于在线强化学习的缓解方法依赖于基于结果的奖励或粗粒度的CoT评估，当最终答案正确时，这些方法可能无意中强化不忠实的推理。为解决这些限制，本文提出忠实感知步骤级强化学习（FaithRL），通过过程奖励模型引入步骤级监督的显式忠实性奖励，以及从忠实前缀生成对比信号的隐式截断重采样策略。在多个SRM和开放书籍QA基准上的实验表明，FaithRL持续减少CoT和最终答案中的幻觉，从而实现更忠实和可靠的推理。

---

### 48 Codified Finite-state Machines for Role-playing

**link**: https://arxiv.org/pdf/2602.05905.pdf  
**date**: 2026-02-06  
**keywords**: latent space  
**abs**: 建模潜在角色状态对于大型语言模型（LLMs）进行一致且引人入胜的角色扮演（RP）至关重要。然而，现有的基于提示的方法主要捕捉表面行为，往往无法跟踪驱动交互的潜在状态。我们重新审视有限状态机（FSMs），其在游戏设计中长期用于建模状态转换。虽然传统的手工制作、基于规则的FSM在小型、明确指定的状态空间中有效，但它们难以适应RP的开放式语义空间。为解决这一问题，我们引入编码有限状态机（CFSMs），这是一个使用基于LLM的编码将文本角色配置文件自动编码为FSM的框架。CFSMs直接从配置文件中提取关键状态和转换，生成可解释的结构以确保角色一致性。为进一步捕捉不确定性和可变性，我们将CFSMs扩展为编码概率有限状态机（CPFSMs），其中转换被建模为状态上的概率分布。通过合成评估和在已建立工件中的真实世界RP场景，我们证明CFSM和CPFSM优于普遍应用的基线，不仅在结构化任务中，而且在开放式随机状态探索中验证了有效性。

---

### 49 KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs

**link**: https://arxiv.org/pdf/2602.05929.pdf  
**date**: 2026-02-06  
**keywords**: LLM Memory  
**abs**: 大型语言模型依赖KV缓存来避免自回归解码过程中的冗余计算，但随着上下文长度的增长，缓存的读写会迅速饱和GPU内存带宽。最近的工作探索了KV缓存压缩，但大多数方法忽视了KV缓存的数据依赖性及其跨层变化。我们引入KV-CoRE（KV缓存压缩率评估），这是一种基于SVD的方法，用于量化KV缓存的数据相关低秩可压缩性。KV-CoRE计算Frobenius范数下的最优低秩近似，并且由于无梯度和增量特性，能够进行高效的数据集级、层级评估。使用这种方法，我们分析了跨越五个英语领域和十六种语言的多个模型和数据集，发现了将可压缩性与模型架构、训练数据和语言覆盖相关联的系统模式。作为分析的一部分，我们采用归一化有效秩作为可压缩性的度量，并表明它与压缩下的性能下降强烈相关。我们的研究建立了一个原则性的评估框架和LLMs中KV缓存可压缩性的首个大规模基准，为动态、数据感知压缩和以数据为中心的模型开发提供见解。