### 1 The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI

**link**: https://arxiv.org/pdf/2602.17127.pdf
**date**: 2026-02-20
**keywords**: latent space
**abs**: 随着大型语言模型（LLMs）从独立聊天界面过渡到多智能体系统和递归评估循环中的基础推理层，检测持久的提供者级行为签名成为安全和治理的关键要求。传统基准测量瞬态任务准确性，但未能捕捉稳定的潜在反应策略——即训练和对齐过程中嵌入的、比单个模型版本更持久的“主导思维模式”。本研究提出了一个心理测量框架，旨在审计生成式AI中的潜在偏见和复合风险，通过系统分析模型在投射心理评估中的表现，揭示其认知-表征和情感-关系组件的潜在特征。

---

### 2 Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study

**link**: https://arxiv.org/pdf/2602.17262.pdf
**date**: 2026-02-20
**keywords**: latent space
**abs**: 人类自陈问卷越来越多地用于NLP领域以基准测试和审计大型语言模型（LLMs），涉及从人格一致性到安全性和偏见评估等多个方面。然而，这些工具假设被试会诚实回答；在评估环境中，LLMs可能倾向于选择社会偏好的答案——一种社会期望反应（SDR）——这会扭曲问卷得出的分数和下游结论。本研究提出了一个心理测量框架来量化和减轻LLM问卷评估中的SDR。为量化SDR，在HONEST与FAKE-GOOD指令下施测相同问卷，并通过项目反应理论（IRT）估计的潜在分数计算方向校正的标准化效应量作为SDR指标。为减轻SDR，通过约束优化从项目库中选择30个跨领域配对构建匹配期望的分级强制选择（GFC）大五人格问卷。结果表明，Likert式问卷显示持续显著的SDR，而期望匹配的GFC能大幅减弱SDR，同时基本保留目标人格特征的恢复能力。

---

### 3 Representation Collapse in Machine Translation Through the Lens of Angular Dispersion

**link**: https://arxiv.org/pdf/2602.17287.pdf
**date**: 2026-02-20
**keywords**: latent space
**abs**: 基于Transformer架构的现代神经翻译模型在高资源数据集上表现出高性能，但其标准的下一个token预测训练策略可能导致表示崩溃等未被充分关注的缺陷。先前研究表明，深层Transformer层的表示尤其容易出现此问题，导致无法有效利用几何空间。在端到端连续输出神经机器翻译训练中，表示崩溃更为明显，此时所有向量被设置为相同值成为 trivial 解。本文分析了离散和连续NMTTransformer在训练过程中不同层级的表示崩溃动态，引入基于角度分散的正则化方法，并实证表明该方法不仅缓解崩溃，还能提高翻译质量。此外，量化模型表现出类似的崩溃行为，且正则化的益处在量化后仍得以保留。

---

### 4 Variational inference via radial transport

**link**: https://arxiv.org/pdf/2602.17525.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 在变分推断（VI）中，从业者通常使用简单的替代分布（如高斯分布）来近似高维分布π。然而，在许多实际应用中，高斯分布可能无法捕捉π的正确径向轮廓，导致覆盖性较差。本文从优化这些径向轮廓的角度解决VI问题，提出了radVI算法，作为现有VI方案（如高斯均值场VI和拉普拉斯近似）的一种廉价且有效的附加组件。基于Wasserstein空间优化的最新进展以及Caffarelli风格的径向传输映射的新正则性，本文为该算法提供了理论收敛保证。

---

### 5 When to Trust the Cheap Check: Weak and Strong Verification for Reasoning

**link**: https://arxiv.org/pdf/2602.17633.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 大型语言模型（LLM）的推理过程越来越多地在更广泛的验证循环中展开。在内部，系统使用廉价检查（如自一致性或代理奖励，称为弱验证）；在外部，用户检查输出并通过反馈引导模型直至结果可信（称为强验证）。这些信号在成本和可靠性上差异显著：强验证可建立信任但资源密集，弱验证快速可扩展但嘈杂且不完美。本文通过弱-强验证策略形式化这种张力，决定何时基于弱验证接受/拒绝或推迟到强验证。引入衡量错误接受、错误拒绝和强验证频率的指标，证明最优策略具有双阈值结构，并开发一种在线算法，在不假设查询流、语言模型或弱验证器的情况下，可证明地控制接受和拒绝错误。

---

### 6 One-step Language Modeling via Continuous Denoising

**link**: https://arxiv.org/pdf/2602.16813.pdf
**date**: 2026-02-20
**keywords**: cs.CL
**abs**: 基于离散扩散的语言模型因其比自回归模型生成更快的潜力而受到广泛关注，但在少步生成时样本质量急剧下降，未能实现这一潜力。本文表明，利用基于流的连续去噪的语言模型在质量和速度上均可优于离散扩散。通过重新审视离散模态上流的基础，构建流基语言模型（FLM），对独热令牌编码执行欧几里得去噪。模型通过交叉熵目标预测干净数据，并引入简单的时间重参数化显著提高训练稳定性和生成质量。将FLM蒸馏为其关联的流映射，得到能够少步生成的蒸馏流映射语言模型（FMLM）。在LM1B和OWT语言数据集上，FLM的生成质量与最先进的离散扩散模型相当；FMLM的一步生成质量超过了现有少步语言模型的8步生成质量。本文质疑了离散扩散过程对离散模态生成建模的必要性，并为加速大规模流基语言建模铺平了道路。

---

### 7 ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders

**link**: https://arxiv.org/pdf/2602.16938.pdf
**date**: 2026-02-20
**keywords**: LLM Rec
**abs**: 基于大型语言模型（LLM）的用户模拟器在改进对话式人工智能方面的潜力因存在严重的“真实性差距”而受阻，导致系统虽在模拟交互中表现优化，但在现实世界中可能无法良好运行。本文介绍了ConvApparel，这是一个新的人机对话数据集，旨在解决这一差距。其独特的双智能体数据收集协议——同时使用“优秀”和“糟糕”的推荐器——通过捕捉广泛的用户体验并辅以第一人称用户满意度注释，实现了反事实验证。本文提出了一个综合验证框架，结合统计对齐、类人度评分和反事实验证来测试泛化能力。实验表明，所有模拟器都存在显著的真实性差距，但该框架也显示，数据驱动的模拟器优于提示基线，尤其在反事实验证中，它们能更真实地适应未见过的行为，表明它们体现了更稳健（尽管仍不完美）的用户模型。

---

### 8 Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History

**link**: https://arxiv.org/pdf/2602.17003.pdf
**date**: 2026-02-20
**keywords**: LLM Memory
**abs**: 大型语言模型推动了网络智能体的发展，但当前智能体缺乏个性化能力。由于用户很少详细说明其意图的每个细节，实用的网络智能体必须能够通过推断用户偏好和上下文来解释模糊查询。为解决这一挑战，本文提出Persona2Web，这是首个用于在真实开放网络上评估个性化网络智能体的基准，其构建基于“澄清以个性化”原则，要求智能体基于用户历史而非依赖显式指令来解决模糊性。Persona2Web包括：（1）在长时间跨度内隐含揭示偏好的用户历史，（2）需要智能体推断隐含用户偏好的模糊查询，以及（3）支持细粒度个性化评估的推理感知评估框架。本文通过各种智能体架构、骨干模型、历史访问方案以及不同模糊程度的查询进行了广泛实验，揭示了个性化网络智能体行为中的关键挑战。

---

### 9 ReIn: Conversational Error Recovery with Reasoning Inception

**link**: https://arxiv.org/pdf/2602.17022.pdf
**date**: 2026-02-20
**keywords**: latent reasoning
**abs**: 集成工具的大型语言模型（LLM）驱动的对话智能体在固定任务导向对话数据集上表现出色，但仍易受用户引发的意外错误影响。本文不关注错误预防，而是聚焦错误恢复，这需要准确诊断错误对话上下文并执行适当的恢复计划。在由于显著成本和时间要求而无法进行模型微调或提示修改的现实约束下，本文探索智能体是否能从上下文有缺陷的交互中恢复，以及如何在不改变模型参数和提示的情况下调整其行为。为此，提出推理植入（ReIn），这是一种在测试时将初始推理植入智能体决策过程的干预方法。具体而言，外部植入模块识别对话上下文中的预定义错误并生成恢复计划，随后将其集成到智能体的内部推理过程中以指导纠正行动，且无需修改模型参数和系统提示。通过系统模拟直接阻碍用户目标成功完成的对话失败场景（用户的模糊和不支持请求）进行评估，结果表明，在不同智能体模型和植入模块组合下，ReIn显著提高了任务成功率，并能泛化到未见过的错误类型。此外，它始终优于显式提示修改方法，突显其作为高效即时方法的实用性。

---

### 10 Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems

**link**: https://arxiv.org/pdf/2602.16715.pdf
**date**: 2026-02-20
**keywords**: cs.AI
**abs**: 本文探索了大型语言模型（LLMs）、检索增强生成（RAG）和基于图的RAG（GraphRAG）在生成设计结构矩阵（DSM）方面的潜力。研究在两个不同用例（电动螺丝刀和CubeSat，均有已知架构参考）上测试了这些方法，评估了它们在两个关键任务上的性能：确定预定义组件之间的关系，以及更复杂的组件识别及其后续关系识别挑战。通过评估DSM的每个元素和整体架构来衡量性能。尽管存在设计和计算挑战，但研究发现了自动化DSM生成的机会，所有代码公开以确保可重复性并获取领域专家的进一步反馈。

---

### 11 The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR→LLM Pipelines?

**link**: https://arxiv.org/pdf/2602.17598.pdf
**date**: 2026-02-20
**keywords**: cs.CL
**abs**: 当前语音大型语言模型（LLMs）在很大程度上执行隐式自动语音识别（ASR）：在可通过转录本解决的任务上，它们在行为和机制上等价于简单的Whisper→LLM级联。本文通过四个语音LLM和六个任务的匹配骨干测试证明了这一点，首次控制了LLM骨干。Ultravox与其匹配的级联在统计上无法区分（κ=0.93）；logit lens揭示隐藏状态中出现文本；LEACE概念擦除证实文本表示在测试的两种架构中都是因果必要的，会将准确率降至接近零。Qwen2-Audio则真正表现出差异，表明级联等价性是架构相关的，而非普遍现象。对于大多数部署用例，当前语音LLMs是昂贵的级联，且在噪声环境下性能更差，在0 dB时干净条件下的优势最多逆转7.6%。

---

### 12 Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking

**link**: https://arxiv.org/pdf/2602.16746.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: Grokking——在小型算法任务中从记忆到泛化的延迟转变——仍然知之甚少。本文对训练于模运算的Transformer的优化动力学进行了几何分析。注意力权重轨迹的PCA表明，训练主要在低维执行子空间内演化，单个主成分捕获了68-83%的轨迹方差。为了探究损失景观几何，我们测量了交换子缺陷——连续梯度步骤的非交换性——并将其投影到该学习子空间上。我们发现，在与执行子空间正交的方向上曲率急剧增长，而轨迹仍主要局限于该子空间内。重要的是，在不同学习率和超参数制度下，曲率增长始终先于泛化，前置时间服从grokking时间尺度的幂律。因果干预实验表明，沿学习子空间的运动对于grokking是必要的，而人为增加曲率则不足够。这些结果共同支持了一种几何解释，即grokking反映了从以低维限制和横向曲率积累为特征的亚稳状态的逃逸。所有发现在该学习率范围内、一个定性不同的慢制度（lr=5e-5，wd=0.1，3层）和三个随机种子上均可重复，尽管不同制度之间的对齐动力学在数量上有所不同。因果干预实验证实，正交梯度流对于grokking是必要但非充分的：抑制它会阻止泛化，且在四种操作中呈现单调剂量反应，而人为增强曲率缺陷则无影响。

---

### 13 HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind

**link**: https://arxiv.org/pdf/2602.16826.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 心理理论（ToM）使AI系统能够推断智能体的隐藏目标和心理状态，但现有方法主要集中在人类可理解的小型网格世界空间。我们引入HiVAE，一种分层变分架构，将ToM推理扩展到现实的时空领域。受人类认知的信念-欲望-意图结构启发，我们的三级VAE层次结构在3185节点的校园导航任务上实现了显著的性能提升。然而，我们发现一个关键限制：虽然我们的分层结构改善了预测，但学习到的潜在表征缺乏与实际心理状态的明确接地。我们提出了自监督对齐策略，并展示这项工作以征求社区对接地方法的反馈。

---

### 14 Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy

**link**: https://arxiv.org/pdf/2602.17229.pdf
**date**: 2026-02-20
**keywords**: latent space
**abs**: 大型语言模型的黑箱性质需要超越表面性能指标的新型评估框架。本研究使用布鲁姆分类法作为层次透镜，调查认知复杂性的内部神经表征。通过分析不同LLM的高维激活向量，我们探究从基本回忆（记住）到抽象综合（创造）的不同认知水平是否在模型的残差流中线性可分。结果表明，线性分类器在所有布鲁姆水平上达到约95%的平均准确率，提供了强有力的证据，证明认知水平被编码在模型表征的线性可访问子空间中。这些发现表明，模型在正向传播早期解决提示的认知难度，表征在各层中变得越来越可分。

---

### 15 The Anxiety of Influence: Bloom Filters in Transformer Attention Heads

**link**: https://arxiv.org/pdf/2602.17526.pdf
**date**: 2026-02-20
**keywords**: LLM Memory
**abs**: 一些Transformer注意力头似乎充当成员测试器，致力于回答“该token是否在上下文中出现过？”的问题。我们在四个语言模型（GPT-2小、中、大型；Pythia-160M）中识别这些头，并表明它们形成了成员测试策略的谱系。两个头（GPT-2小型中的L0H1和L0H5）作为高精度成员过滤器，即使在180个独特上下文token时，假阳性率为0-4%——远高于经典布隆过滤器的d_head=64位容量。第三个头（L1H11）显示经典布隆过滤器容量曲线：其假阳性率遵循理论公式p≈(1-e^(-kn/m))^k，R²=1.0，拟合容量m≈5位，在n≈20个独特token时饱和。研究表明这些注意力头形成了模型记忆上下文信息的机制。

---

### 16 Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability

**link**: https://arxiv.org/pdf/2602.17544.pdf
**date**: 2026-02-20
**keywords**: latent reasoning
**abs**: 在多智能体IR管道中，基于LLM的智能体通过思维链（CoT）交换中间推理。当前CoT评估仅关注任务准确性，无法评估推理过程质量。本研究引入可重用性（Executor重用Thinker的CoT的难易程度）和可验证性（Executor使用CoT匹配Thinker答案的频率）两个新指标。通过Thinker-Executor框架在五个基准上的实验表明，这两个指标与标准准确性不相关，暴露了当前评估体系的盲点。研究还发现专门推理模型的CoT并不比通用LLM更可重用或可验证。

---

### 17 KLong: Training LLM Agent for Extremely Long-horizon Tasks

**link**: https://arxiv.org/pdf/2602.17547.pdf
**date**: 2026-02-20
**keywords**: LLM Memory
**abs**: 本文介绍KLong，一个开源LLM代理，旨在解决极长期任务。其核心方法是通过轨迹分割SFT冷启动模型，再通过渐进式RL训练扩展能力。研究首先使用全面SFT激活基础模型的智能体能力，然后通过Research-Factory管道生成高质量训练数据（从Claude 4.5 Sonnet蒸馏的数千条长期轨迹）。为处理超长轨迹，提出轨迹分割SFT（保留早期上下文，逐步截断后期上下文）和渐进式RL（多阶段训练，逐步延长超时时间）。实验表明KLong在PaperBench上超越Kimi K2 Thinking 11.28%，性能泛化到SWE-bench Verified等编码基准。

---

### 18 Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders

**link**: https://arxiv.org/pdf/2602.17050.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 嵌入表是大规模推荐系统的关键组件，可将高基数分类特征高效映射为稠密向量表示。然而，随着唯一ID数量的增长，传统基于哈希的索引方法会遭遇碰撞，导致模型性能和个性化质量下降。本文提出多探测零碰撞哈希（MPZCH），一种基于线性探测的新型索引机制，能有效缓解嵌入碰撞。通过合理的表大小设计，该机制通常可完全消除碰撞，同时保持生产级效率。MPZCH利用辅助张量和高性能CUDA内核实现可配置的探测和主动驱逐策略。通过淘汰过时ID并重置重新分配的槽位，MPZCH避免了哈希方法典型的陈旧嵌入继承问题，确保新特征从零开始有效学习。尽管存在碰撞缓解开销，该系统仍保持与现有方法相当的训练QPS和推理延迟。在线实验表明，MPZCH实现了用户嵌入零碰撞，并显著提升了物品嵌入的新鲜度和质量。该解决方案已在开源TorchRec库中发布。

---

### 19 Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling

**link**: https://arxiv.org/pdf/2602.17089.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 最近为生成AI任务开发的扩散模型能够生成高质量样本，同时保持样本多样性以提升模式覆盖率，为学习随机闭合模型提供了 promising 路径。与GAN和VAE等其他生成模型相比，扩散模型的采样速度是已知的主要劣势。通过在二维Kolmogorov流数值示例上系统比较基于传输的生成模型，本文表明低维潜在空间中的流匹配适用于随机闭合模型的快速采样，可实现比迭代扩散方法快两个数量级的单步采样。为控制潜在空间失真并确保采样闭合项的物理保真度，本文比较了联合训练方案提供的隐式正则化与两种显式正则化器：度量保持（MP）和几何感知（GA）约束。除了更快的采样速度外，显式和隐式正则化的潜在空间均继承了原始复杂动力系统低维流形的关键拓扑信息，使随机闭合模型的学习无需大量训练数据。

---

### 20 VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation

**link**: https://arxiv.org/pdf/2602.17133.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 该论文提出VP-VAE（向量扰动VAE），一种通过自适应向量扰动重新思考向量量化的新范式。其核心创新是在训练中消除显式码本，将表示学习与离散化解耦，认为量化过程可视为在潜在空间中注入结构化扰动。通过Metropolis-Hastings采样生成分布一致且尺度自适应的潜在扰动，替代不可微的量化器，实现稳定训练并增强对推理时量化误差的鲁棒性。在近似均匀潜在变量假设下，推导了FSP（有限标量扰动）变体，为FSQ风格固定量化器提供统一理论解释和实用改进。图像和音频基准实验表明，VP-VAE和FSP提升了重建保真度，实现更平衡的 token  usage，同时避免了码本耦合训练的不稳定性。

---

### 21 Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems

**link**: https://arxiv.org/pdf/2602.17263.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 控制自由电子激光光阴极注入器中的纵向激光脉冲形状是优化电子束质量的有力手段，但由于暴力脉冲传播模拟成本高昂，对广阔设计空间的系统性探索受到限制。我们提出了一种基于Wasserstein自编码器的生成建模框架，以学习脉冲整形和下游束流动力学之间的可微潜空间接口。我们的实证结果表明，所学习的潜空间是连续且可解释的，同时保持高保真度的重建。高阶高斯等脉冲族追踪连贯轨迹，而标准化时间脉冲长度显示出与脉冲能量相关的潜组织。通过主成分和高斯混合模型的分析揭示了良好的潜几何结构，能够通过线性插值实现不同脉冲类型之间的平滑过渡。该模型从模拟数据推广到真实实验脉冲测量，准确重建脉冲并将其一致地嵌入到学习的流形中。总体而言，该方法减少了对昂贵脉冲传播模拟的依赖，并促进了下游束流动力学模拟和分析。

---

### 22 Unified Latents (UL): How to train your latents

**link**: https://arxiv.org/pdf/2602.17270.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 我们提出了统一潜变量（UL），这是一个学习潜变量表示的框架，该表示通过扩散先验联合正则化并由扩散模型解码。通过将编码器的输出噪声与先验的最小噪声水平相关联，我们获得了一个简单的训练目标，该目标为潜变量比特率提供了严格的上限。在ImageNet-512上，我们的方法实现了具有竞争力的FID为1.4，具有高重建质量（PSNR），同时比在Stable Diffusion潜变量上训练的模型需要更少的训练FLOPs。在Kinetics-600上，我们设置了新的最先进FVD为1.3。

---

### 23 Fail-Closed Alignment for Large Language Models

**link**: https://arxiv.org/pdf/2602.16977.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 我们发现当前大型语言模型（LLM）对齐中的一个结构性弱点：现代拒绝机制是故障开放的。尽管现有方法通过多个潜在特征对拒绝行为进行编码，但通过基于提示的越狱攻击抑制单个主导特征可能导致对齐崩溃，从而产生不安全的生成内容。受此启发，我们提出故障关闭对齐作为鲁棒LLM安全的设计原则：拒绝机制应通过冗余、独立的因果路径在部分故障情况下仍保持有效。我们提出了这一原则的具体实例：一个渐进式对齐框架，该框架迭代地识别和消融先前学习的拒绝方向，迫使模型沿着新的、独立的子空间重建安全性。在四次越狱攻击中，我们实现了最强的整体鲁棒性，同时减轻了过度拒绝并保持了生成质量，且计算开销较小。我们的机制分析证实，使用我们的方法训练的模型编码了多个因果独立的拒绝方向，基于提示的越狱攻击无法同时抑制这些方向，为故障关闭对齐作为鲁棒LLM安全的原则性基础提供了实证支持。

---

### 24 Discovering Universal Activation Directions for PII Leakage in Language Models

**link**: https://arxiv.org/pdf/2602.16980.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 现代语言模型展现出丰富的内部结构，但对于隐私敏感行为（如个人身份信息（PII）泄露）如何在其隐藏状态中表示和调制，我们知之甚少。我们提出了UniLeak，一个机制可解释性框架，用于识别通用激活方向：模型残差流中的潜在方向，在推理时线性添加这些方向会持续增加跨提示生成PII的可能性。这些模型特定的方向跨上下文泛化，并放大PII生成概率，同时对生成质量的影响最小。UniLeak无需访问训练数据或真实PII，仅依靠自生成文本即可恢复这些方向。在多个模型和数据集上，与现有基于提示的提取方法相比，沿着这些通用方向引导显著增加了PII泄露。我们的结果为PII泄露提供了新视角：模型表示中潜在信号的叠加，使得风险放大和缓解成为可能。

---

### 25 WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning

**link**: https://arxiv.org/pdf/2602.17025.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 群体相对策略优化（GRPO）在训练语言模型进行复杂推理方面是有效的。然而，由于目标是相对于一组采样轨迹定义的，延长 deliberation 会创造更多实现相对收益的机会，导致推理效率低下和过度思考，并使正确性与 rollout 效率之间的权衡复杂化。在实践中控制这种行为很困难，原因包括：（i）长度惩罚难以校准，因为更长的 rollout 可能反映需要更长推理的更难问题，惩罚 token 可能会截断有用的推理和冗余的继续；（ii）除了最终答案的正确性外，通常无法获得直接指示何时继续或停止的监督。我们提出弱监督 GRPO（WS-GRPO），通过将终端奖励转换为对部分轨迹的正确性感知指导来提高 rollout 效率。与难以校准的全局长度惩罚不同，WS-GRPO 从仅结果正确性中训练偏好模型，以产生前缀级信号，指示何时额外的继续是有益的。因此，WS-GRPO 提供了基于结果的继续/停止指导，在保持准确性的同时减少冗余 deliberation。我们提供了理论结果，并在推理基准上实证表明，WS-GRPO 显著减少了 rollout 长度，同时保持了与 GRPO 基线相当的性能。

---

### 26 Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency

**link**: https://arxiv.org/pdf/2602.16787.pdf
**date**: 2026-02-20
**keywords**: cs.LG, latent reasoning
**abs**: 尽管大型语言模型（LLMs）在推理基准测试中表现出色，但在面对反事实问题时却显得脆弱，表明其因果推理能力存在不足。虽然最近的研究表明带标签的反事实任务可以作为评估LLMs因果推理能力的基准，但生成覆盖广阔反事实空间所需规模的数据存在局限性。在本研究中，我们引入了双重反事实一致性（DCC），这是一种轻量级的推理时方法，用于测量和引导LLMs的因果推理能力，且无需带标签的反事实数据。DCC验证模型执行因果推理的两个重要元素：因果干预和反事实预测。我们使用DCC在一系列推理任务和干预措施上评估了各种领先LLMs的因果推理能力。此外，我们证明了DCC作为一种无需训练的测试时拒绝采样准则的有效性，并表明它可以直接提高多个模型家族在推理任务上的性能。

---

### 27 Training Large Reasoning Models Efficiently via Progressive Thought Encoding

**link**: https://arxiv.org/pdf/2602.16839.pdf
**date**: 2026-02-20
**keywords**: cs.LG, latent reasoning
**abs**: 大型推理模型（LRMs）在复杂问题上表现出色，但面临效率的关键障碍：强化学习（RL）训练需要基于结果奖励的长序列展开，其中自回归解码占据了时间和内存使用的主导地位。虽然滑动窗口缓存策略可以限制内存，但会破坏长上下文推理并降低性能。本研究引入了渐进式思维编码（Progressive Thought Encoding），这是一种参数高效的微调方法，使LRMs能够在固定大小的缓存下有效推理。通过将中间推理逐步编码为固定大小的向量表示，我们的方法消除了对全缓存展开进行反向传播的需求，从而减少内存使用，同时在推理过程中保持恒定内存。在三个模型（包括Qwen2.5-3B-Instruct、Qwen2.5-7B-Instruct和DeepSeek-R1-Distill-Llama-8B）上，在六个广泛使用的具有挑战性的数学基准测试中进行的实验显示出一致的增益：我们的方法比基于LoRA的微调平均提高19.3%，比未微调的LRMs平均提高29.9%，在相同的严格缓存预算下，AIME2024/2025的准确率最高提高23.4。这些结果表明，渐进式思维编码不仅提高了推理准确性，还使LRMs的RL训练在现实世界的内存约束下更加高效和可扩展。

---

### 28 Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents

**link**: https://arxiv.org/pdf/2602.16855.pdf
**date**: 2026-02-20
**keywords**: cs.AI, LLM Memory
**abs**: 本文介绍了GUI-Owl-1.5，这是最新的原生GUI代理模型，具有多种尺寸（2B/4B/8B/32B/235B）的指令/思考变体，并支持多种平台（桌面、移动设备、浏览器等），以实现云边协同和实时交互。GUI-Owl-1.5在20多个开源模型的GUI基准测试中取得了最先进的结果：（1）在GUI自动化任务上，OSWorld得分为56.5，AndroidWorld为71.6，WebArena为48.4；（2）在接地任务上，ScreenSpotPro得分为80.3；（3）在工具调用任务上，OSWorld-MCP得分为47.6，MobileWorld为46.8；（4）在记忆和知识任务上，GUI-Knowledge Bench得分为75.5。GUI-Owl-1.5包含几项关键创新：（1）混合数据飞轮：我们基于模拟环境和基于云的沙箱环境相结合，构建了UI理解和轨迹生成的数据管道，以提高数据收集的效率和质量。（2）代理能力的统一增强：我们使用统一的思维合成管道来增强模型的推理能力，同时特别强调提高关键代理能力，包括工具/MCP使用、记忆和多代理适应；（3）多平台环境RL扩展：我们提出了一种新的环境RL算法MRPO，以解决多平台冲突和长horizon任务训练效率低的挑战。GUI-Owl-1.5模型已开源，并提供在线云沙箱演示。

---

### 29 RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models

**link**: https://arxiv.org/pdf/2602.17053.pdf
**date**: 2026-02-20
**keywords**: cs.AI, latent reasoning
**abs**: 大型推理模型（LRMs）表现出强大的性能，但经常产生听起来合理但无法反映其真实决策过程的推理依据，从而削弱了可靠性和信任度。我们引入了一个推理忠实性的正式框架，该框架由两个可测试条件定义：立场一致性（将推理与答案联系起来的连贯立场）和因果影响（所述推理在输出级干预下对答案产生因果驱动），明确与准确性解耦。为了实现这一点，我们提出了RFEval，这是一个包含7个任务、7186个实例的基准，通过受控的输出级反事实干预来探测忠实性。对12个开源LRMs的评估发现，49.7%的输出存在不忠实问题，主要源于立场不一致。失败集中在数学和代码等脆弱、收敛的领域，并且与训练后机制的相关性比与规模的相关性更强：家族内消融实验表明，在监督微调（SFT）基础上添加当前的RL风格目标会降低推理忠实性，即使准确性得以维持。准确性既不是忠实性的充分条件，也不是可靠的代理指标：在控制模型和任务后，准确性与忠实性之间的联系微弱且统计上不显著。我们的工作建立了一种严格的LRM可靠性审计方法，并表明可信AI不仅需要优化正确的结果，还需要优化推理过程的结构完整性。

---

### 30 Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models

**link**: https://arxiv.org/pdf/2602.17497.pdf
**date**: 2026-02-20
**keywords**: cs.LG
**abs**: 从自我采样数据和稀疏环境反馈中学习仍是训练自进化智能体的基本挑战。时间信用分配通过将稀疏反馈转化为密集监督信号来缓解这一问题。然而，先前的方法通常依赖于学习特定任务的价值函数进行信用分配，存在样本效率低和泛化能力有限的问题。在本研究中，我们提出利用大型语言模型（LLMs）的预训练知识，通过回顾性上下文学习（RICL）将稀疏奖励转化为密集训练信号（即优势函数）。我们进一步提出了一个在线学习框架RICOL，该框架基于RICL的信用分配结果迭代优化策略。实验表明，RICL能够在样本有限的情况下准确估计优势函数，并有效识别环境中的关键状态以进行时间信用分配。在四个BabyAI场景上的扩展评估显示，RICOL与传统在线强化学习算法相比，收敛性能相当，但样本效率显著提高。我们的研究结果突显了利用LLMs进行时间信用分配的潜力，为更具样本效率和泛化能力的强化学习范式铺平了道路。