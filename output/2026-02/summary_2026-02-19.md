### 1 VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering

**link**: https://arxiv.org/pdf/2602.15870.pdf  
**date**: 2026-02-19  
**keywords**: latent space  
**abs**: 本文提出VDLM，一种模块化可变扩散语言模型，将语义规划与文本渲染分离。VDLM应用LLaDA风格的掩码扩散于语义变量嵌入，在潜在空间中实现迭代优化，并使用嵌入空间奖励和值通过轨迹感知优化对规划器进行后训练，避免在RL循环内进行文本解码。为将规划嵌入转换回文本，使用Vec2Text渲染器并引入嵌入扰动以增强规划器噪声下的解码稳健性。在九个基准测试中，VDLM在预训练阶段具有竞争力，并在后训练阶段在长文本生成任务上取得显著改进，优于其他基线，突显了嵌入空间后训练和潜在到文本渲染的有效性。

---

### 2 Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion

**link**: https://arxiv.org/pdf/2602.15895.pdf  
**date**: 2026-02-19  
**keywords**: LLM Memory  
**abs**: 本文提出CogitoRAG，一种模拟人类认知记忆过程的RAG框架，核心在于语义要点的提取与演化。在离线索引阶段，将非结构化语料推导为要点记忆语料，并转换为整合实体、关系事实和记忆节点的多维度知识图；在线检索阶段，通过查询分解处理复杂查询，实体扩散模块在图上执行关联检索，并引入CogniRank算法重排候选段落。实验表明，CogitoRAG在五个QA基准和GraphBench上优于最先进的RAG方法，在复杂知识整合和推理方面表现出优越能力。

---

### 3 Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs

**link**: https://arxiv.org/pdf/2602.15846.pdf  
**date**: 2026-02-19  
**keywords**: LLM Memory, latent reasoning  
**abs**: 本文引入门控树交叉注意力（GTCA）分支，用于在仅解码器大型语言模型中注入显式句法结构，同时保持骨干架构不变。GTCA读取预计算的 constituency 块记忆，使用令牌更新掩码和分阶段训练控制结构更新。在多个基准上，GTCA增强了句法鲁棒性，超过了持续训练的基线，且不损害多项选择问答或常识推理性能，为更具句法鲁棒性的模型提供实用途径。

---

### 4 Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models

**link**: https://arxiv.org/pdf/2602.15847.pdf  
**date**: 2026-02-19  
**keywords**: latent space  
**abs**: 本文通过分析大五人格引导方向的几何关系，检验了LLMs中人格特质可独立控制的假设。研究表明，引导方向表现出显著依赖性：引导一个特质会引起其他特质变化，即使去除线性重叠。硬正交化虽强制几何独立性，但不能消除跨特质影响，且降低引导强度。这表明人格特质在LLMs中占据轻微耦合的子空间，限制了完全独立的控制。

---

### 5 MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks

**link**: https://arxiv.org/pdf/2602.16313.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文引入MemoryArena评估平台，用于在多会话Memory-Agent-Environment循环中基准化代理内存。该基准包含相互依赖子任务，代理需将经验提炼为内存以指导后期行动。实验涵盖网页导航、偏好约束规划等任务，结果显示在现有基准上性能饱和的代理在该环境中表现不佳，暴露了当前代理内存评估的差距。

---

### 6 Doc-to-LoRA: Learning to Instantly Internalize Contexts

**link**: https://arxiv.org/pdf/2602.15902.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文提出Doc-to-LoRA（D2L），一种轻量级超网络，通过元学习在单次前向传播中实现近似上下文蒸馏，将上下文信息编码为LoRA适配器。该方法减少LLM推理时的内存消耗和延迟，在长上下文任务中表现出色，能扩展模型上下文窗口，并在真实QA数据集上优于传统上下文蒸馏方法。

---

### 7 Surgical Activation Steering via Generative Causal Mediation

**link**: https://arxiv.org/pdf/2602.16080.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文提出生成因果中介（GCM）方法，通过量化模型组件对特定概念的中介作用，实现对LLM长文本响应的精确引导。GCM在拒绝生成、谄媚行为控制和风格转换等任务中优于传统方法，能够有效定位并干预模型内部的潜在推理过程。

---

### 8 Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities

**link**: https://arxiv.org/pdf/2602.16093.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文提出DiSC方法，通过上下文蒸馏实现LLM的参数知识更新，同时保留训练后的能力。该方法通过分割上下文段并最小化分布差异，在学习新知识的同时减少对原有技能的遗忘，在多个模型和任务上展现出良好的知识更新与能力保留平衡。

---

### 9 Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents

**link**: https://arxiv.org/pdf/2602.16699.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文引入Calibrate-Then-Act（CTA）框架，引导LLMs在不确定性下权衡探索成本与收益，以更优地执行环境探索。在信息检索和编码任务中，CTA通过提供额外上下文使智能体发现更优决策策略，即使在强化学习训练下也保持改进。

---

### 10 Reinforced Fast Weights with Next-Sequence Prediction

**link**: https://arxiv.org/pdf/2602.16704.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文提出REFINE框架，在下一代序列预测目标下训练快速权重模型。REFINE基于预测熵选择信息性标记位置，生成多标记滚动，分配自监督序列级奖励，并通过组相对策略优化模型。实验表明，REFINE在检索、长上下文问答和LongBench任务上优于使用NTP的监督微调，为改进快速权重架构中的长上下文建模提供有效框架。

---

### 11 From Growing to Looping: A Unified View of Iterative Computation in LLMs

**link**: https://arxiv.org/pdf/2602.16490.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文探讨了LLM中循环和深度增长两种技术与推理能力的关系，发现它们表现出趋同的深度特征，性能增益源于共同的迭代计算形式。研究表明，深度增长模型在使用数学密集型数据时实现最大推理增益，通过调整中间块进行循环可进一步提升性能，将两种技术定位为诱导和扩展迭代计算的互补方法。

---

### 12 Optimizing Soft Prompt Tuning via Structural Evolution

**link**: https://arxiv.org/pdf/2602.16500.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文提出基于拓扑形态演化的软提示调优优化方法，利用拓扑数据分析量化软提示的结构表示及其训练过程演化。构建拓扑软提示损失函数以优化调优，通过量化参数间连接性和冗余性引导模型学习结构稳定的适应。实验显示，该方法加速收敛并提高性能，为理解与优化软提示调优提供可解释方法。

---

### 13 Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach

**link**: https://arxiv.org/pdf/2602.15857.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文提出协作推理与自适应融合框架，通过结构化多阶段推理机制集成传统方法与大型语言模型。关键创新包括跨平台协作注意力模块、分层自适应融合机制、联合优化策略和新型多模态提取能力。理论分析表明，框架降低泛化界，实验在多个数据集上实现平均0.76的主题聚类ARI和0.84的情感分析F1分数。

---

### 14 State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models

**link**: https://arxiv.org/pdf/2602.15858.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文在顺序决策基准上系统改变状态表示的粒度、结构和空间接地，发现轨迹总结提高性能，自然语言表示最稳健，基于文本的空间编码最有效。状态表示设计是性能的决定性因素，但当前LLMs在长horizon任务中仍脆弱。

---

### 15 CAST: Achieving Stable LLM-based Text Analysis for Data Analytics

**link**: https://arxiv.org/pdf/2602.15861.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文引入CAST框架，通过约束模型的潜在推理路径增强输出稳定性。CAST结合算法提示和先思考后表达策略，在总结和标记任务中提高稳定性。实验表明，CAST在所有基线中实现最佳稳定性，将稳定性分数提高高达16.2%，同时保持或提高输出质量。

---

### 16 Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning

**link**: https://arxiv.org/pdf/2602.15863.pdf  
**date**: 2026-02-19  
**keywords**: cs.CL  
**abs**: 本文评估上下文学习的三种提示策略：零样本、集成和解耦提示。实验表明，集成提示（自生成示例创建过程）始终优于其他策略，而解耦提示仅提供微小增益。注意力分析揭示集成提示的优势来自问题创建过程，为设计更有效的提示策略提供见解。

---

### 17 Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks

**link**: https://arxiv.org/pdf/2602.15997.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文追踪了模型训练中的能力涌现机制，发现训练始于通用表征坍缩，达到尺度不变任务特定下限；坍缩通过层自上而下传播；存在几何层次结构，表征几何先于涌现。几何度量编码粗略任务难度但不编码细粒度时间，在Pythia模型上可复现全局模式。

---

### 18 Learning Personalized Agents from Human Feedback

**link**: https://arxiv.org/pdf/2602.16173.pdf  
**date**: 2026-02-19  
**keywords**: cs.AI  
**abs**: 本文提出基于人类反馈的个性化智能体框架，通过显式每用户记忆实现智能体从实时交互中在线学习的持续个性化。框架实施三步循环：行动前寻求澄清、基于记忆检索偏好的行动、整合行动后反馈更新记忆。实验表明，显式记忆与双反馈通道结合减少初始个性化误差并实现偏好漂移的快速适应。

---

### 19 On the Power of Source Screening for Learning Shared Feature Extractors

**link**: https://arxiv.org/pdf/2602.16125.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文研究了源筛选在共享表示学习中的作用，发现对于广泛问题实例，在信息性子群体上训练足以实现极小极大最优性。形式化信息子群体概念，开发识别算法，并通过理论和实证分析验证有效性。

---

### 20 GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation

**link**: https://arxiv.org/pdf/2602.16449.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出GICDM方法，用于校正生成模型评估中的中心性现象。基于迭代上下文差异度量，引入多尺度扩展以改善性能。实验表明，GICDM解决中心性导致的失效，恢复可靠度量行为，并提高与人类判断的一致性。

---

### 21 BamaER: A Behavior-Aware Memory-Augmented Model for Exercise Recommendation

**link**: https://arxiv.org/pdf/2602.15879.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出BamaER框架，包括学习进度预测模块、记忆增强知识追踪模块和练习过滤模块。框架通过三向混合编码捕获行为交互，维护动态记忆矩阵，并将候选选择表述为多样性感知优化问题。实验在五个数据集上显示BamaER优于最先进基线。

---

### 22 Distributed physics-informed neural networks via domain decomposition for fast flow reconstruction

**link**: https://arxiv.org/pdf/2602.15883.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出分布式PINNs框架，通过时空域分解实现高效流动重建。通过参考锚点归一化策略结合解耦不对称加权解决压力不确定性，并实现高性能训练管道。验证表明，方法实现近线性强扩展性和高保真重建。

---

### 23 Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting

**link**: https://arxiv.org/pdf/2602.16188.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出时间先验条件，将时间提升为一等模态，在多个深度对模型进行条件约束。TPC将时间序列令牌附加到补丁流中，在选定层与时间嵌入交互。实验表明，TPC在长期预测任务上持续优于全微调和浅条件策略，实现最先进性能。

---

### 24 Geometric Neural Operators via Lie Group-Constrained Latent Dynamics

**link**: https://arxiv.org/pdf/2602.16209.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出基于李群的流形约束方法，对神经算子的潜在表示执行群作用更新。该方法作为高效模块，为现有神经算子施加几何归纳偏置。实验在多个偏微分方程上显示，方法将相对预测误差降低30-50%，提高长期预测保真度。

---

### 25 Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC

**link**: https://arxiv.org/pdf/2602.16456.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出LoRSum方法，通过将LoRA优化转换为近端子问题并使用交替最小二乘更新求解，避免全矩阵SVD投影。方法结合结构化度量如对角线K-FAC，保持内存效率。实验表明，LoRSum匹配或改进LoRA基线，同时保留参数效率。

---

### 26 Synthesis and Verification of Transformer Programs

**link**: https://arxiv.org/pdf/2602.16473.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文开发C-RASP编程语言的自动验证算法，通过与同步数据流程序验证建立连接，利用模型检查器和SMT求解器。同时提供从示例中学习C-RASP的算法，实现对文献基准的有效应用，包括程序优化和约束学习。

---

### 27 Factored Latent Action World Models

**link**: https://arxiv.org/pdf/2602.16229.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出分解潜在动作模型，将场景分解为独立因子，每个因子推断潜在动作并预测下一步值。这种分解结构更准确建模多实体动力学，提高视频生成质量。实验表明，FLAM在模拟和真实数据集上优于先前工作，并促进下游策略学习。

---

### 28 Fast KV Compaction via Attention Matching

**link**: https://arxiv.org/pdf/2602.16284.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文提出通过注意力匹配在潜在空间中进行快速上下文压缩的方法，构建紧凑键值以重现注意力输出。框架将问题分解为简单子问题，实现高达50倍压缩且质量损失小，推动压缩时间与质量的Pareto前沿。

---

### 29 A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models

**link**: https://arxiv.org/pdf/2602.16626.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文系统评估脑磁图数据上的样本级tokenization策略，比较可学习和非可学习tokenizer。提出基于自编码器的新方法，实验在三个数据集上进行。结果表明，简单固定tokenization策略可用于神经基础模型开发，两者均实现高重建精度和相当性能。

---

### 30 Optimizer choice matters for the emergence of Neural Collapse

**link**: https://arxiv.org/pdf/2602.16642.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG  
**abs**: 本文证明优化器选择对神经崩溃的出现起关键作用。引入新诊断指标NC0，证明在自适应优化器中使用解耦权重衰减时NC无法出现。实验显示，带耦合权重衰减的SGD和SignGD与AdamW有本质不同，且动量加速NC出现。大规模实验证实理论结果。

---

### 31 Knowledge-Embedded Latent Projection for Robust Representation Learning

**link**: https://arxiv.org/pdf/2602.16709.pdf  
**date**: 2026-02-19  
**keywords**: cs.LG, latent space  
**abs**: 本文提出知识嵌入的潜在投影模型，利用语义辅助信息正则化表示学习。通过核映射将列嵌入建模为语义嵌入的平滑函数，开发两步估计程序。建立误差界和收敛保证，模拟和真实应用证明方法有效性。

---

### 32 Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage

**link**: https://arxiv.org/pdf/2602.16192.pdf  
**date**: 2026-02-19  
**keywords**: cs.AI, LLM Memory  
**abs**: 本文探索实现人工超级智能的“记忆”设计概念，强调“先存储后按需提取”方法以避免信息丢失。还强调从概率经验集合中发现深层见解和共享存储经验提高效率。讨论限制这些方向的主要挑战并提出研究课题。

---

### 33 Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach

**link**: https://arxiv.org/pdf/2602.16481.pdf  
**date**: 2026-02-19  
**keywords**: cs.AI  
**abs**: 本文探索使用LLMs作为Causal ABA的不完美专家，从变量描述中提取语义结构先验，并与条件独立证据结合。实验在标准基准和合成图上表明，方法实现最先进性能，并引入协议减轻记忆偏差。

---

### 34 Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs

**link**: https://arxiv.org/pdf/2602.16512.pdf  
**date**: 2026-02-19  
**keywords**: cs.AI  
**abs**: 本文引入思维框架，用于构建和优化动态推理方案。框架内置超参数调优、提示优化、并行执行和智能缓存。通过在FoT中实现思维树等方案，实证表明方法实现更快执行速度、更低成本和更好任务分数。