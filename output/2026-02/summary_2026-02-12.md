### Driving Reaction Trajectories via Latent Flow Matching
**link**: https://arxiv.org/pdf/2602.10476.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 本文提出LatentRxnFlow，一种新的反应预测范式，将反应建模为锚定在热力学产物状态的连续潜在轨迹。该方法基于条件流匹配，直接从标准反应物-产物对中学习时间依赖的潜在动力学，无需机制注释或中间标签。除了在USPTO基准上实现最先进性能外，连续公式还揭示了完整的生成轨迹，支持轨迹级诊断，如定位失败模式、通过门控推理减轻错误，以及利用轨迹几何属性提供认知不确定性信号，有助于优先考虑可靠预测结果并标记模糊案例。

---

### Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks
**link**: https://arxiv.org/pdf/2602.10496.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 本文研究了过参数化Transformer模型在模块化算术任务中的学习动态几何结构。主要发现是，尽管在高维参数空间（d=128）中运行，Transformer训练轨迹迅速坍缩到3-4维的低维执行流形上。这种维度坍缩在随机种子和中等任务难度下具有鲁棒性，流形在参数空间中的方向因运行而异。研究表明，这种几何结构是多个经验观察现象的基础：（1）注意力集中作为执行流形内路由坐标的饱和出现；（2）随机梯度下降（SGD）在投影到执行子空间时表现出近似可积动力学；（3）稀疏自编码器捕获辅助路由结构但无法隔离执行本身，后者仍分布在低维流形上。这些发现为理解Transformer学习提供了统一几何框架，对可解释性、训练课程设计和过参数化作用有启示意义。

---

### Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics
**link**: https://arxiv.org/pdf/2602.10885.pdf
**date**: 2026-02-12
**keywords**: cs.AI, latent reasoning
**abs**: 尽管思维链（CoT）在大语言模型（LLM）推理中发挥着关键作用，但直接对其进行奖励存在困难：训练奖励模型需要大量人工标注，且静态奖励模型难以应对不断变化的CoT分布和奖励黑客攻击。为解决这些挑战，本文提出了RLCER（基于自进化评分准则的思维链监督强化学习）方法，该方法受近期自进化训练方法的启发，通过自提出和自进化的评分准则对CoT进行奖励，从而增强以结果为中心的RLVR。研究表明，即使没有结果奖励，自提出和自进化的评分准则也能提供可靠的CoT监督信号，使RLCER优于以结果为中心的RLVR。此外，当这些自提出的评分准则用作提示中的提示时，还能进一步提高推理时的性能。

---

### Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models
**link**: https://arxiv.org/pdf/2602.10224.pdf
**date**: 2026-02-12
**keywords**: LLM Memory
**abs**: 强化学习与可验证奖励（RLVR）已成为增强大型语言模型（LLM）推理能力的有效方法。然而，RLVR面临元学习瓶颈：缺乏人类学习周期中超越练习和验证的错误归因与经验内化机制，限制了细粒度信用分配和可重用知识形成。本文提出元经验学习（MEL）框架，将自蒸馏的元经验整合到模型的参数记忆中。基于标准RLVR，引入利用LLM自验证能力对正确与错误轨迹进行对比分析的设计，识别推理错误的精确分叉点并总结为可泛化的元经验。通过最小化负对数似然将元经验内化到LLM参数记忆中，形成连接正确与错误推理轨迹的语言模型奖励信号，促进知识重用。实验结果表明，MEL在不同模型规模上实现3.92%--4.73%的Pass@1提升。

---

### Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents
**link**: https://arxiv.org/pdf/2602.10226.pdf
**date**: 2026-02-12
**keywords**: LLM Rec
**abs**: 优化大规模机器学习系统（如全球视频平台的推荐模型）需要导航庞大的超参数搜索空间，更关键的是设计复杂的优化器、架构和奖励函数以捕捉细微的用户行为。这些领域的显著改进传统上依赖大量手动迭代测试新假设。本文提出一种自进化系统，利用大型语言模型（LLM，特别是Google Gemini系列）在端到端自动化工作流中自主生成、训练和部署高性能复杂模型变更。该系统由离线代理（内循环）和在线代理（外循环）组成：离线代理使用代理指标执行高通量假设生成，在线代理在生产环境中根据延迟的核心业务指标验证候选方案。代理表现出深度推理能力，发现优化算法和模型架构的新改进，并制定针对长期用户参与的创新奖励函数。通过YouTube的多个成功生产部署证明了该方法的有效性，证实LLM驱动的自主进化在开发速度和模型性能上均超越传统工程工作流。

---

### Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs
**link**: https://arxiv.org/pdf/2602.10388.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）的训练后数据多样性对下游任务性能至关重要。现有方法多采用文本指标量化多样性，但对决定下游性能的任务相关特征信号较弱。本文提出特征激活覆盖率（FAC），在可解释的特征空间中衡量数据多样性，并基于此提出FAC Synthesis框架：利用稀疏自编码器识别种子数据中缺失的特征，然后生成明确反映这些特征的合成样本。实验表明，该方法在指令跟随、毒性检测、奖励建模和行为引导等多种任务上持续提升数据多样性和下游性能，并发现跨模型家族（LLaMA、Mistral、Qwen）存在共享的可解释特征空间，支持跨模型知识迁移。

---

### Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation
**link**: https://arxiv.org/pdf/2602.10430.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 基于策略的强化学习（RL）已成为生成式推荐中优化序列用户交互的主流范式，但应用于离线历史日志时，低质量数据的主导导致严重的模型崩溃。本文首先建立了排斥优化的发散理论，揭示负梯度更新在离线策略训练中会引发指数级强度爆炸，阐明现有方法无法兼顾方差减少和噪声模仿的困境。为解决此问题，本文将目标重构为乐观分布鲁棒优化（DRO）问题，提出分布鲁棒策略优化（DRPO）。证明硬过滤是该DRO目标的精确解，使DRPO能最优恢复高质量行为，同时严格丢弃引发发散的噪声。实验表明DRPO在混合质量推荐基准上达到最先进性能。

---

### Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering
**link**: https://arxiv.org/pdf/2602.10437.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 稀疏自编码器（SAEs）可将语言模型激活分解为可解释特征，但现有方法仅揭示哪些特征被激活，而非放大时哪些会改变模型输出。本文引入控制强化学习（CRL），训练策略在每个token处选择SAE特征进行引导，生成可解释的干预日志。自适应特征掩码促进特征多样性同时保持单特征可解释性。该框架具备新分析能力：分支点跟踪定位特征选择决定输出正确性的token；评论家轨迹分析区分策略限制与价值估计误差；层间比较揭示早期层的句法特征和后期层的语义特征。在Gemma-2 2B模型上的MMLU、BBQ、GSM8K、HarmBench和XSTest任务中，CRL在提供逐token干预日志的同时实现性能提升，确立了学习特征引导作为机制可解释性工具，补充静态特征分析与动态干预探测。

---

### Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets
**link**: https://arxiv.org/pdf/2602.10583.pdf
**date**: 2026-02-12
**keywords**: latent space
**abs**: 标准自回归语言模型从固定词汇表逐token生成文本，将token采样视为动作时会形成树状状态空间，限制了灵活性和表达能力。最近的研究通过采样检索到的文本片段引入动态词汇，但忽略了同一语句可由不同长度的片段组成，缺乏对有向无环图（DAG）状态空间的显式建模。这导致对组合路径的探索受限，并偏向所选路径。生成流网络（GFlowNets）在高效探索和泛化状态空间（尤其是DAG结构）方面表现强大。然而，先前基于GFlowNets的语言模型在token级别运行，仍局限于树状空间，限制了其潜力。本工作提出Flow of SpanS（FOSS），一个用于片段生成的原则性GFlowNets框架。FoSS通过灵活分割检索到的文本构建动态片段词汇表，确保DAG结构的状态空间，使GFlowNets能够探索多样化的组合路径并提高泛化能力。通过专门的奖励模型，FoSS生成多样化、高质量的文本。实验表明，FoSS在文本生成上比Transformer提高了高达12.5%的MAUVE分数，并在知识密集型任务上实现了3.5%的增益，持续优于最先进的方法。扩展实验进一步证明，FoSS受益于更大的模型、更多的数据和更丰富的检索语料库，保持了对强基线的优势。

---

### Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation
**link**: https://arxiv.org/pdf/2602.10699.pdf
**date**: 2026-02-12
**keywords**: LLM Rec
**abs**: 通过自回归模型进行生成式推荐已将检索和排序统一为单一条件生成框架。然而，使用强化学习（RL）微调这些模型时，往往面临根本的概率-奖励不匹配问题。传统的似然主导解码（如束搜索）表现出对局部可能前缀的短视偏见，导致两个关键失败：（1）探索不足，即低概率分支中的高奖励项目被过早剪枝且很少被采样；（2）优势压缩，即共享高概率前缀的轨迹获得高度相关的奖励，组内方差低，为RL提供弱比较信号。为解决这些挑战，我们提出V-STAR，一个基于价值引导的采样和树状优势强化框架。V-STAR通过两个协同组件形成自进化循环。首先，开发价值引导高效解码（VED）以识别决定性节点并选择性地深化高潜力前缀。这在无需 exhaustive 树搜索的情况下提高了探索效率。其次，提出Sibling-GRPO，利用诱导的树拓扑结构计算兄弟相对优势，并将学习信号集中在决定性的分支决策上。在离线和在线数据集上的大量实验表明，V-STAR优于最先进的基线，在严格的延迟约束下提供卓越的准确性和候选集多样性。

---

### Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models
**link**: https://arxiv.org/pdf/2602.10953.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 该论文提出了SOAR，一种无需训练的解码算法，用于扩散语言模型（DLMs）。标准解码采用贪婪规则可能导致次优解，尤其在推理密集型提示上。SOAR根据模型不确定性自适应调整行为：低置信度时扩展搜索替代解掩蔽决策以避免过早承诺，高置信度时并行解码多个位置以减少去噪迭代次数。在数学推理（GSM8K）和代码生成（MBPP、HumanEval）基准上，SOAR在Dream-7B和LLaDA-8B模型上提升了生成质量，同时保持了竞争性推理速度，为DLM解码提供了平衡质量与效率的实用方法。

---

### Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers
**link**: https://arxiv.org/pdf/2602.10959.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 旋转位置嵌入（RoPE）广泛用于大型语言模型，通过乘法旋转编码token位置，但其在长上下文长度下的行为表征尚不充分。本研究将RoPE重新解释为应用于复杂振荡器组的相位调制，从而能够通过经典信号处理理论进行分析，为长上下文Transformer中RoPE基的理论边界提供了新的见解，与LLM的记忆机制（处理长上下文信息）相关。

---

### Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks
**link**: https://arxiv.org/pdf/2602.10780.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 机器学习模型在日常生活中的应用日益广泛，因此成为对抗性攻击者试图操纵我们所交互系统的目标。一个众所周知的漏洞是通过恶意训练数据或训练过程在神经网络中植入的后门，后门可通过在输入中包含特定触发器来诱导不需要的行为。现有的缓解方法包括过滤训练数据、修改模型或对样本进行昂贵的输入修改，但当易受攻击的模型已部署时，这些策略要么无效要么效率低下。为此，我们提出了推理时后门缓解方法FIRE（Feature-space Inference-time REpair）。我们假设触发器会在模型内部表示中引起结构化且可重复的变化，并将触发器视为层间 latent space 中的方向，可反向应用这些方向来纠正推理机制。通过操纵模型的 latent 表示并沿着后门方向移动中毒样本的特征以中和触发器，我们利用受后门攻击的模型来对抗其自身。评估表明，FIRE计算开销低，在各种攻击、数据集和网络架构的图像基准测试中优于当前的运行时缓解方法。

---

### Beyond Confidence: The Rhythms of Reasoning in Generative Models
**link**: https://arxiv.org/pdf/2602.10816.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）展现出令人印象深刻的能力，但对输入上下文的细微变化较为敏感，影响了可靠性。传统指标如准确率和困惑度无法评估局部预测的鲁棒性，因为归一化的输出概率可能掩盖LLM内部状态对扰动的潜在恢复能力。我们引入令牌约束边界（δ_TCB），这是一种新颖指标，用于量化LLM在其主导的下一个令牌预测发生显著变化前能承受的最大内部状态扰动。δ_TCB与输出嵌入空间（一种 latent space）几何特性内在相关，为模型内部预测承诺的稳定性提供见解。实验表明，δ_TCB与有效的提示工程相关，并揭示了在上下文学习和文本生成中被困惑度忽略的关键预测不稳定性，为分析和提升LLM预测的上下文稳定性提供了原则性的补充方法。

---

### Conversational Behavior Modeling Foundation Model With Multi-Level Perception
**link**: https://arxiv.org/pdf/2602.11065.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 人类对话由一系列隐含的思维链组织，并表现为定时的言语行为。捕捉这一感知路径是构建自然全双工交互系统的关键。本文引入一个框架，将此过程建模为多层次感知，然后通过思维图（Graph-of-Thoughts, GoT）对会话行为进行推理。该方法利用层次化标签方案形式化意图到行动的路径，预测高层交际意图和低层言语行为，以学习它们的因果和时间依赖关系。为训练该系统，研究人员开发了高质量语料库，将可控、事件丰富的对话数据与人工标注标签配对。GoT框架将流式预测构建为演化图，使Transformer能够预测下一个言语行为，为其决策生成简洁理由，并动态优化推理过程。在合成和真实双工对话上的实验表明，该框架实现了稳健的行为检测，生成可解释的推理链，并为全双工口语对话系统中的会话推理基准测试奠定了基础。

---

### Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away
**link**: https://arxiv.org/pdf/2602.11096.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 基于强化学习（RL）的显式思维链后训练（如GRPO）可提升多模态大规模推理模型（MLRMs）的推理能力，但近期研究表明这可能同时降低安全对齐性并提高越狱成功率。本文提出SafeThink，一种轻量级推理时防御方法，将安全恢复视为满足约束而非最大化目标。SafeThink通过安全奖励模型监控演化的推理轨迹，仅当安全阈值被违反时，有条件地注入优化的简短纠正前缀（“Wait, think safely”）。在六个开源MLRMs和四个越狱基准测试（JailbreakV-28K、Hades、FigStep和MM-SafetyBench）上的评估显示，SafeThink将攻击成功率降低30-60%（例如，LlamaV-o1在JailbreakV-28K上从63.33%降至5.74%，R1-Onevision在Hades上从69.07%降至5.65%），同时保持推理性能（MathVista准确率从65.20%降至65.00%）。实验的关键发现是：安全恢复通常只需几步早期引导，干预前1-3个推理步骤通常足以将整个生成重定向至安全完成。

---

### When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning
**link**: https://arxiv.org/pdf/2602.10560.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 尽管长上下文推理对各种现实应用至关重要，但大型语言模型（LLMs）在上下文长度增加时会面临性能下降的挑战。现有工作MemAgent尝试通过类似RNN的循环逐块处理上下文并更新文本记忆来解决此问题，但这种简单的循环记忆更新存在两个关键缺点：（i）记忆可能因不加区分地更新（即使是无证据的块）而迅速膨胀；（ii）循环缺乏退出机制，导致在收集到足够证据后仍进行不必要的计算。为解决这些问题，本文提出GRU-Mem，其集成了两个文本控制门以实现更稳定高效的长上下文推理。具体而言，在GRU-Mem中，只有当更新门打开时记忆才会更新，且一旦退出门打开，循环立即终止。为赋予模型此类能力，在端到端强化学习中引入两个奖励信号r^update和r^exit，分别奖励正确的更新和退出行为。在各种长上下文推理任务上的实验证明了GRU-Mem的有效性和效率，其通常优于普通MemAgent，推理速度最高提升400%。

---

### Online Causal Kalman Filtering for Stable and Effective Policy Optimization
**link**: https://arxiv.org/pdf/2602.10609.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 大型语言模型的强化学习面临高方差的token级重要性采样（IS）比率问题，这会破坏大规模策略优化的稳定性。为提高稳定性，现有方法通常对序列中所有token使用固定的序列级IS比率或单独调整每个token的IS比率，从而忽略了序列中token间的时间离策略偏差。本文首先实证发现，token级别的局部离策略偏差在结构上不一致，这可能扭曲相邻token的策略梯度更新并导致训练崩溃。为解决此问题，提出KPO（Online Causal Kalman Filtering for Policy Optimization）。具体而言，将期望的IS比率建模为随token演化的潜状态，并应用卡尔曼滤波器基于过去token的状态在线自回归地更新该状态，而不依赖未来token。过滤后的IS比率保留了token级别的局部结构感知变化，同时显著平滑噪声峰值，从而实现更稳定有效的策略更新。在具有挑战性的数学推理数据集上的实验表明，KPO优于最先进的方法。

---

### Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling
**link**: https://arxiv.org/pdf/2602.10623.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 从人类偏好中学习的奖励模型是通过强化学习从人类反馈（RLHF）对齐大型语言模型（LLMs）的核心，但由于噪声注释和响应长度或风格等系统偏差，它们常易受奖励黑客攻击。本文提出BNRM（Bayesian Non-Negative Reward Model），这是一个将非负因子分析集成到Bradley-Terry（BT）偏好模型中的原则性奖励建模框架。BNRM通过稀疏、非负的潜因子生成过程表示奖励，该过程在两个互补级别运行：实例特定的潜变量诱导解纠缠的奖励表示，而全局潜因子的稀疏性作为抑制虚假相关性的隐式去偏机制。这种“先解纠缠后去偏”的结构实现了鲁棒的不确定性感知奖励学习。为将BNRM扩展到现代LLMs，开发了一个以深度模型表示为条件的摊销变分推理网络，允许高效的端到端训练。大量实证结果表明，BNRM显著减轻了奖励过优化，提高了分布偏移下的鲁棒性，并产生了比强基线更可解释的奖励分解。

---

### Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents
**link**: https://arxiv.org/pdf/2602.10715.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 长期对话记忆是基于LLM的对话系统的核心能力，但现有基准和评估协议主要关注表面级别的事实回忆。在现实交互中，适当的响应往往依赖于用户状态、目标或价值观等未被显式查询的隐式约束。为评估这种场景，本文引入LoCoMo-Plus，一个用于评估“线索-触发语义断开”下认知记忆的基准，其中模型必须在长对话上下文中保留并应用隐式约束。进一步表明，传统的字符串匹配指标和显式任务类型提示与此类场景不匹配，并提出基于约束一致性的统一评估框架。在不同骨干模型、基于检索的方法和记忆系统上的实验表明，认知记忆仍然具有挑战性，且揭示了现有基准未捕获的失败模式。

---

### Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens
**link**: https://arxiv.org/pdf/2602.10229.pdf
**date**: 2026-02-12
**keywords**: latent space, latent reasoning
**abs**: 虽然显式思维链(CoT)为大型语言模型(LLMs)提供了强大的推理能力，但它要求模型将每个中间步骤用文本标记表达出来，从而将模型思维限制在离散词汇空间中。最近，连续潜在空间中的推理已成为一种有前景的替代方案，能够实现更稳健的推理和超越离散标记约束的灵活计算。然而，当前的潜在范式经常遭受特征崩溃和不稳定性问题，这源于将隐藏状态循环用作输入嵌入时的分布不匹配，或依赖辅助模型时的对齐问题。为解决这些问题，本文提出了潜在思维调优(LT-Tuning)框架，重新定义了潜在思维的构建和部署方式。该方法不单纯依赖原始隐藏状态，而是引入了上下文-预测-融合机制，联合利用上下文隐藏状态和来自词汇嵌入空间的预测语义指导。结合渐进式三阶段课程学习管道，LT-Tuning还能够在潜在和显式思维模式之间动态切换。实验表明，该方法优于现有的潜在推理基线，有效缓解了特征崩溃并实现了稳健的推理准确性。

---

### Learning to Evict from Key-Value Cache
**link**: https://arxiv.org/pdf/2602.10238.pdf
**date**: 2026-02-12
**keywords**: LLM Memory
**abs**: 大型语言模型(LLMs)的规模增长使得高效推理面临挑战，主要是由于自回归键值(KV)缓存的内存需求。现有的驱逐或压缩方法虽然降低了成本，但依赖于启发式方法，如新近度或过去的注意力分数，这些仅作为令牌未来效用的间接代理，并引入计算开销。本文将KV缓存驱逐重新定义为强化学习(RL)问题：学习通过预测令牌对未来解码的有用性来对令牌进行排序。为此，本文引入了KV策略(KVP)，这是一个轻量级的每头RL代理框架，仅使用键和值向量在预计算的生成轨迹上进行训练。每个代理学习由未来效用引导的专门驱逐策略，评估所有缓存预算下的排序质量，不需要修改底层LLM或额外的推理。在长上下文基准RULER和多轮对话基准OASST2-4k上对两个不同模型系列进行的评估表明，KVP显著优于基线。此外，在标准下游任务(如LongBench、BOOLQ、ARC)上的零样本测试表明，KVP能够很好地泛化到训练分布之外和更长的上下文长度。这些结果表明，学习预测未来令牌效用是一种强大且可扩展的自适应KV缓存管理范式。

---

### Canvas-of-Thought: Grounding Reasoning via Mutable Structured States
**link**: https://arxiv.org/pdf/2602.10494.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 尽管思维链（CoT）提示显著提升了多模态大型语言模型（MLLMs）的推理能力，但仅依赖线性文本序列仍是复杂任务的瓶颈。现有方法常将推理历史视为不可变流，局部错误修正需生成冗长后续修正或重新生成整个上下文，增加了标记消耗和认知负荷，尤其在几何和SVG设计等高维领域，文本形式的CoT缺乏明确视觉指导，限制推理精度。为此，本文提出Canvas-of-Thought（Canvas-CoT），利用HTML Canvas作为外部推理 substrate，使模型能执行基于DOM的原子CRUD操作，实现就地状态修订而不干扰周围上下文，明确维护“真值”。此外，集成基于渲染的批判循环作为硬约束验证器，提供显式视觉反馈以解决文本难以表达的复杂任务。在VCode、RBench-V和MathVista上的实验表明，Canvas-CoT显著优于现有基线，为上下文高效的多模态推理建立新范式。

---

### How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning
**link**: https://arxiv.org/pdf/2602.10622.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 仅解码器大型语言模型越来越多地用作用户表示学习的行为编码器，但注意力掩码对用户嵌入质量的影响尚未得到充分探索。本文在统一的对比学习框架中系统研究了因果、混合和双向注意力掩码，该框架在整合长期异质用户行为的大规模真实支付宝数据上训练。为改善从因果注意力过渡到双向注意力时的训练动态，提出梯度引导软掩码（Gradient-Guided Soft Masking），在线性调度器之前应用基于梯度的预热，在优化过程中逐渐开放未来注意力。在涵盖预测、偏好和营销敏感性任务的9个工业用户认知基准上的评估表明，与因果、混合和仅调度器基线相比，该方法始终产生更稳定的训练和更高质量的双向表示，同时与解码器预训练兼容。

---

### UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory
**link**: https://arxiv.org/pdf/2602.10652.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 自进化记忆是基于大型语言模型（LLMs）的智能体的可训练参数，其中提取（从经验中提炼见解）和管理（更新记忆库）必须紧密协调。现有方法主要优化记忆管理，而将记忆提取视为静态过程，导致泛化能力差，智能体积累实例特定噪声而非稳健记忆。为此，本文提出统一记忆提取与管理（UMEM）框架，该自进化智能体框架联合优化大型语言模型以同时提取和管理记忆。为减轻对特定实例的过拟合，引入语义邻域建模，并通过GRPO以邻域级边际效用奖励优化模型。该方法通过评估跨语义相关查询集群的记忆效用确保记忆泛化性。在五个基准上的广泛实验表明，UMEM显著优于竞争力强的基线，在多轮交互任务中实现高达10.67%的改进，并且在持续进化过程中保持单调增长曲线。

---

### Diffusion-Pretrained Dense and Contextual Embeddings
**link**: https://arxiv.org/pdf/2602.11151.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 本文介绍了pplx-embed系列多语言嵌入模型，该模型基于扩散预训练语言模型骨干，通过多阶段对比学习实现网页级检索。利用扩散预训练的双向注意力机制，模型能够捕捉段落中的全面双向上下文，并采用均值池化和延迟分块策略来更好地保留长文档的全局上下文。该研究发布了两种模型类型：pplx-embed-v1用于标准检索任务，pplx-embed-context-v1则用于将全局文档上下文融入段落表示的上下文嵌入任务。实验表明，pplx-embed-v1在MTEB（多语言v2）、MTEB（代码）、MIRACL、BERGEN和ToolRet等检索基准上表现出竞争力，而pplx-embed-context-v1在ConTEB基准上创下新纪录。此外，该模型在包含数千万文档的大规模实际搜索场景中也展现了强大性能。

---

### How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge
**link**: https://arxiv.org/pdf/2602.10210.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）在需要最新信息和多跳推理的知识密集型问题上仍存在困难。通过混合外部知识（如非结构化文本和结构化知识图谱）增强LLMs为昂贵的持续预训练提供了一种有前景的替代方案。然而，许多现有基准与LLM预训练数据重叠，导致难以区分真正的检索推理与参数记忆。本文引入HybridRAG-Bench，这是一个用于评估混合知识上检索密集型多跳推理的基准构建框架。该框架自动耦合来自arXiv近期科学文献的非结构化文本和结构化知识图谱表示，并生成基于显式推理路径的知识密集型问答对。实验表明，HybridRAG-Bench奖励真正的检索和推理而非参数记忆，为评估混合知识增强推理系统提供了原则性测试平台。

---

### ELROND: Exploring and decomposing intrinsic capabilities of diffusion models
**link**: https://arxiv.org/pdf/2602.10216.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 扩散模型的单个文本提示通常会产生由随机过程决定的多种视觉输出，使用户无法直接控制图像中出现的特定语义变化。现有无监督方法尝试通过输出特征分析这些变化，但忽略了潜在的生成过程。本文提出了一个框架，通过反向传播固定提示的随机实现之间的差异来收集梯度，然后使用主成分分析（PCA）或稀疏自编码器将这些梯度分解为有意义的引导方向，从而在输入嵌入空间中直接解耦语义方向。该方法有三个关键贡献：（1）分离可解释的、可引导的方向，以实现对单个概念的精确细粒度控制；（2）通过重新引入丢失的多样性，有效缓解蒸馏模型中的模式崩溃；（3）基于发现的子空间维度，建立特定模型下概念复杂性的新估计器。

---

### Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation
**link**: https://arxiv.org/pdf/2602.10249.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 本文提出了一种上下文感知推荐系统，该系统利用学生在课程中提交的源代码嵌入来建模其编程技能。这些嵌入预测学生在多个编程主题上的技能，生成与未见过的作业问题所需技能相匹配的学生档案。为了生成推荐，系统计算学生档案与问题技能向量之间的余弦相似度，并根据与每个学生当前能力的对齐程度对练习进行排序。通过对大学introductory编程课程的真实数据进行评估，结果表明，Jina嵌入在大多数技能上优于TF-IDF、CodeBERT-cpp和GraphCodeBERT。此外，该系统比基于正确性或解决时间的基线产生了更合适的推荐，表明预测的编程技能为问题推荐提供了更强的信号。

---

### Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models
**link**: https://arxiv.org/pdf/2602.10282.pdf
**date**: 2026-02-12
**keywords**: latent reasoning
**abs**: 大型语言模型（LLMs）在识别定性因果关系方面显示出潜力，但其在连续域中执行定量因果推理（估计参数化功能关系的效应大小）的能力仍未得到充分探索。本文引入Linear-LLM-SCM，一个即插即用的基准框架，用于评估LLMs在线性高斯结构因果模型（SCM）参数化中的表现（当给定DAG时）。该框架将DAG分解为局部父子集，并提示LLM为每个节点生成回归风格的结构方程，然后聚合并与可用的真实参数进行比较。实验表明，此类基准任务存在若干挑战，即在某些模型中结果具有强烈的随机性，以及在连续域中易受DAG错误指定（通过虚假边）的影响。在不同模型中，观察到某些设置下系数估计的显著变异性，以及对结构和语义扰动的敏感性，突出了LLMs作为定量因果参数化工具的当前局限性。

---

### What Does Preference Learning Recover from Pairwise Comparison Data?
**link**: https://arxiv.org/pdf/2602.10286.pdf
**date**: 2026-02-12
**keywords**: LLM Rec
**abs**: 成对偏好学习是机器学习的核心，近期在语言模型与人类偏好对齐方面有应用。典型数据集包含三元组(x, y+, y-)，其中响应y+在上下文x中优于y-。Bradley-Terry（BT）模型是主要方法，将偏好概率建模为潜在分数差异的函数。标准实践假设数据遵循该模型并据此学习潜在分数。然而，真实数据可能违反此假设，且尚不清楚BT学习在此情况下会恢复什么。本文从三元组比较数据出发，通过条件偏好分布（CPRD）形式化其编码的偏好信息。给出了BT适用于建模CPRD的精确条件，并确定了控制样本效率的因素——即边际和连通性。这些结果为理解偏好学习实际恢复的内容提供了以数据为中心的基础。

---

### R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting
**link**: https://arxiv.org/pdf/2602.10312.pdf
**date**: 2026-02-12
**keywords**: latent reasoning
**abs**: R2RAG-Flood是一个用于风暴后财产损失即时预测的推理增强型无训练检索增强生成框架。该框架在现有监督表格预测器的基础上，构建了一个以推理为中心的知识库，包含带标签的表格记录，每个样本包括结构化预测器、紧凑的自然语言文本模式摘要和模型生成的推理轨迹。在推理过程中，R2RAG-Flood发出上下文增强提示，从附近的地理空间邻居和规范类原型中检索并条件化相关推理轨迹，使大型语言模型主干能够模拟和适应先前的推理，而无需学习新的任务特定参数。预测遵循两阶段过程：首先确定财产损失是否发生，然后在三级财产损失程度分类中细化严重程度，并通过条件降级步骤纠正过度预测的严重程度。在德克萨斯州哈里斯县的案例研究中，直接在结构化预测器上训练的监督表格基线实现了0.714的总体准确率和0.859的中高损失类别准确率。在七个大型语言模型主干上，R2RAG-Flood达到0.613至0.668的总体准确率和0.757至0.896的损失类别准确率，接近监督基线，同时为每个预测生成结构化理由。使用从API定价和GPU实例成本导出的每成本严重性效率指标，轻量级R2RAG-Flood变体显示出比监督表格基线和更大语言模型显著更高的效率，且无需特定任务的训练或微调。

---

### Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution
**link**: https://arxiv.org/pdf/2602.10357.pdf
**date**: 2026-02-12
**keywords**: latent space
**abs**: 对比学习已成为学习可泛化表示的强大框架，但其理论理解仍有限，特别是在现实应用中普遍存在的不平衡数据分布下。这种不平衡会降低表示质量并导致模型行为偏差，但对这些影响的严格表征尚缺乏。本文开发了一个理论框架，用于分析不平衡数据下基于Transformer编码器的对比学习训练动态。结果表明，神经元权重通过三个不同的训练阶段演化，对多数特征、少数特征和噪声具有不同的动态。进一步表明，少数特征会降低表示能力，增加对更复杂架构的需求，并阻碍真实特征与噪声的分离。受这些神经元级行为的启发，本文表明剪枝可以恢复因不平衡而退化的性能并增强特征分离，提供了概念见解和实用指导。主要理论发现通过数值实验得到验证。

---

### Simple LLM Baselines are Competitive for Model Diffing
**link**: https://arxiv.org/pdf/2602.10371.pdf
**date**: 2026-02-12
**keywords**: latent space
**abs**: 标准LLM评估仅测试评估者设计的能力或倾向，遗漏了意外差异，如模型修订之间的行为变化或突发的错位倾向。模型差异分析通过自动呈现系统性行为差异来解决此限制。近期方法包括生成自然语言描述的基于LLM的方法和识别可解释特征的稀疏自编码器（SAE）-基于方法。然而，尚无这些方法的系统比较，也没有既定的评估标准。本文通过提出关键需求（泛化性、趣味性和抽象级别）的评估指标来解决此差距，并使用这些指标比较现有方法。结果表明，改进的基于LLM的基线与基于SAE的方法性能相当，同时通常呈现更抽象的行为差异。

---

### Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM
**link**: https://arxiv.org/pdf/2602.10801.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）取得了显著成功，然而内容生成失真（幻觉）的出现限制了其实际应用。幻觉的核心原因在于LLMs缺乏对其存储的内部知识的认知，无法像人类一样对超出其内部知识边界的问题表达自己的知识状态。然而，现有关于知识边界表达的研究主要集中在白盒LLMs上，而适用于仅提供API访问但不披露内部参数的黑盒LLMs的方法在很大程度上尚未被探索。在此背景下，本文提出了LSCL（LLM监督置信学习），一种基于深度学习的黑盒LLMs知识边界表达方法。该方法基于知识蒸馏框架设计了一个深度学习模型，将黑盒LLM的输入问题、输出答案和token概率作为输入，构建了输入与模型内部知识状态之间的映射，实现了黑盒LLM知识边界的量化和表达。在多个公开数据集和多个主流黑盒LLMs上进行的实验表明，LSCL能有效帮助黑盒LLMs准确表达其知识边界，在准确率和召回率等指标上显著优于现有基线模型。此外，考虑到部分黑盒LLMs不支持访问token概率的场景，本文还提出了一种自适应替代方法，其性能接近LSCL且优于基线模型。

---

### C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution
**link**: https://arxiv.org/pdf/2602.10874.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 自动提示优化是提升大型语言模型（LLMs）性能的一个有前景的方向。然而，现有方法常受噪声和冲突更新信号的困扰。在本研究中，我们提出了C-MOP（基于聚类的动量优化提示）框架，通过边界感知对比采样（BACS）和动量引导语义聚类（MGSC）来稳定优化过程。具体而言，BACS利用批次级信息挖掘三元特征——难负样本、锚点和边界对，以精确刻画正负提示样本的典型表示和决策边界。为解决语义冲突，MGSC引入了带时间衰减的文本动量机制，从迭代过程中波动的梯度中提取持久的共识。大量实验表明，C-MOP持续优于PromptWizard和ProTeGi等最先进基线，平均增益分别为1.58%和3.35%。值得注意的是，C-MOP使具有30亿激活参数的通用LLM能够超越700亿参数的领域特定密集型LLM，突显了其在驱动精确提示进化方面的有效性。

---

### Embedding Inversion via Conditional Masked Diffusion Language Models
**link**: https://arxiv.org/pdf/2602.11047.pdf
**date**: 2026-02-12
**keywords**: cs.CL
**abs**: 我们将嵌入反转构建为条件掩码扩散问题，通过迭代去噪并行恢复所有token，而非顺序自回归生成。掩码扩散语言模型通过自适应层归一化以目标嵌入为条件，仅需通过一个7800万参数的模型进行8次前向传递，且无需访问目标编码器。在三个嵌入模型的32-token序列上，该方法实现了81.3%的token准确率和0.87的余弦相似度。

---

### LUCID: Attention with Preconditioned Representations
**link**: https://arxiv.org/pdf/2602.10410.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 基于softmax的点积注意力是Transformer架构的基石，能够实现上下文学习等显著能力。然而，随着上下文长度的增加，softmax函数的一个基本局限性显现出来：它倾向于将概率质量扩散到无关标记上，从而在长序列场景中降低性能。此外，通过降低softmax温度来增强焦点的尝试会因梯度消失而阻碍学习能力。我们引入LUCID注意力，这是一种对注意力概率应用预条件器的架构修改。该预条件器源自指数化的键-键相似度，可最小化再生核希尔伯特空间中键之间的重叠，从而允许查询在大量键中准确聚焦于重要键，且计算复杂度与标准注意力相同。此外，LUCID基于预条件器的检索方法无需低温，避免了相关的学习能力问题。我们通过训练约10亿参数的语言模型（在多达128K标记上进行评估）来验证我们的方法。结果表明，我们的方法在长上下文检索任务（特别是BABILong、RULER、SCROLLS和LongBench的检索任务）上取得了显著收益。例如，LUCID在BABILong上实现了高达18%的改进，在RULER多针性能上比标准注意力提高了14%。

---

### Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning
**link**: https://arxiv.org/pdf/2602.10420.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 流匹配已成为生成建模的强大框架，最近的实证成功突显了信号空间预测（x预测）的有效性。在这项工作中，我们研究了将该范式转移到二进制流形的情况，这是离散数据生成建模的基本设置。虽然x预测仍然有效，但我们发现当它与基于速度的目标（v损失）结合时会出现潜在的结构不匹配，导致时间相关的奇异权重，放大梯度对近似误差的敏感性。受此观察启发，我们将预测-损失对齐形式化为流匹配训练的必要条件。我们证明，将目标与信号空间（x损失）重新对齐可以消除奇异权重，产生均匀有界的梯度，并在均匀时间步采样下实现稳健训练，而无需依赖启发式调度。最后，在确保对齐后，我们检查了二进制数据特有的设计选择，揭示了概率目标（如交叉熵）和几何损失（如均方误差）之间依赖于拓扑的区别。这些结果共同为二进制及相关离散域上的稳健流匹配提供了理论基础和实用指南，将信号空间对齐定位为稳健扩散学习的关键原则。

---

### Learning Mixture Density via Natural Gradient Expectation Maximization
**link**: https://arxiv.org/pdf/2602.10602.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 混合密度网络是一种生成高斯混合以表示连续多模态条件密度的神经网络。标准训练过程采用基于负对数似然（NLL）目标的最大似然估计，存在收敛缓慢和模态崩溃问题。本文通过整合其信息几何改进混合密度网络的优化。具体而言，将混合密度网络解释为深度潜变量模型，并通过期望最大化框架对其进行分析，揭示了与自然梯度下降的惊人理论联系。随后利用这些联系推导出自然梯度期望最大化（nGEM）目标。经验表明，nGEM实现了高达10倍的收敛加速，同时几乎不增加计算开销，并且能很好地扩展到NLL otherwise失败的高维数据。

---

### dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning
**link**: https://arxiv.org/pdf/2602.10603.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 基因组基础模型有潜力解码DNA语法，但在输入表示方面面临基本权衡。标准固定词汇标记器会分割具有生物学意义的基序（如密码子和调控元件），而核苷酸级模型保留生物连贯性但对长上下文产生高昂计算成本。本文介绍dnaHNet，这是一种最先进的无标记器自回归模型，可端到端地分割和建模基因组序列。dnaHNet使用可微动态分块机制，将原始核苷酸自适应压缩为潜变量标记，平衡压缩与预测准确性。在原核基因组上预训练后，dnaHNet在缩放和效率方面优于包括StripedHyena2在内的领先架构。这种递归分块实现了二次FLOP减少，使推理速度比Transformers快3倍以上。在零样本任务中，dnaHNet在预测蛋白质变体适应性和基因必要性方面实现了卓越性能，同时无需监督即可自动发现层次化生物结构。这些结果确立了dnaHNet作为下一代基因组建模的可扩展、可解释框架。

---

### Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving
**link**: https://arxiv.org/pdf/2602.10512.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 本文对基于LLM的交互式定理证明助手（如Lean）中的策略推荐进行理论分析，将其建模为有限 horizon 确定性MDP中的随机策略。为捕捉现代表示学习，我们将状态和动作空间视为一般紧致度量空间并假设Lipschitz策略。为解释最坏情况难度与经验成功之间的差距，我们引入由参考策略q生成的问题分布，包括一个潜变量模型，其中证明展示出可重用的切割/引理/草图结构，由证明DAG表示。在top-k搜索协议和Tsybakov型边际条件下，我们推导出有限 horizon 成功概率的下界，该下界分解为搜索项和学习项，学习项由序列Rademacher/覆盖复杂度控制。主要分离结果表明，当消去切割将深度为D的DAG扩展为大小为Ω(Λ^D)的无切割树，而感知切割的分层过程大小为O(λ^D)（λ≪Λ）时，平坦（无切割）学习器需要的数据集大小在理论上比感知切割的分层学习器呈指数级增长。这为近期智能体定理证明器中的子目标分解提供了原则性证明。

---

### Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models
**link**: https://arxiv.org/pdf/2602.10520.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 循环语言模型（LoopLMs）在生成token之前执行多步潜推理，在较小参数规模下于推理基准上优于传统LLM。然而，使用强化学习进一步提升LoopLM推理能力的尝试均告失败——诸如组相对策略优化（GRPO）等标准目标仅将信用分配给最终潜状态，与模型内部计算存在根本不匹配。为解决此问题，我们引入RLTT（奖励潜思维轨迹），这是一种在整个潜推理轨迹上分配奖励的强化学习框架。RLTT无需依赖外部验证器即可提供密集的轨迹级信用分配，并能以可忽略的开销直接替代GRPO。在Ouro-2.6B-Thinking模型的大量实验中，在相同训练和推理条件下，RLTT在具有挑战性的数学推理基准上显著优于GRPO：MATH-500准确率提升+14.4%，AIME24提升+16.6%，BeyondAIME提升+10.0%。尽管仅在数学领域训练，RLTT也能有效迁移至非数学推理基准，证明了轨迹级信用分配在LoopLMs强化学习中的有效性。

---

### MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs
**link**: https://arxiv.org/pdf/2602.10965.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 知识编辑（KE）能够对大型语言模型（LLMs）中的事实性内容进行精确修改。现有KE方法主要针对密集型架构设计，限制了其在日益流行的稀疏混合专家（MoE）模型中的应用，而MoE模型是现代可扩展LLMs的基础。尽管MoE具有高效的效率和容量扩展能力，但简单适配密集模型编辑器不仅计算成本高昂，还容易导致路由分布偏移，从而破坏稳定性和一致性。为解决这些挑战，我们提出MoEEdit，这是首个用于MoE LLMs中参数修改知识编辑的路由稳定框架。我们的方法通过每个专家的零空间投影重新参数化专家更新，保持路由器输入不变，从而抑制路由偏移。由此产生的块结构优化通过块坐标下降（BCD）求解器高效求解。实验表明，MoEEdit在保持高特异性和路由稳定性的同时，实现了最先进的效能和泛化能力，并具有卓越的计算和内存效率。这些结果为稀疏LLMs中可扩展、精确的知识编辑奠定了坚实基础，并强调了路由稳定干预的重要性。

---

### RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization
**link**: https://arxiv.org/pdf/2602.10819.pdf
**date**: 2026-02-12
**keywords**: cs.LG
**abs**: 在特定领域数据上对齐大型语言模型（LLMs）仍是一项基本挑战。监督微调（SFT）虽能直接注入领域知识，但常降低模型通用性；而在线策略强化学习（RL）虽保留通用性，却难以有效吸收超出模型当前推理水平的困难样本。近期离线策略RL尝试提升困难样本利用率，却因强制向离线策略知识的分布偏移导致严重训练不稳定性。为协调有效的离线策略知识吸收与在线策略RL的稳定性，我们提出重述策略优化（RePO）。该方法中，策略模型先理解离线策略知识，再将其重述为符合自身风格和参数分布的轨迹，并动态用这些高质量重述轨迹替换低奖励展开。此策略引导模型走向正确推理路径，同时严格保留在线策略训练动态。多个基准实验表明，RePO提高了困难样本利用率并优于现有基线，实现了最先进性能。

---

### A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization
**link**: https://arxiv.org/pdf/2602.10680.pdf
**date**: 2026-02-12
**keywords**: stat.ML
**abs**: 许多现实世界数据集包含无法通过输入特征间简单线性相关性检测到的隐藏结构，例如潜在因子可能以协同方式影响数据，但其影响无法通过PCA等基于协方差的方法识别。在实践中，非线性神经网络通常能在无监督和自监督学习中成功提取此类隐藏结构。然而，构建一个可严格分析这种优势的最小高维模型仍是理论挑战。本文引入了一个具有两个潜在因子的可处理高维尖峰模型：一个对协方差可见，另一个统计相关但不相关，仅出现在高阶矩中。PCA和线性自编码器无法恢复后者，而最小非线性自编码器可证明地提取两者。本文分析了总体风险和经验风险最小化，还提供了一个可处理示例，其中自监督测试损失与表示质量不一致：非线性自编码器恢复了线性方法遗漏的潜在结构，尽管其重构损失更高。

---

### TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents
**link**: https://arxiv.org/pdf/2602.10986.pdf
**date**: 2026-02-12
**keywords**: LLM Memory
**abs**: 在LLM代理的强化学习后训练中，外部工具调用耗时较长，导致GPU闲置并增加训练时间和成本。尽管许多工具调用在并行rollout中重复出现，理论上可缓存，但由于工具输出依赖于先前代理交互诱导的环境状态，简单缓存输出并不正确。本文提出TVCACHE，一种用于LLM代理后训练的有状态工具值缓存。TVCACHE维护观察到的工具调用序列树，并执行最长前缀匹配进行缓存查找：只有当代理的完整工具历史与先前执行的序列匹配时才发生缓存命中，从而保证环境状态相同。在终端任务、SQL生成和视频理解三个不同工作负载上，TVCACHE实现了高达70%的缓存命中率，并将工具调用的中位执行时间减少了6.9倍，同时不降低后训练奖励累积。

---

### MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation
**link**: https://arxiv.org/pdf/2602.11062.pdf
**date**: 2026-02-12
**keywords**: latent space
**abs**: 图神经网络（GNN）通过有效建模复杂的用户-物品交互革新了推荐系统，但数据稀疏性和物品冷启动问题严重影响性能，尤其是对于交互历史有限或无交互的新物品。虽然多模态内容提供了有前景的解决方案，但现有方法由于稀疏数据中的噪声和纠缠，导致新物品的表示效果欠佳。为此，本文将多模态推荐转化为离散语义令牌化。提出MoToRec（稀疏正则化多模态令牌化冷启动推荐）框架，其核心是稀疏正则化残差量化变分自编码器（RQ-VAE），可生成离散、可解释令牌的组合语义代码，促进解纠缠表示。MoToRec的架构通过三个协同组件增强：（1）稀疏正则化RQ-VAE以促进解纠缠表示；（2）新的自适应稀有度增强，优先学习冷启动物品；（3）分层多源图编码器，用于与协同信号的鲁棒融合。在三个大规模数据集上的广泛实验表明，MoToRec在整体和冷启动场景下均优于最先进的方法。

---

### Power-SMC: Low-Latency Sequence-Level Power Sampling for Training-Free LLM Reasoning
**link**: https://arxiv.org/pdf/2602.10273.pdf
**date**: 2026-02-12
**keywords**: stat.ML, LLM Reasoning, Latent Reasoning
**abs**: 近期大型语言模型的许多推理性能提升可归因于分布锐化：即偏向生成预训练模型已支持的高似然轨迹，而非修改模型权重。一种自然的形式化方法是序列级幂分布π_α(y|x) ∝ p_θ(y|x)^α（α>1），它将概率质量集中在整个序列上，而非调整 token 级温度。先前工作表明，从该分布进行 Metropolis-Hastings (MH) 采样可恢复强大的推理性能，但推理延迟增加了一个数量级。本文提出 Power-SMC，一种无训练的序贯蒙特卡洛方案，其目标与 MH 采样相同，但延迟接近标准解码。Power-SMC 并行推进一小部分粒子，逐 token 校正重要性权重，并在必要时重采样，所有这些都在单个 GPU 友好的批处理解码中完成。我们证明温度τ=1/α是最小化增量权重方差的唯一前缀仅提议，并通过前缀条件 Rényi 熵解释残余不稳定性，同时引入指数桥接调度以在不改变目标的情况下提高粒子稳定性。在 MATH500 数据集上，Power-SMC 匹配或超过 MH 幂采样的性能，同时将延迟从基线解码的 16-28 倍降低到 1.4-3.3 倍。

---

### Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models
**link**: https://arxiv.org/pdf/2602.10538.pdf
**date**: 2026-02-12
**keywords**: stat.ML, Mathematical Reasoning, Latent Reasoning
**abs**: 智能体定理证明器——将数学推理模型与库检索、子目标分解/搜索规划器以及证明助手验证器相结合的流水线——最近取得了显著的实证成功，但尚不清楚哪些组件驱动性能，以及为何此类系统尽管证明搜索存在经典困难性却仍能工作。我们提出一种分布视角并引入**统计可证性**，定义为在实例分布上平均的、在有限时间内达成验证证明的成功概率，并将现代定理证明流水线形式化为时间有界的马尔可夫决策过程（MDP）。利用贝尔曼结构，我们证明在温和的正则条件下最优策略的存在性，通过子/超解不等式推导可证性证书，并根据近似误差、序贯统计复杂度、表示几何（度量熵/加倍结构）和动作间隙边际尾部来界定分数引导规划（贪婪/top-k/束搜索/滚动）的性能差距。总之，我们的理论为智能体定理证明器在有偏真实世界问题分布上何时以及为何成功提供了原则性的、组件敏感的解释，同时阐明了其在最坏情况或对抗性环境中的局限性。