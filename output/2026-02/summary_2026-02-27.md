### 1 How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?

**link**: https://arxiv.org/pdf/2602.22441.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文全面分析了潜在推理方法在弱监督和强监督下的表现，发现存在普遍捷径行为，即模型无需依赖潜在推理即可达到高准确率。同时，检验了潜在推理支持类BFS探索的假设，发现推理过程未忠实实现结构化搜索，而是表现出隐式剪枝和压缩。研究揭示了监督强度相关的权衡：强监督减轻捷径行为但限制潜在表示维持多样化假设的能力，弱监督允许更丰富的潜在表示但增加捷径行为。

---

### 2 Causality $\\neq$ Invariance: Function and Concept Vectors in LLMs

**link**: https://arxiv.org/pdf/2602.22424.pdf  
**date**: 2026-02-27  
**keywords**: cs.CL  
**abs**: 本文探讨大型语言模型是否能抽象表示概念，重新审视函数向量（FVs）在上下文学习中的作用。实验表明，不同输入格式提取的FVs几乎正交，说明FVs不具备完全不变性。研究提出概念向量（CVs），通过表征相似性分析构建跨格式一致的概念表示。结果显示，FVs在格式匹配时表现优异，而CVs在跨问题类型和语言的分布外场景中泛化能力更强，表明LLMs包含抽象概念表示，但与驱动ICL性能的机制不同。

---

### 3 Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads

**link**: https://arxiv.org/pdf/2602.22453.pdf  
**date**: 2026-02-27  
**keywords**: cs.CL  
**abs**: 本文研究多语言语境下的检索头，发现检索-过渡头（RTH）在多语言LLMs的思维链推理中起关键作用。实验显示，掩盖RTH比掩盖检索头导致更大性能下降，揭示了多语言LLMs中负责映射目标语言的注意力头机制，增进了对潜在推理与目标语言生成关联的理解。

---

### 4 Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference

**link**: https://arxiv.org/pdf/2602.22865.pdf  
**date**: 2026-02-27  
**keywords**: cs.CL  
**abs**: 本文提出ReMix框架，通过连续混合状态解决扩散大型语言模型并行解码中的组合矛盾问题。该框架引入中间状态，允许标记表示在连续空间中迭代优化，并通过拒绝规则将不确定表示恢复为掩码状态重新处理。实验表明，ReMix实现2-8倍推理加速且无质量损失，有效缓解组合矛盾。

---

### 5 Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching

**link**: https://arxiv.org/pdf/2602.22871.pdf  
**date**: 2026-02-27  
**keywords**: cs.CL  
**abs**: 本文提出Stitching Noisy Diffusion Thoughts框架，通过掩码扩散语言模型采样多样化推理轨迹，用过程奖励模型评分中间步骤，并将最高质量步骤缝合为复合理由。该框架在数学推理基准上提高准确率高达23.8%，并减少延迟1.8倍，实现探索与评估的分离。

---

### 6 Imagination Helps Visual Reasoning, But Not Yet in Latent Space

**link**: https://arxiv.org/pdf/2602.22766.pdf  
**date**: 2026-02-27  
**keywords**: cs.CL  
**abs**: 本文通过因果中介分析研究潜在视觉推理的有效性，发现输入-潜在脱节和潜在-答案脱节问题，即潜在标记未能有效关注输入序列且对结果因果效应有限。探测分析显示潜在标记编码视觉信息有限，因此提出CapImagine方法，通过文本显式想象在视觉推理基准上显著优于潜在空间基线。

---

### 7 Probing for Knowledge Attribution in Large Language Models

**link**: https://arxiv.org/pdf/2602.22787.pdf  
**date**: 2026-02-27  
**keywords**: cs.CL  
**abs**: 本文引入AttriWiki框架，通过探针预测LLMs输出背后的主导知识源（用户上下文或内部知识）。在多个模型上，探针实现高达0.96的Macro-F1，并在域外基准上达到0.94-0.99的Macro-F1。研究显示，归因不匹配使错误率增加70%，表明知识源混淆与不忠实答案直接关联。

---

### 8 Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training

**link**: https://arxiv.org/pdf/2602.22576.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory, latent reasoning  
**abs**: 本文提出Search-P1框架，引入路径中心奖励塑造，包括路径中心奖励评估推理轨迹结构质量和双轨路径评分结合自一致性与参考对齐。在多个QA基准上，Search-P1相比基线平均提高7.7点准确率，解决Agentic RAG训练中的奖励稀疏和样本效率问题。

---

### 9 Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA

**link**: https://arxiv.org/pdf/2602.22584.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory, latent reasoning  
**abs**: 本文提出强化共适应框架，联合优化检索与生成：图感知检索建模实体关系结构实现多跳证据选择，证据约束强化学习结合多维度奖励。在广告QA数据集上，专家评估显示各维度性能提升，幻觉率降低72%；线上A/B测试点赞率提升28.6%，URL幻觉减少92.7%。

---

### 10 dLLM: Simple Diffusion Language Modeling

**link**: https://arxiv.org/pdf/2602.22661.pdf  
**date**: 2026-02-27  
**keywords**: latent space  
**abs**: 本文提出dLLM框架，标准化扩散语言模型通用组件，支持新方法和架构的灵活性，旨在解决现有模型核心组件分散和缺乏透明实现的问题。

---

### 11 Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization

**link**: https://arxiv.org/pdf/2602.22675.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory, latent reasoning  
**abs**: 本文提出SMTL框架，通过并行证据获取替代顺序推理，实现高效上下文管理。引入统一数据合成管道构建搜索任务，通过监督微调与强化学习训练端到端智能体。在多个基准上实现SOTA性能，减少70.7%推理步骤同时提高准确率。

---

### 12 Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs

**link**: https://arxiv.org/pdf/2602.22698.pdf  
**date**: 2026-02-27  
**keywords**: latent space  
**abs**: 本文提出KGT框架，通过专用实体token解决LLMs与知识图谱的粒度不匹配问题：引入专用tokenization构建实体级特征表示，关系引导门控机制融合预训练结构和文本特征，独立头实现解耦预测。实验表明KGT在多个基准上持续优于SOTA方法。

---

### 13 RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format

**link**: https://arxiv.org/pdf/2602.22538.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出RAIN-Merging方法，通过无梯度方式整合指令微调模型到大型推理模型中：将任务向量投影到思考特殊标记处前向特征的零空间以保留推理机制，使用指令校准集估计指令注意力并推导缩放因子。在指令跟随和推理基准上，RAIN-Merging显著提高指令遵循性同时保持推理质量。

---

### 14 Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function

**link**: https://arxiv.org/pdf/2602.22255.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出序列建模框架，潜在状态为复值波函数，在学习到的含时哈密顿量作用下演化。利用量子干涉使冲突解释相互抵消，兼容解释相互增强。动力学严格幺正，玻恩规则提取token概率。理论贡献为分离定理，刻画复幺正模型在消歧任务中的表示优势。

---

### 15 A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring

**link**: https://arxiv.org/pdf/2602.23163.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文提出隐写术的决策理论观点，引入广义V-信息框架测量输入中可用信息量，并定义“隐写差距”量化隐写术。通过实证验证，该形式化方法可用于检测、量化和缓解LLM中的隐写推理。

---

### 16 InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models

**link**: https://arxiv.org/pdf/2602.23200.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出InnerQ方案，应用组级量化降低KV缓存解码延迟：沿内维度分组对齐反量化与向量-矩阵乘法，整合混合量化、高精度窗口和键缓存逐通道归一化。评估显示，InnerQ保持与非量化KV缓存相当性能，并超越先前量化方法。

---

### 17 UpSkill: Mutual Information Skill Learning for Structured Response Diversity in LLMs

**link**: https://arxiv.org/pdf/2602.22296.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文介绍UpSkill方法，通过互信息技能学习优化pass@k正确性：提出令牌级互信息奖励鼓励轨迹特异性，在组相对策略优化中实现。实验表明，UpSkill提高多次尝试指标，pass@k平均增益约3%，且不降低pass@1性能。

---

### 18 Structure and Redundancy in Large Language Models: A Spectral Study via Random Matrix Theory

**link**: https://arxiv.org/pdf/2602.22345.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文通过谱几何和随机矩阵理论框架分析模型可靠性：EigenTrack实时检测LLM幻觉和分布外行为，RMT-KD基于异常特征值解释实现网络压缩。结果表明，谱统计提供模型行为紧凑、稳定且可解释视角。

---

### 19 Support Tokens, Stability Margins, and a New Foundation for Robust LLMs

**link**: https://arxiv.org/pdf/2602.22271.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文在概率框架内重新解释因果自注意力Transformer，揭示自注意力参数上障碍约束，在token空间诱导结构化几何形状。研究显示注意力病态条件化边界导致边际解释，并自然产生“支持token”概念。

---

### 20 Manifold of Failure: Behavioral Attraction Basins in Language Models

**link**: https://arxiv.org/pdf/2602.22291.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出绘制LLMs失败流形框架，将漏洞搜索构建为质量多样性问题，使用MAP-Elites阐明行为吸引盆拓扑结构。实验发现370个不同漏洞生态位，揭示模型特定拓扑特征差异，生成可解释全局安全景观地图。

---

### 21 LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training

**link**: https://arxiv.org/pdf/2602.22660.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出LEDA模型，引入维度投影单元将不同域特征对齐到共享语义空间，设计变分语义推理模块获得共享潜在分布。评估显示，LEDA在少样本跨域设置中显著优于域内基线和先进预训练模型。

---

### 22 Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization

**link**: https://arxiv.org/pdf/2602.22536.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出持久非负矩阵分解（pNMF），生成持久性对齐的嵌入序列：利用持久同调识别规范尺度集，形成耦合NMF公式。分析嵌入结构特性，开发序贯交替优化算法。实验证明pNMF在多尺度低秩嵌入方面有效。

---

### 23 Multilingual Safety Alignment Via Sparse Weight Editing

**link**: https://arxiv.org/pdf/2602.22554.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出基于稀疏权重编辑的无训练对齐框架：识别安全神经元集合，将跨语言对齐问题表述为约束线性变换。实验表明，该方法显著降低低资源语言攻击成功率，对一般推理能力影响可忽略。

---

### 24 TRC²: Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns

**link**: https://arxiv.org/pdf/2602.22479.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory  
**abs**: 本文介绍TRC²架构，结合皮质柱稀疏丘脑路由与调制机制，支持快速适应而不破坏较慢参数。评估显示，TRC²改善稳定性-可塑性权衡，实现流式适应同时保留已有行为。

---

### 25 Reinforcement-aware Knowledge Distillation for LLM Reasoning

**link**: https://arxiv.org/pdf/2602.22495.pdf  
**date**: 2026-02-27  
**keywords**: latent reasoning  
**abs**: 本文提出RL感知蒸馏（RLAD），在强化学习期间执行选择性模仿：用PPO/GRPO风格似然比目标替代KL正则化，实现优势感知、信任区域有界的蒸馏。实验表明，RLAD在逻辑推理和数学基准上持续优于离线蒸馏和标准方法。

---

### 26 IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck

**link**: https://arxiv.org/pdf/2602.22581.pdf  
**date**: 2026-02-27  
**keywords**: latent reasoning  
**abs**: 本文提出IBCircuit方法，基于信息瓶颈原理端到端识别信息丰富的电路。在间接对象识别和大于任务中，IBCircuit识别出比相关工作更忠实、更精简的电路。

---

### 27 Transformers converge to invariant algorithmic cores

**link**: https://arxiv.org/pdf/2602.22600.pdf  
**date**: 2026-02-27  
**keywords**: latent space  
**abs**: 本文提取算法核心作为任务性能所需紧凑子空间，发现独立训练Transformer收敛到相同核心。结果揭示跨训练和尺度持续存在的低维不变量，表明Transformer计算围绕紧凑、共享算法结构组织。

---

### 28 Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA

**link**: https://arxiv.org/pdf/2602.22617.pdf  
**date**: 2026-02-27  
**keywords**: latent space  
**abs**: 本文提出语义管预测（STP）任务，将隐藏状态轨迹限制在测地线管状邻域内。实证表明，STP使LLMs用16倍更少训练数据匹配基线准确性，违反Chinchilla式缩放定律数据项。

---

### 29 Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning

**link**: https://arxiv.org/pdf/2602.22642.pdf  
**date**: 2026-02-27  
**keywords**: latent reasoning  
**abs**: 本文提出CEEH方法，通过动态评估实例难度应用选择性熵正则化：为难题保留多样化搜索空间，对简单实例允许激进压缩。在推理基准上，CEEH减少响应长度同时保持准确性。

---

### 30 Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching

**link**: https://arxiv.org/pdf/2602.22812.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory  
**abs**: 本文提出分布式提示缓存技术，通过协作共享中间处理状态提升推理性能。引入基于布隆过滤器的目录抑制不必要通信。实验显示，该方法平均将TTFT和TTLT分别减少93.12%和50.07%。

---

### 31 Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks

**link**: https://arxiv.org/pdf/2602.22817.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory  
**abs**: 本文提出HGPO框架，根据历史上下文一致性将步骤分配到多个层次组，计算不同优势并通过自适应加权聚合。评估显示，HGPO在智能体任务上显著优于现有方法。

---

### 32 SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning

**link**: https://arxiv.org/pdf/2602.22603.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文提出SideQuest方法，利用大型推理模型自身评估token有用性执行KV缓存压缩。评估显示，SideQuest在智能体任务上减少65%峰值token使用量，准确性下降极小。

---

### 33 AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications

**link**: https://arxiv.org/pdf/2602.22769.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文引入AMA-Bench评估LLMs在智能体应用中的长时记忆，包含真实世界和合成智能体轨迹。研究显示，现有记忆系统表现不佳，提出AMA-Agent实现57.22%平均准确率，超过基线11.16%。

---

### 34 ParamMem: Augmenting Language Agents with Parametric Reflective Memory

**link**: https://arxiv.org/pdf/2602.23320.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文引入ParamMem参数化记忆模块，将跨样本反思模式编码到模型参数中。基于此提出ParamAgent框架，整合参数化记忆与情景记忆。实验表明，ParamMem具有样本高效性，支持跨模型规模弱到强迁移。

---

### 35 SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport

**link**: https://arxiv.org/pdf/2602.23353.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出SOTAlign框架，利用线性教师恢复共享几何结构，通过基于最优传输散度在未配对样本上优化对齐。实验显示，SOTAlign学习鲁棒联合嵌入，显著优于有监督和半监督基线。

---

### 36 DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding

**link**: https://arxiv.org/pdf/2602.23135.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出DyGnROLE架构，使用单独嵌入词汇表和角色语义位置编码捕获源节点和目标节点特有结构和时间上下文。通过自监督预训练目标编码结构偏差，评估显示DyGnROLE在未来边分类任务上显著优于基线。

---

### 37 Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation

**link**: https://arxiv.org/pdf/2602.23188.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出参数化降阶模型高效再训练策略，将激活分解为主subspace和随机subspace，通过集合卡尔曼滤波同化数据。结果表明，再训练可限于自编码器，实现轻量级实时适应。

---

### 38 Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language

**link**: https://arxiv.org/pdf/2602.23201.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出广义神经记忆系统，基于自然语言指令执行灵活更新，使自适应智能体从异质信息源中选择性学习。

---

### 39 Towards Autonomous Memory Agents

**link**: https://arxiv.org/pdf/2602.22406.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文提出U-Mem实现自主记忆智能体：成本感知知识提取级联从廉价信号升级到工具验证，语义感知汤普森采样平衡探索与利用。评估显示，U-Mem在基准上持续优于基线。

---

### 40 Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space

**link**: https://arxiv.org/pdf/2602.22879.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文提出基于LLM的知识追踪方法，通过在双曲空间中实现LLM与学生层次化行为对齐，以增强模型性能。

---

### 41 CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines

**link**: https://arxiv.org/pdf/2602.22452.pdf  
**date**: 2026-02-27  
**keywords**: latent space  
**abs**: 本文提出对比世界模型（CWM），通过InfoNCE对比目标和难负例微调LLM作为动作评分器，将有效动作与无效动作在潜在空间中分离。实验表明，CWM优于监督微调，证明对比训练能更忠实地捕获物理可行性表示。

---

### 42 Mirroring the Mind: Distilling Human-Like Metacognitive Strategies into Large Language Models

**link**: https://arxiv.org/pdf/2602.22508.pdf  
**date**: 2026-02-27  
**keywords**: latent reasoning  
**abs**: 本文提出元认知行为调优（MBT）框架，显式注入元认知行为到模型思维过程。实验表明，MBT在多跳QA基准上消除推理崩溃，提高准确率并减少token消耗。

---

### 43 Agentic AI for Intent-driven Optimization in Cell-free O-RAN

**link**: https://arxiv.org/pdf/2602.22539.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory  
**abs**: 本文提出用于无小区O-RAN中意图翻译与优化的智能体AI框架。监督代理转化意图，用户加权代理通过内存模块确定用户优先级权重，O-RU管理代理激活O-RU集合。采用参数高效微调减少内存使用92%，节能模式下活跃O-RU减少41.93%。

---

### 44 ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks

**link**: https://arxiv.org/pdf/2602.23285.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文提出ODEBRAIN框架，将时空频率特征整合到谱图节点中，使用神经ODE对连续潜在动态建模。实验验证，ODEBRAIN在预测EEG动态方面显著优于现有方法。

---

### 45 Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions

**link**: https://arxiv.org/pdf/2602.22680.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本综述从能力角度回顾个性化LLM驱动智能体，围绕用户画像建模、记忆、规划和动作执行组织文献。分析用户信号表示、传播和利用，探讨评估指标和应用场景，为设计和理解个性化智能体提供框架。

---

### 46 Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning

**link**: https://arxiv.org/pdf/2602.22751.pdf  
**date**: 2026-02-27  
**keywords**: cs.AI  
**abs**: 本文提出EGPO框架，将内在不确定性整合到强化学习中：利用token级似然熵代理估计不确定性，通过非对称校准机制对齐外在正确性。实验表明，EGPO显著提升推理性能。

---

### 47 PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering

**link**: https://arxiv.org/pdf/2602.23161.pdf  
**date**: 2026-02-27  
**keywords**: latent reasoning  
**abs**: 本文提出PATRA模型，引入模式感知机制从时间序列中提取趋势和季节性模式实现深度对齐，设计任务感知平衡奖励协调不同难度任务学习。实验显示，PATRA在时间序列问答任务上优于基线。

---

### 48 ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering

**link**: https://arxiv.org/pdf/2602.23193.pdf  
**date**: 2026-02-27  
**keywords**: LLM Memory  
**abs**: 本文提出ESAA架构，受事件溯源模式启发，将智能体认知意图与项目状态突变分离。智能体仅以结构化JSON格式发出意图，解决长周期上下文退化和概率生成与确定性执行差距问题。

---

### 49 Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization

**link**: https://arxiv.org/pdf/2602.23008.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出EMPO²框架，利用记忆进行探索，结合在线和离线策略更新。在ScienceWorld和WebShop上，EMPO²比GRPO提高128.6%和11.3%，并在分布外测试中表现优异适应性。

---

### 50 PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training

**link**: https://arxiv.org/pdf/2602.23111.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出PRAC方法，将激活分解为主subspace和随机subspace，通过精确缩放因子产生无偏梯度估计器。实验显示，PRAC实现高达36%总内存减少，性能下降可忽略。

---

### 51 Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements

**link**: https://arxiv.org/pdf/2602.23035.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出物理知情潜在关系框架，将心脏涡流建模为交互节点。模型结合神经关系推理与物理启发交互能量，生成对疾病敏感的潜在图。实验表明，潜在图熵与疾病严重程度相关，可作为可解释标志物。

---

### 52 Latent Matters: Learning Deep State-Space Models

**link**: https://arxiv.org/pdf/2602.23050.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出约束优化框架训练深度状态空间模型，引入扩展卡尔曼VAE（EKVAE）结合摊销变分推理与贝叶斯滤波。结果证明，约束优化提高系统辨识和预测精度，EKVAE优于先前模型。

---

### 53 RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection

**link**: https://arxiv.org/pdf/2602.23060.pdf  
**date**: 2026-02-27  
**keywords**: cs.LG  
**abs**: 本文提出RhythmBERT模型，通过基于自编码器的潜在表示将ECG段编码为符号令牌，将ECG视为语言范式。预训练后，RhythmBERT以标签高效方式学习上下文表示，性能与12导联基线相当或更优。