### 1 Evaluating Memory Structure in LLM Agents

**link**: https://arxiv.org/pdf/2602.11243.pdf  
**date**: 2026-02-13  
**keywords**: LLM Memory, cs.LG  
**abs**: 现代基于LLM的智能体和聊天助手依赖长期记忆框架来存储可重用知识、回忆用户偏好并增强推理能力。随着研究人员创建更复杂的记忆架构，分析其能力和指导未来记忆设计变得越来越困难。大多数长期记忆基准测试侧重于简单的事实保留、多跳回忆和基于时间的变化。虽然这些能力无疑很重要，但通常可以通过简单的检索增强型LLM实现，且无法测试复杂的记忆层次结构。为弥合这一差距，本文提出StructMemEval基准，测试智能体组织长期记忆的能力，而非仅测试事实回忆。研究收集了一系列人类通过特定结构（如交易账簿、待办事项列表、树结构等）组织知识来解决的任务。初步实验表明，简单的检索增强型LLM难以完成这些任务，而记忆智能体如果被提示如何组织记忆则能可靠解决。然而，研究还发现，现代LLM在未被提示时并不总是能识别记忆结构，这为LLM训练和记忆框架的未来改进指明了重要方向。  

---

### 2 Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal Co-occurrence

**link**: https://arxiv.org/pdf/2602.11322.pdf  
**date**: 2026-02-13  
**keywords**: LLM Memory, latent space, cs.LG  
**abs**: 当前神经系统中的记忆方法依赖于基于相似性的检索：给定查询，找到表示最相似的存储状态。这种“有用记忆即相似记忆”的假设未能捕捉生物记忆的基本特性——时间共现关联。本文提出预测关联记忆（PAM）架构，其中基于JEPA风格的预测器通过在连续体验流上训练时间共现，学习导航嵌入空间的关联结构。研究引入内向JEPA（Inward JEPA）处理存储的体验（预测可关联到达的过去状态），作为处理传入感官数据的标准外向JEPA（预测未来状态）的补充。评估PAM作为关联回忆系统（测试体验关联的回忆忠实度），而非评估对未见过关联的泛化能力。在合成基准上，预测器的顶级检索在97%的情况下是真实时间关联物（关联精确率@1=0.970）；在余弦相似度为零的情况下实现跨边界召回率@20=0.421；将共同体验状态与从未共同体验状态区分的AUC为0.916（余弦相似度：0.789）。即使限于嵌入相似度无信息的跨房间对，预测器仍实现AUC=0.849（余弦相似度：0.503，随机水平）。时间打乱控制证实信号是真实的时间共现结构而非嵌入几何：打乱使跨边界召回率下降90%，在训练种子间可重复。所有结果在种子（标准差<0.006）和查询选择（标准差≤0.012）上均稳定。  

---

### 3 Retrieval-Aware Distillation for Transformer-SSM Hybrids

**link**: https://arxiv.org/pdf/2602.11374.pdf  
**date**: 2026-02-13  
**keywords**: LLM Memory, cs.LG  
**abs**: 状态空间模型（SSMs）提供高效的序列建模，但在需要上下文检索的基准上落后于Transformer。先前研究将此差距归因于一小部分称为“聚集-聚合”（G&A）的注意力头，而SSMs难以重现这些头。本文提出“检索感知蒸馏”，通过仅保留这些检索关键头并将其余部分蒸馏为循环头，将预训练Transformer转换为混合学生模型。通过在合成检索任务上的消融实验识别关键头，生成具有稀疏、非均匀注意力放置的混合模型。研究表明，**仅保留2%的注意力头即可恢复教师在检索密集型任务上95%以上的性能**（1B模型中保留10个头），所需头数远少于至少保留25%注意力头的混合模型。进一步发现，大的循环状态通常可以补偿缺失的检索能力：一旦检索由这些头处理，SSM主干可以简化且损失有限，即使状态维度减少8倍。通过减少注意力缓存和SSM状态，所得混合模型比可比混合模型内存效率高5-6倍，以极低的内存成本缩小了Transformer与SSM之间的差距。  

---

### 4 RooflineBench: A Benchmarking Framework for On-Device LLMs via Roofline Analysis

**link**: https://arxiv.org/pdf/2602.11506.pdf  
**date**: 2026-02-13  
**keywords**: latent space  
**abs**: 随着小型语言模型（SLMs）向本地化智能的转变，对资源受限边缘硬件上的严格性能表征需求日益增加。然而，客观衡量不同架构在异构平台上的理论性能上限仍是一项艰巨挑战。本文提出了一个基于Roofline模型的系统框架，通过运算强度（OI）统一架构原语和硬件约束。通过定义推理潜力区域，引入相对推理潜力作为新指标，用于比较同一硬件基板上大型语言模型（LLMs）的效率差异。跨不同计算层级的广泛实证分析表明，性能和OI的变化受序列长度影响显著。研究还发现，随着模型深度增加，OI会出现明显下降。此外，研究结果强调了硬件异构性导致的效率陷阱，并证明了如多头潜在注意力（MLA）等结构改进可以有效释放各种硬件基板上的潜在推理潜力。这些见解为硬件-软件协同设计提供了可操作方向，以实现神经结构与设备端智能中物理约束的对齐。发布的代码见附录C。  

---

### 5 PASCAL: A Phase-Aware Scheduling Algorithm for Serving Reasoning-based Large Language Models

**link**: https://arxiv.org/pdf/2602.11530.pdf  
**date**: 2026-02-13  
**keywords**: latent reasoning  
**abs**: 基于推理的大型语言模型（LLM）利用思维链（CoT）推理的出现带来了新的服务挑战，其延长的推理阶段会延迟用户可见输出并增加首 token 时间（TTFT）。现有的 LLM 服务框架无法区分推理阶段和回答阶段，导致在 GPU 内存约束下性能下降。本文提出 PASCAL，一种相位感知调度算法，通过优先处理推理阶段来减少 TTFT，同时在回答阶段使用受控抢占和 token pacing 以保持用户体验质量（QoE）。该分层调度器结合了实例级放置和实例内执行，并允许在相位边界动态迁移以平衡负载和减少干扰。在使用 DeepSeek-R1-Distill-Qwen-32B 的基准测试中，PASCAL 将尾部 TTFT 减少了高达 72%，同时保持了回答阶段的服务等级目标（SLO）达成率，证明了相位感知调度在基于推理的 LLM 部署中的重要性。  

---

### 6 Towards Compressive and Scalable Recurrent Memory

**link**: https://arxiv.org/pdf/2602.11212.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: Transformer在扩展到长上下文时面临注意力的二次瓶颈。最近的方法引入循环记忆以将上下文扩展到当前窗口之外，但这些方法往往面临理论原则与实际可扩展性之间的根本权衡。为解决此问题，我们提出了Elastic Memory，这是一种基于HiPPO框架的新型记忆架构，用于在线函数逼近。Elastic Memory将历史序列视为连续信号的样本，应用最优在线压缩将其编码为固定大小的记忆状态。在检索方面，我们提出了一种灵活的多项式采样机制，从该压缩状态中重建历史摘要。Elastic Memory在三个领域的长上下文（32k+）数据集上持续优于基线模型。在参数相等的情况下，它的记忆容量比Memorizing Transformer高出16倍，并且在所有记忆大小下都优于Melodi，即使Melodi拥有30%更多的参数。当扩展模型大小时，Elastic Memory仍然领先于所有基线，并且在4倍大小下比Melodi显著更快。此外，其解耦设计允许在测试时注入归纳偏置以提升性能。  

---

### 7 Learning Glioblastoma Tumor Heterogeneity Using Brain Inspired Topological Neural Networks

**link**: https://arxiv.org/pdf/2602.11234.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 使用深度学习（DL）对胶质母细胞瘤（GBM）进行准确预后受到极端空间和结构异质性的阻碍。此外，不同机构间不一致的MRI采集协议影响了模型的泛化能力。传统的Transformer和DL管道往往无法捕捉多尺度形态多样性，如碎片化的坏死核心、浸润边缘和不连续的强化成分，导致扫描仪特异性伪影和较差的跨中心预后。我们提出了TopoGBM，这是一种学习框架，旨在从多参数3D MRI中捕捉保留异质性、对扫描仪稳健的表示。我们方法的核心是一个3D卷积自编码器，通过拓扑正则化进行正则化，在压缩的latent space中保留肿瘤流形的复杂非欧几里得不变量。通过强制执行这些拓扑先验，TopoGBM明确建模了侵袭性GBM特有的高方差结构特征。在异质性队列（UPENN、UCSF、RHUH）中进行评估，并在TCGA上进行外部验证，TopoGBM取得了更好的性能（测试集C指数0.67，验证集0.58），优于在域转移下性能下降的基线模型。机制可解释性分析表明，重建残差高度定位于病理异质性区域，肿瘤限制区域和健康组织的误差显著较低（测试集：0.03，验证集：0.09）。此外，基于遮挡的归因将约50%的预后信号定位于肿瘤和多样化的瘤周微环境，证明了这种无监督学习方法的临床可靠性。我们的研究结果表明，纳入拓扑先验能够学习捕捉肿瘤异质性的形态忠实嵌入，同时保持跨机构的稳健性。  

---

### 8 Sparse Semantic Dimension as a Generalization Certificate for LLMs

**link**: https://arxiv.org/pdf/2602.11388.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 标准统计学习理论预测大型语言模型（LLMs）由于参数数量远超训练token数应会过拟合，但实际中它们泛化能力强。本文提出有效容量控制泛化能力在于模型内部表示的几何结构：尽管参数空间高维，激活状态却位于低维稀疏流形上。引入稀疏语义维度（SSD），这是一种从训练于模型层的稀疏自编码器（SAE）的活跃特征词汇表导出的复杂度度量。将LLM和SAE视为冻结的oracle，该框架将模型的泛化能力归因于字典的稀疏性而非总参数数。在GPT-2 Small和Gemma-2B上验证，表明该边界在实际样本量下提供非空证书。发现反直觉的“特征锐度”缩放定律：尽管Gemma-2B大一个数量级，但识别其活跃流形所需的校准样本显著少于GPT-2，表明更大模型学习更可压缩、更独特的语义结构。最后，该框架可作为可靠安全监控器：分布外输入触发“特征爆炸”（活跃特征激增），通过学习到的特征违反有效信号认知不确定性。  

---

### 9 CADET: Context-Conditioned Ads CTR Prediction With a Decoder-Only Transformer

**link**: https://arxiv.org/pdf/2602.11410.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 点击率（CTR）预测是在线广告系统的基础。尽管具有显式特征交互的深度学习推荐模型（DLRMs）长期主导该领域，但生成式推荐器的最新进展在内容推荐中显示出良好结果。然而，将这些基于Transformer的架构适应广告CTR预测仍面临独特挑战，包括处理评分后上下文信号、维持离线-在线一致性以及扩展到工业工作负载。本文提出CADET（上下文条件广告解码器仅Transformer），一种端到端解码器仅Transformer，用于LinkedIn部署的广告CTR预测。该方法引入多项关键创新：（1）上下文条件解码架构，带有多塔预测头，显式建模广告位置等评分后信号，解决预测CTR与排序之间的鸡生蛋问题；（2）自门控注意力机制，通过自适应调节表示和交互级别的信息流来稳定训练；（3）基于时间戳的Rotary Position Embedding（RoPE）变体，捕获从秒到月的时间尺度上的时间关系；（4）会话掩码策略，防止模型学习不可用会话内事件的依赖关系，解决训练-服务偏差；（5）生产工程技术，包括张量打包、序列分块和自定义Flash Attention内核，实现大规模高效训练和服务。在线A/B测试中，CADET相比生产LiRank基线模型（DCNv2和序列编码器的混合集成）实现11.04%的CTR提升。该系统已成功部署在LinkedIn的广告平台，服务主页赞助更新的主要流量。  

---

### 10 Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification

**link**: https://arxiv.org/pdf/2602.11448.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 可解释设计模型在计算机视觉中越来越受关注，因为它们为预测提供忠实解释。在图像分类中，这些模型通常从图像中恢复人类可解释的概念并用于分类。稀疏概念恢复方法利用视觉语言模型的潜在空间，将图像嵌入表示为概念嵌入的稀疏组合。然而，由于此类方法忽略概念的层次结构，可能产生预测正确但解释与层次结构不一致的情况。本文提出层次概念嵌入与追踪（HCEP）框架，在潜在空间中诱导概念嵌入的层次结构，并使用层次稀疏编码恢复图像中存在的概念。给定语义概念的层次结构，构建相应的概念嵌入层次结构，并假设图像的正确概念形成层次结构中的根路径，推导在嵌入空间中识别它们的理想条件。表明层次稀疏编码可靠地恢复层次概念嵌入，而普通稀疏编码失败。在真实世界数据集上的实验表明，HCEP在概念精度和召回率上优于基线，同时保持有竞争力的分类准确率。此外，当样本数量有限时，HCEP实现更优的分类准确率和概念恢复。这些结果表明，将层次结构纳入稀疏编码可产生更可靠和可解释的图像分类模型。  

---

### 11 KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models

**link**: https://arxiv.org/pdf/2602.11184.pdf  
**date**: 2026-02-13  
**keywords**: LLM Memory  
**abs**: 混合专家（MoE）模型通过稀疏专家激活在提升性能的同时保持了计算效率，但其庞大的参数规模和内存需求对资源受限环境中的部署构成了重大挑战。向量量化（VQ）通过利用码本将权重向量映射到最相似的离散码字，为大型语言模型（LLMs）的超低比特压缩提供了有前景的方法。然而，将VQ直接应用于MoE通常会导致性能显著下降，主要由于两个关键障碍：（1）专家间的冗余表示导致VQ对每个专家重复量化相似表示，造成有限码本容量的低效利用；（2）MoE层中的专家聚合会放大累积输出偏差，导致量化输出的分布偏移。为解决这些问题，本文提出KBVQ-MoE，一种新颖的VQ框架，用于增强基于MoE的LLM的极低比特量化。KBVQ-MoE集成了两种技术：（1）输入驱动的冗余消除，通过Karhunen-Loeve变换（KLT）引导的奇异值分解（SVD）提取主导权重分量并在专家间共享；（2）偏差校正的输出稳定，仅对专家特定（非冗余）表示应用向量量化，并通过通道-wise仿射补偿校正量化输出。在各种MoE LLM上的实验表明，KBVQ-MoE比现有量化方法更好地保持了准确性。例如，对Qwen1.5-MoE-A2.7B进行3比特量化时，平均准确率达到67.99，几乎与FP16基线的68.07相同，突显了KBVQ-MoE在边缘设备和其他资源受限平台上高效部署的潜力。  

---

### 12 MELINOE: Fine-Tuning Enables Memory-Efficient Inference for Mixture-of-Experts Models

**link**: https://arxiv.org/pdf/2602.11192.pdf  
**date**: 2026-02-13  
**keywords**: LLM Memory  
**abs**: 混合专家（MoE）模型架构通过显著减少每个token的激活参数数量，实现了高效的训练和推理。然而，其庞大的总参数数量和模型大小阻碍了它们在资源受限环境中的广泛使用，因为所有参数仍必须加载到GPU内存中。现有工作旨在通过将某些专家卸载到CPU内存中，并仅在激活时将它们移植到GPU内存中来解决这一内存瓶颈。但在实践中，这些方法受到专家传输带来的显著I/O延迟的影响。本文提出MELINOE方法，该方法微调MoE模型以更强烈地倾向于每个序列激活更少数量的专家。将这些偏好专家缓存在GPU内存中可减少专家 churn 和CPU-GPU传输开销。MELINOE相比高效基线将吞吐量提高1.2-3倍，相比传输密集型基线最高提高14.7倍，同时保持甚至提升模型在下游任务上的性能，使其成为提高MoE推理效率的可靠方法。  

---

### 13 Native Reasoning Models: Training Language Models to Reason on Unverifiable Data

**link**: https://arxiv.org/pdf/2602.11549.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG, latent reasoning  
**abs**: 现有大型推理模型的训练范式（SFT+RLVR）依赖高质量人工标注推理数据和外部验证器，存在数据成本高、嵌入人类偏见及局限于可验证领域等问题。本文提出NRT（原生推理训练）框架，仅使用标准问答对让模型生成自身推理轨迹，无需专家演示。NRT将推理过程视为潜变量，采用统一训练目标，通过优化提高生成真实答案的可能性，形成自我强化反馈循环。在Llama和Mistral模型家族上的实验表明，NRT在无验证器方法中实现最先进性能，显著优于标准SFT和现有无验证器RL方法，尤其在复杂推理领域表现突出且对策略崩溃具有高鲁棒性。  

---

### 14 UMAP Is Spectral Clustering on the Fuzzy Nearest-Neighbor Graph

**link**: https://arxiv.org/pdf/2602.11662.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: UMAP（均匀流形近似与投影）是最广泛使用的非线性降维和数据可视化算法之一。尽管其受欢迎，且通过代数拓扑的视角呈现，但其与经典谱方法的确切关系仍不明确。本文证明UMAP在模糊k近邻图上执行谱聚类。证明分为三步：（1）表明UMAP带负采样的随机优化是相似图上的对比学习目标；（2）引用HaoChen等人的结果，证实相似图上的对比学习等价于谱聚类；（3）验证UMAP的谱初始化计算该谱问题的精确线性解。对于高斯核，等价性精确成立；对于UMAP默认的柯西型核，等价性作为一阶近似成立。该结果将UMAP、对比学习和谱聚类统一在单一框架下，并为UMAP行为的若干经验观察提供了理论基础。  

---

### 15 Latent-Variable Learning of SPDEs via Wiener Chaos

**link**: https://arxiv.org/pdf/2602.11794.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 本文研究了从时空观测中学习带加性高斯强迫的线性随机偏微分方程（SPDEs）规律的问题。大多数现有深度学习方法要么假设可以获取驱动噪声或初始条件，要么依赖确定性代理模型，而这些模型无法捕捉内在的随机性。我们提出了一种结构化潜变量公式，该公式仅需解的实现观测即可学习潜在的随机强迫动力学。我们的方法将谱伽辽金投影与截断维纳混沌展开相结合，实现了确定性演化和随机强迫之间的原则性分离。这将无限维SPDE简化为一个有限的参数化常微分方程组，用于控制潜在的时间动力学。通过变分学习联合推断潜在动力学和随机强迫，能够在训练过程中无需显式观测或模拟噪声即可恢复随机结构。在合成数据上的实证评估表明，在有界和无界一维空间域的可比建模假设下，该方法表现出最先进的性能。  

---

### 16 Deep Kernel Fusion for Transformers

**link**: https://arxiv.org/pdf/2602.11808.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 长上下文的智能体大语言模型（LLM）推理越来越受到内存带宽而非计算能力的限制。在这种情况下，SwiGLU MLP块的大权重超出缓存容量，成为一个主要但未被充分优化的瓶颈。我们提出了DeepFusionKernel，一种深度融合的内核，它减少了HBM流量并提高了缓存重用率，在H100上比SGLang提供高达13.2%的加速，在A100上提供9.7%的加速。DeepFusionKernel与SGLang集成并搭配内核调度器，确保在生成长度上的一致加速，同时保持对不同模型、推理配置和硬件平台的适应性。  

---

### 17 SpiralFormer: Looped Transformers Can Learn Hierarchical Dependencies via Multi-Resolution Recursion

**link**: https://arxiv.org/pdf/2602.11698.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG, latent reasoning, latent space  
**abs**: 递归（循环）Transformer通过重复应用共享层将计算深度与参数深度解耦，为迭代优化和潜在推理（latent reasoning）提供了明确的架构原语。然而，早期的循环Transformer往往不如同等计算量的非递归基线模型表现好。尽管最近的文献引入了更有效的递归机制来缩小这一差距，但现有架构仍在固定的全令牌分辨率下运行，忽略了基于压缩潜在表示（latent representations）进行计算的潜在效率。本文提出SpiralFormer，一种在多分辨率递归调度下执行循环的循环Transformer。我们提供的探测证据表明，多分辨率递归通过在不同尺度上诱导迭代级功能特化，使模型能够学习层次依赖关系。实证研究表明，在160M到1.4B的模型规模上，SpiralFormer比循环和非循环基线模型都具有更好的参数和计算效率，确立了序列分辨率作为递归架构扩展的一个潜在维度。  

---

### 18 A²V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production

**link**: https://arxiv.org/pdf/2602.11861.pdf  
**date**: 2026-02-13  
**keywords**: latent space  
**abs**: 本文基于手语生成的结构解耦框架，提出A²V-SLP——一种对齐感知变分框架，该框架学习关节级解耦的潜在分布而非确定性嵌入。通过解耦变分自编码器（VAE）对真实手语姿态序列进行编码，提取关节特定的均值和方差向量，作为非自回归Transformer训练的分布监督。Transformer根据文本嵌入预测潜在均值和对数方差，VAE解码器通过解码阶段的随机采样重建最终手语姿态序列。此方法通过分布性潜在建模避免确定性潜在坍缩，同时整合gloss注意力机制加强语言输入与关节运动的对齐。实验结果表明，该方法相比确定性潜在回归持续提升性能，在完全无gloss设置下实现了最先进的反向翻译性能和改进的运动真实性。  

---

### 19 In-Context Function Learning in Large Language Models

**link**: https://arxiv.org/pdf/2602.11863.pdf  
**date**: 2026-02-13  
**keywords**: LLM, latent reasoning  
**abs**: 本文通过高斯过程（GPs）研究大语言模型（LLMs）的上下文学习现象。在受控实验中，模型观察从已知GP先验中抽取的多变量标量函数样本序列，评估预测误差与演示次数的关系，并与经验GP回归学习器（下界）和1-最近邻规则（上界）进行比较。结果发现，不同模型大小的LLM学习曲线受函数生成核的强烈影响，随着演示次数增加接近GP下界。通过似然分析发现LLM预测在较不光滑的GP核下可能性最高，并证明后训练（强化学习和监督微调）可有效调整归纳偏差以提高从光滑核GP采样函数的样本效率。  

---

### 20 Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration

**link**: https://arxiv.org/pdf/2602.11937.pdf  
**date**: 2026-02-13  
**keywords**: LLM, latent reasoning  
**abs**: 推理聚焦型大语言模型通过生成更长推理轨迹提高答案质量，但额外 tokens 显著增加服务成本，因此需要推理优化。本文扩展并应用 Puzzle 后训练神经架构搜索框架到 gpt-oss-120B，生成部署优化的 gpt-oss-puzzle-88B。该方法结合异构 MoE 专家剪枝、全上下文注意力与窗口注意力的选择性替换、带校准尺度的 FP8 KV 缓存量化以及后训练强化学习来恢复精度，同时保持低生成长度。在 8XH100 节点上，长上下文和短上下文设置下的每 token 吞吐量分别提升 1.63 倍和 1.22 倍，单 NVIDIA H100 GPU 上达 2.82 倍。提出请求级效率指标，证明 gpt-oss-puzzle-88B 在整个精度-速度前沿上优于原始模型，在各种基准测试中匹配或略超父模型的平均精度。  

---

### 21 RAM-Net: Expressive Linear Attention with Selectively Addressable Memory

**link**: https://arxiv.org/pdf/2602.11958.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 线性注意力架构虽推理高效，但将无限历史压缩到固定大小内存中会限制表达能力并导致信息丢失。本文提出RAM-Net，通过将输入映射到高维稀疏向量作为显式地址，使模型能选择性访问大规模内存状态。该设计实现指数级状态大小扩展而不增加参数，减少信号干扰并提高检索保真度，同时保持计算效率。实验表明，RAM-Net在细粒度长程检索任务上超越现有基线，并在语言建模和零样本常识推理中表现竞争力。  

---

### 22 Manifold-Aware Temporal Domain Generalization for Large Language Models

**link**: https://arxiv.org/pdf/2602.11965.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 大语言模型在实际部署中常面临时间分布偏移问题。本文提出流形感知时间LoRA（MaT-LoRA），将时间更新约束在低秩适应子空间内的共享低维流形上，通过结构化时间核心建模其演化。该方法在参数高效微调下保留模型演化的低维时间结构，降低时间建模复杂度同时保持表达能力。实验表明，MaT-LoRA在科学文献、新闻和评论评分等数据集上实现优异的时间泛化性能。  

---

### 23 Protein Circuit Tracing via Cross-layer Transcoders

**link**: https://arxiv.org/pdf/2602.12026.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 蛋白质语言模型（pLMs）能预测蛋白质结构和功能，但其预测背后的计算电路仍不明确。现有解释方法独立处理各层，无法捕捉跨层计算。本文提出ProtoMech框架，使用跨层转码器学习跨层联合稀疏latent表示，以捕获模型完整计算电路。应用于ESM2模型，ProtoMech恢复82-89%的原始性能，并识别出使用<1% latent空间却保留79%准确率的压缩电路，与结构和功能基序相关，且能指导高适应性蛋白质设计。  

---

### 24 PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving

**link**: https://arxiv.org/pdf/2602.12029.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 多智能体系统常协调多个专用语言模型解决复杂问题，重复处理相同提示前缀，导致各模型冗余执行预填充阶段并维护各自KV缓存，增加负载和延迟。本文提出PrefillShare，通过分解模型为预填充和解码模块，冻结预填充模块并仅微调解码模块，使多个任务特定模型共享预填充模块和KV缓存。该方法在多模型代理工作负载中实现4.5倍p95延迟降低和3.9倍吞吐量提升，同时匹配全微调精度。  

---

### 25 Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction

**link**: https://arxiv.org/pdf/2602.12204.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 混合架构结合状态空间模型和注意力机制在效率与质量权衡上表现出色，但现有方法要么均匀应用注意力，要么学习静态稀疏模式。本文通过分析GPT-2模型发现88%的注意力操作检索的信息已可从模型隐藏状态预测，且这种冗余在训练中不会减少。为此，提出了一种受生物学启发的记忆巩固机制CRAM，该机制逐渐将情景检索提炼为参数化语义记忆。与先前的稀疏注意力方法不同，CRAM在训练过程中表现出注意力利用率的降低，在约3K步时通过急剧相变实现37.8×的减少。在SRCD基准上，CRAM以1.6%的注意力计算实现100%的检索准确率，并且巩固的模式无需重新训练即可转移到未见任务，实现48-52%的注意力减少。学习到的巩固动态在数量上匹配认知心理学中的人类情景到语义记忆转换曲线。  

---

### 26 The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics

**link**: https://arxiv.org/pdf/2602.12218.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 确定神经模型是否将物理定律内化为世界模型而非利用统计捷径仍然具有挑战性，尤其是在分布外（OOD）偏移下。标准评估通常通过下游适应（如微调或高容量探针）测试潜在能力，但此类干预可能改变被测量的表示，从而混淆自监督学习（SSL）期间所学内容。本文提出非侵入性评估协议PhyIP，通过测试物理量是否可从冻结表示线性解码来验证。在流体动力学和轨道力学任务中，当SSL实现低误差时，潜在结构变得可线性访问，PhyIP在OOD测试中恢复内能和牛顿平方反比定律（ρ>0.90）。相比之下，基于适应的评估会破坏这种结构（ρ≈0.05）。研究结果表明，基于适应的评估可能掩盖潜在结构，而低容量探针提供更准确的物理世界模型评估。  

---

### 27 Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling

**link**: https://arxiv.org/pdf/2602.12045.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 发现新的晶体材料需要能够处理周期边界条件、晶体对称性和物理约束的生成模型，同时能扩展到大型和结构多样的晶胞。本文提出一种倒易空间生成流水线，通过物种分辨晶胞密度的截断傅里叶变换来表示晶体，而非直接建模原子坐标。这种表示具有原生周期性，允许空间群对称性的简单代数操作，并在生成过程中自然支持可变原子多重性，解决了基于粒子方法的常见限制。使用每个空间维度仅九个傅里叶基函数，该方法能重建每个化学物种包含多达108个原子的晶胞。本文实例化了该流水线，包括基于复值傅里叶系数的 transformer 变分自编码器，以及在压缩 latent space（ latent space）中生成的潜扩散模型。在 LeMaterial 基准上评估了重建和潜扩散性能，并在小晶胞区域（每个晶胞≤16个原子）中将无条件生成与基于坐标的基线进行了比较。  

---

### 28 Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL

**link**: https://arxiv.org/pdf/2602.12087.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 在强化学习（RL）中，从高维、多模态和含噪声的观测中估计环境状态是一项基本挑战。传统方法依赖概率模型来解释不确定性，但通常需要明确的噪声假设，进而限制了泛化能力。本文提出一种新方法，用于学习结构化的 latent representation（ latent space相关），其中状态之间的距离直接与转换它们所需的最小动作数相关。所提出的度量空间公式提供了不确定性的几何解释，无需显式的概率建模。为实现这一点，本文引入了多模态潜转换模型和基于反距离加权的传感器融合机制，允许在无需先验噪声分布知识的情况下自适应整合多种传感器模态。在一系列多模态 RL 任务上进行了实证验证，证明了该方法对传感器噪声的更强鲁棒性和优于基线方法的状态估计能力。实验表明，通过学习到的表示，RL 智能体的性能得到提升，无需显式的噪声增强。结果表明，利用转换感知的度量空间为顺序决策中的鲁棒状态估计提供了一种原则性和可扩展的解决方案。  

---

### 29 Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models

**link**: https://arxiv.org/pdf/2602.11824.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 尽管大型视觉语言模型（LVLMs）具备先进的能力，但它们经常遭受对象幻觉问题。原因之一是视觉特征和预训练文本表示在网络深层往往相互交织。为解决此问题，我们提出REVIS，这是一个无需训练的框架，旨在显式重新激活被抑制的视觉信息。基于 latent space 几何结构，REVIS通过正交投影提取纯视觉信息向量，并采用校准策略仅在抑制发生的精确深度进行稀疏干预。这种精准方法以最小的计算成本有效恢复了视觉信息。在标准基准上的实证评估表明，与最先进的基线相比，REVIS将对象幻觉率降低了约19%，同时保留了一般推理能力。  

---

### 30 Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments

**link**: https://arxiv.org/pdf/2602.11176.pdf  
**date**: 2026-02-13  
**keywords**: latent reasoning  
**abs**: 本文研究了大型语言模型（LLMs）在低数据环境中是否能够通过紧凑的上下文线索对日常活动进行推理，以预测人类活动及其持续时间。研究采用了一种检索增强的提示策略，整合了时间、空间、行为历史和人物角色四种上下文来源，并在CASAS Aruba智能家居数据集上进行评估。评估涵盖了两项任务：带持续时间估计的下一个活动预测和多步骤日常序列生成，每种任务都使用不同数量的少样本示例进行测试。结果表明，LLMs表现出强大的人类行为内在时间理解能力：即使在零样本设置下，它们也能生成连贯的日常活动预测，而添加一两个示例进一步优化了持续时间校准和分类准确性。这些发现表明，预训练语言模型可以作为有前景的时间推理器，捕捉重复的日常活动和依赖上下文的行为变化。  

---

### 31 Latent Generative Solvers for Generalizable Long-Term Physics Simulation

**link**: https://arxiv.org/pdf/2602.11229.pdf  
**date**: 2026-02-13  
**keywords**: latent space  
**abs**: 本文研究了跨异质偏微分方程（PDE）系统的长期代理模拟，提出了潜在生成求解器（LGS）框架。该框架分为两个阶段：（i）使用预训练的变分自编码器（VAE）将不同的PDE状态映射到共享的潜在物理空间；（ii）通过流匹配训练的Transformer学习概率潜在动力学。关键机制包括训练和推理期间扰动潜在输入的不确定性旋钮，以纠正流形外的滚动漂移并稳定自回归预测，以及使用流强迫从模型生成的轨迹更新系统描述符以对齐训练/测试条件。在涵盖12个PDE家族的约250万条轨迹上预训练后，LGS在短时间范围内匹配强确定性神经算子基线，同时显著减少长期滚动漂移，且计算量比非生成基线低70倍，能够高效适应分布外数据。  

---

### 32 Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification

**link**: https://arxiv.org/pdf/2602.11361.pdf  
**date**: 2026-02-13  
**keywords**: latent reasoning  
**abs**: 大型语言模型在复杂推理任务中常因幻觉和中间步骤错误累积而性能下降。本文提出PPCV框架以改善LLMs的推理能力。PPCV分为两个阶段：第一阶段，从原始问题生成初始推理路径，将问题的释义版本与该推理路径连接，基于推理路径中预测的top-1 token与预期token的不匹配识别关键token，并通过标准确认最终关键token；第二阶段，用候选替代词替换关键token，为原始和释义问题生成新推理路径，通过检查这些并行推理过程输出的一致性确定最终答案。在多个基准测试中，PPCV显著提升了主流LLMs的推理性能。  

---

### 33 ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces

**link**: https://arxiv.org/pdf/2602.11683.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 近期研究探索潜在推理（latent reasoning）以通过潜在空间中的连续表示替代显式推理轨迹来提升推理效率，但其效果因场景而异。分析表明，错误答案的推理轨迹包含的低置信步骤少于正确答案，而低置信思考的软嵌入聚合可能引入噪声，导致不可靠推理轨迹的高置信度。为此，本文提出ThinkRouter，一种推理时置信感知路由机制，通过在模型置信度低时路由至离散token空间，否则使用潜在空间，以避免高置信误差和噪声。在STEM推理和编码基准上的实验表明，ThinkRouter在多种大型推理模型上优于显式CoT、随机路由和潜在推理基线，Pass@1平均提升19.70个百分点，生成长度减少高达15.55%，并能校准两种推理方式的误差，通过全局降低模型置信度加速推理结束token生成。  

---

### 34 MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling

**link**: https://arxiv.org/pdf/2602.11761.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）向超长上下文应用的演进面临Transformer架构的高计算和内存成本挑战。现有稀疏和线性注意力机制虽试图缓解这些问题，但通常需在内存效率和模型性能间权衡。本文提出MiniCPM-SALA，一种9B参数混合架构，集成稀疏注意力（InfLLM-V2）的高保真长上下文建模与线性注意力（Lightning Attention）的全局效率。通过层选择算法以1:3比例整合两种机制，并使用混合位置编码（HyPE），模型在长上下文任务中保持效率和性能。此外，本文引入经济高效的持续训练框架，将预训练Transformer模型转换为混合模型，训练成本较从头训练降低约75%。实验表明，MiniCPM-SALA在256K token序列长度下推理速度达全注意力模型的3.5倍，支持1M token上下文，同时保持与全注意力模型相当的通用能力。  

---

### 35 Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces

**link**: https://arxiv.org/pdf/2602.12245.pdf  
**date**: 2026-02-13  
**keywords**: cs.LG  
**abs**: 联合嵌入预测架构（JEPAs）旨在通过从上下文嵌入预测目标嵌入来学习表示，在潜在空间中诱导标量兼容性能量。相比之下，准度量强化学习（QRL）通过有向距离值（成本到目标）研究目标条件控制，支持非对称动态下的目标达成。本文通过关注一类原则性的JEPA能量函数（内在能量，定义为可允许轨迹上累积局部努力的下确界）连接这两种观点。在温和的闭包和可加性假设下，任何内在能量都是准度量。在目标达成控制中，最优成本到目标函数恰好具有这种内在形式；反过来，训练用于建模内在能量的JEPAs属于QRL目标的准度量值类。此外，本文观察到为什么对称有限能量在单向可达性方面存在结构不匹配，从而在方向性重要时需要非对称（准度量）能量。  

---

### 36 Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis

**link**: https://arxiv.org/pdf/2602.11169.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: Transformer隐藏状态将信息编码为高维向量，但方向（表示空间中的取向）和幅度（向量范数）是否具有不同的功能作用尚不清楚。通过研究Pythia系列模型，本文发现显著的交叉分离：角度扰动对语言建模损失造成的损害高达42.9倍，而幅度扰动对句法处理造成的损害不成比例地更大（主谓一致任务准确率下降20.4% vs.1.6%）。这一发现得益于L2匹配扰动分析方法，确保角度和幅度扰动实现相同的欧几里得位移。因果干预表明，角度损伤主要通过注意力路径传递（通过注意力修复恢复28.4%的损失），而幅度损伤部分通过LayerNorm路径传递（通过LayerNorm修复恢复29.9%）。这些模式在Pythia架构家族的不同规模模型中一致复现。研究结果为方向和幅度在基于LayerNorm的架构中支持部分不同的计算角色提供了证据：方向优先影响注意力路由，而幅度调节细粒度句法判断的处理强度。在基于RMSNorm的架构中发现了不同模式，表明这种分离依赖于架构选择。这些结果完善了线性表示假设，并对模型编辑和可解释性研究具有启示意义。  

---

### 37 LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation

**link**: https://arxiv.org/pdf/2602.11451.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 循环Transformer已成为语言领域推理任务中高效且强大的模型类别。近期研究表明，这些模型在算法和推理任务上表现出色，暗示循环架构具有潜在推理的归纳偏好。然而，先前的方法在训练和推理阶段固定循环迭代次数，未解决模型能否在可变计算预算下灵活调整计算深度的问题。本文提出LoopFormer，一种在变长轨迹上训练的循环Transformer，以实现预算条件推理。其核心贡献是一种捷径一致性训练方案，用于对齐不同长度的轨迹，确保较短循环产生信息丰富的表示，而较长循环继续优化这些表示。LoopFormer将每个循环条件化于当前时间和步长，使表示能在不同长度的轨迹上一致演化，而非漂移或停滞。实验表明，即使在严格的计算约束下，LoopFormer在语言建模和推理基准测试中仍表现稳健，且随着预算增加能优雅扩展。这些结果表明循环Transformer本质上适用于自适应语言建模，为可控且预算感知的大型语言模型开辟了道路。  

---

### 38 Retrieval Heads are Dynamic

**link**: https://arxiv.org/pdf/2602.11162.pdf  
**date**: 2026-02-13  
**keywords**: latent space  
**abs**: 最近研究发现大型语言模型（LLMs）中负责从输入上下文中提取信息的“检索头”。然而，先前工作主要依赖跨数据集聚合的静态统计，识别平均执行检索的头。此视角忽略了自回归生成的细粒度时间动态。本文从动态角度研究检索头，确立三个核心主张：（1）动态性：检索头跨时间步动态变化；（2）不可替代性：动态检索头在每个时间步特定，无法被静态检索头有效替代；（3）相关性：模型的隐藏状态编码未来检索头模式的预测信号，表明存在内部规划机制。在Needle-in-a-Haystack任务和多跳QA任务上验证发现，并在动态检索增强生成框架中量化动态和静态检索头的效用差异。本研究为LLMs的内部机制提供新见解。  

---

### 39 Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering

**link**: https://arxiv.org/pdf/2602.11167.pdf  
**date**: 2026-02-13  
**keywords**: latent space  
**abs**: 大型语言模型（LLMs）常产生幻觉，生成虚假信息，在敏感领域危害大。为系统研究此现象，本文引入FalseCite数据集，捕获由误导性引用诱导的幻觉响应。通过GPT-4o-mini等模型实验，发现虚假引用会增加幻觉，尤其在GPT-4o-mini中。分析幻觉模型的内部状态，可视化并聚类隐藏状态向量，发现无论是否幻觉，向量均呈现独特角状分布。本工作证明FalseCite可作为评估和缓解LLM幻觉的基础工具。  

---

### 40 PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models

**link**: https://arxiv.org/pdf/2602.11170.pdf  
**date**: 2026-02-13  
**keywords**: latent reasoning  
**abs**: 大型语言模型在算法推理任务上表现有限，本文提出PRIME框架解决此问题。该框架包含执行器（逐步推理）、验证器（约束检查）和协调器（回溯控制）三个智能体，通过策略优化提升性能。同时构建PRIME-Bench基准，含86个算法任务共51,600实例。实验显示，PRIME将平均准确率从26.8%提升至93.8%，尤其在状态跟踪任务（如图灵机模拟、长除法）上改进显著。消融研究表明迭代验证可防止错误传播，小模型经优化后性能接近大模型8倍。本研究为LLM算法推理提供新范式。  

---

### 41 MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization

**link**: https://arxiv.org/pdf/2602.11182.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 现有记忆系统使大型语言模型（LLMs）能够通过持久化超出有限上下文窗口的历史交互来支持长程人机交互。然而，尽管最近的方法成功构建了有效的记忆，但它们往往破坏了交互会话中固有的逻辑和时间关系，导致记忆单元碎片化和推理性能下降。本文提出MetaMem，一种新框架，通过自进化的元记忆增强记忆系统，旨在教会LLMs如何有效利用记忆知识。在元记忆优化过程中，MetaMem通过自我反思推理过程并执行操作来更新当前元记忆状态，迭代地提炼不同任务间的可迁移知识利用经验。累积的元记忆单元作为显式的知识利用经验，指导LLM系统地识别和整合分散记忆片段中的关键证据。大量实验证明了MetaMem的有效性，其性能显著优于强基线超过3.6%。所有代码和数据集均可获取。  

---

### 42 Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm

**link**: https://arxiv.org/pdf/2602.11543.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 预训练大型语言模型（LLMs）通常需要具有数千个高内存GPU的集中式集群。最近的去中心化训练方法通过联邦优化减少了通信开销；然而，它们仍然需要在每个节点上训练整个模型，受限于GPU内存限制。在这项工作中，我们提出了SParse Expert Synchronization（SPES），一种用于预训练混合专家（MoE）LLMs的内存高效去中心化框架。SPES每个节点仅训练专家的一个子集，显著降低了内存占用。每个节点更新其本地专家并定期与其他节点同步，消除了全参数传输，同时确保高效的知识共享。为了加速收敛，我们引入了专家合并预热策略，其中专家在训练早期交换知识，以快速建立基础能力。使用SPES，我们在互联网连接上使用16个独立的48GB GPU训练了一个2B参数的MoE LLM，在相似的计算预算下实现了与集中训练LLMs相当的性能。我们进一步通过从头开始训练7B模型和从密集检查点升级的9B模型展示了可扩展性，两者都匹配了先前的集中式基线。  

---

### 43 Scene-Aware Memory Discrimination: Deciding Which Personal Knowledge Stays

**link**: https://arxiv.org/pdf/2602.11607.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 智能设备已深度融入日常生活，生成大量用户交互，形成有价值的个人知识。在用户内存中高效组织这些知识对于实现个性化应用至关重要。然而，当前使用大型语言模型（LLMs）进行内存写入、管理和读取的研究面临过滤无关信息和计算成本上升的挑战。受人类大脑选择性注意概念的启发，我们引入了内存辨别任务。为应对此任务中的大规模交互和多样化内存标准，我们提出场景感知内存辨别方法（SAMD），包括门控单元模块（GUM）和集群提示模块（CPM）两个关键组件。GUM通过过滤非记忆交互并专注于与应用需求最相关的显著内容来提高处理效率。CPM建立自适应内存标准，指导LLMs辨别应记住或丢弃的信息，并分析用户意图与内存上下文之间的关系以构建有效的集群提示。综合直接和间接评估证明了我们方法的有效性和泛化性。我们独立评估内存辨别的性能，表明SAMD成功召回大部分可记忆数据并在动态场景中保持稳健。此外，当集成到个性化应用中时，SAMD显著提高了内存构建的效率和质量，从而更好地组织个人知识。  

---

### 44 Thinking with Drafting: Optical Decompression via Logical Reconstruction

**link**: https://arxiv.org/pdf/2602.11731.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL, latent reasoning  
**abs**: 现有的多模态大型语言模型已实现高保真视觉感知和探索性视觉生成。然而，在复杂推理任务中存在精度悖论：光学感知系统转录符号但未捕获逻辑拓扑，而基于像素的生成模型产生的视觉伪影缺乏数学精确性。为弥合这一差距，我们提出将视觉输入的推理重新概念化为光学解压缩——从压缩视觉令牌重建潜在逻辑结构的过程。遵循“解析即推理”的公理，我们引入“Thinking with Drafting (TwD)”，它利用极简领域特定语言(DSL)作为接地中间表示。与直接生成答案的标准方法不同，TwD迫使模型将其心智模型编写为可执行代码，生成确定性视觉证明以进行自我验证。为验证这一点，我们提出了VisAlg视觉代数基准。实验表明，TwD可作为优越的认知支架。我们的工作建立了一个闭环系统，其中视觉生成不作为创造性输出，而是作为逻辑验证器，为视觉推理提供了可推广的路径。  

---

### 45 Tiny Recursive Reasoning with Mamba-2 Attention Hybrid

**link**: https://arxiv.org/pdf/2602.12078.pdf  
**date**: 2026-02-13  
**keywords**: latent reasoning, latent space  
**abs**: 最近关于TRM等递归推理模型的研究表明，小型网络（700万参数）可以通过潜在递归（在隐藏表示空间中进行迭代优化，无需生成中间tokens）在抽象推理任务上实现强大性能。本文探讨了算子选择问题：Mamba-2的状态空间递归本身就是一种迭代优化形式，是递归推理的自然候选。通过将TRM中的Transformer块替换为Mamba-2混合算子（保持参数相当：683万 vs 686万参数），在ARC-AGI-1上，混合模型的pass@2提高了2.0%（45.88% vs 43.88%），在更高K值（如pass@100）上一致优于原模型（+4.75%），同时保持pass@1性能相当。结果验证了Mamba-2混合算子在递归框架中保留推理能力，确立了基于SSM的算子作为递归算子设计空间的可行候选，并为理解最佳混合策略迈出了第一步。  

---

### 46 Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning

**link**: https://arxiv.org/pdf/2602.12146.pdf  
**date**: 2026-02-13  
**keywords**: latent space  
**abs**: 高效无损压缩对于最小化存储成本和传输开销同时保持数据完整性至关重要。传统压缩技术（如基于字典和统计的方法）难以最优利用复杂数据格式中的结构和冗余。深度学习的最新进展为压缩开辟了新途径，但许多现有方法依赖密集向量表示，掩盖了底层token结构。为解决这些限制，本文提出一种新型无损压缩方法，将强化学习应用于T5语言模型架构。该方法将数据压缩为token序列而非传统向量表示。与通常将信息编码到连续潜在空间的自编码器不同，本方法保留基于token的结构，更接近原始数据格式，从而在保持语义完整性的同时实现更高压缩比。通过使用离策略强化学习算法训练模型，优化序列长度以最小化冗余并提高压缩效率。本文引入了基于先进强化学习技术的高效自适应数据压缩系统，独立于外部语法或世界知识运行，与传统方法相比显示出显著的压缩比改进。  

---

### 47 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models

**link**: https://arxiv.org/pdf/2602.12036.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 本文针对强化学习中可验证提示（Verifiable Prompts）存在大量非信息性示例且扩展成本高的问题，提出了Composition-RL方法。该方法旨在更好地利用通过率为1的简单提示，通过自动将多个问题组合成新的可验证问题，并将这些组合提示用于强化学习训练。实验表明，Composition-RL在4B到30B不同模型规模上均能持续提升大语言模型的推理能力，其课程学习变体通过逐步增加组合深度可进一步提升性能，同时还能通过组合不同领域的提示实现更有效的跨域强化学习。  

---

### 48 P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling

**link**: https://arxiv.org/pdf/2602.12116.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 针对大语言模型个性化对齐中获取准确用户特定奖励信号的挑战，本文提出了P-GenRM——首个具有测试时用户缩放能力的个性化生成奖励模型。该模型将偏好信号转化为结构化评估链，以推导跨场景的自适应角色和评分标准；通过将用户聚类为用户原型，引入双粒度缩放机制（个体层面自适应缩放聚合用户评分方案，原型层面融合相似用户偏好），从而减轻偏好推理噪声并增强对新用户的泛化能力。实验表明，P-GenRM在个性化奖励模型基准上实现了最先进性能，平均提升2.31%，且在分布外数据集上表现出强泛化性，测试时用户缩放机制额外带来3%的性能提升。  

---

### 49 Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation

**link**: https://arxiv.org/pdf/2602.11799.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 多模态推荐因物品具有文本和图像等丰富属性而备受关注，语义ID方法能将这些信息离散为紧凑token，但存在两个挑战：（1）次优Tokenization：现有方法（如RQ-VAE）缺乏跨模态共享语义与模态特定细节的解耦，导致冗余或坍缩；（2）架构-数据不匹配：普通Transformer将语义ID视为扁平流，忽略用户交互、物品和token的层次结构，将物品扩展为多个token会增加长度和噪声，使注意力偏向局部细节而非整体语义。本文提出Hi-SAM这一层次结构感知的多模态框架，包含两项设计：（1）解耦语义Tokenizer（DST）：通过几何感知对齐统一模态，采用粗到细策略量化，共享码本提取共识，模态特定码本从残差中恢复细节，并通过互信息最小化强制解耦；（2）层次记忆锚定Transformer（HMAT）：通过层次RoPE将位置编码分为项间和项内子空间以恢复层次结构，插入锚定Token将物品浓缩为紧凑记忆，保留当前物品细节的同时仅通过压缩摘要访问历史。在真实世界数据集上持续优于SOTA基线，尤其在冷启动场景，部署于服务数百万用户的大规模社交平台后，核心在线指标提升6.55%。  

---

### 50 Causal-JEPA: Learning World Models through Object-Level Latent Interventions

**link**: https://arxiv.org/pdf/2602.11389.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 世界模型需要强大的关系理解能力以支持预测、推理和控制。尽管以对象为中心的表示提供了有用的抽象，但它们不足以捕捉依赖于交互的动态。因此，我们提出了C-JEPA，一种简单灵活的以对象为中心的世界模型，它将掩码联合嵌入预测从图像补丁扩展到以对象为中心的表示。通过应用对象级掩码（要求从其他对象推断某个对象的状态），C-JEPA诱导具有反事实效应的潜在干预，防止捷径解决方案，使交互推理变得至关重要。实验表明，C-JEPA在视觉问答任务中取得了持续的改进，在反事实推理方面比没有对象级掩码的相同架构绝对提升约20%。在智能体控制任务上，C-JEPA仅使用基于补丁的世界模型所需总潜在输入特征的1%，就能实现相当的性能，从而实现更高效的规划。最后，我们提供了形式化分析，证明对象级掩码通过潜在干预诱导因果归纳偏置。我们的代码可在...  

---

### 51 Human-Inspired Continuous Learning of Internal Reasoning Processes: Learning How to Think for Adaptive AI Systems

**link**: https://arxiv.org/pdf/2602.11516.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI, latent reasoning  
**abs**: 学习内部推理过程对于开发能够在动态现实环境中持续适应的AI系统至关重要。本文提出了一个受人类启发的持续学习框架，将推理、行动、反思和验证统一在一个通过并行学习增强的顺序推理模型中。该框架明确将内部思维过程视为主要学习对象，系统地记录内部推理轨迹和环境交互作为结构化学习材料，使系统不仅能优化任务级内容，还能优化推理活动的组织、调度和演化。这种设计实现了边处理边学习，允许认知结构在执行过程中得到改进。此外，该框架支持用学习到的程序有控制地替换预定义逻辑，并引入分层的学习机制，联合适应任务级参数和学习策略。实验结果表明，在温度传感器异常检测任务中，纳入内部过程学习使平均运行时间减少了23.9%。  

---

### 52 SemaPop: Semantic-Persona Conditioned Population Synthesis

**link**: https://arxiv.org/pdf/2602.11569.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI, latent space  
**abs**: 人口合成是个体层面社会经济模拟的关键组成部分，但由于需要联合表示统计结构和潜在行为语义而仍然具有挑战性。现有人口合成方法主要依赖结构化属性和统计约束，在能够捕捉调查数据中隐含的抽象行为模式的语义条件人口生成方面存在差距。本研究提出了SemaPop，一种将大型语言模型(LLMs)与生成人口建模相结合的语义-统计人口合成模型。SemaPop从个人调查记录中导出高级角色表示，并将其作为人口生成的语义条件信号，同时引入边际正则化以强制与目标人口边际对齐。实验表明，SemaPop-GAN实现了改进的生成性能，在语义条件下产生更接近目标边际和联合分布的结果，同时保持样本级别的可行性和多样性。  

---

### 53 Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation

**link**: https://arxiv.org/pdf/2602.11635.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI, latent reasoning  
**abs**: 多模态大型语言模型(MLLMs)在面向感知的任务上取得了很强的性能，但其执行数学空间推理的能力（定义为解析和操作二维和三维关系的能力）仍不清楚。人类能轻松解决教科书式的空间推理问题，准确率超过95%，但研究发现大多数领先的MLLMs在相同任务上甚至无法达到60%的准确率。这种显著差距凸显了当前模型在空间推理方面的根本弱点。为了研究这一差距，本文提出了MathSpatial，一个用于评估和改进MLLMs中空间推理的统一框架。MathSpatial包括三个互补组件：(i) MathSpatial-Bench，一个包含2K个问题的基准，横跨三个类别和十一个子类型，旨在将推理难度与感知噪声分离；(ii) MathSpatial-Corpus，一个包含8K个带有已验证解决方案的额外问题的训练数据集；(iii) MathSpatial-SRT，将推理建模为由三个原子操作（关联、约束和推断）组成的结构化轨迹。实验表明，在MathSpatial上微调Qwen2.5-VL-7B实现了有竞争力的准确性，同时减少了25%的令牌。  

---

### 54 Query-focused and Memory-aware Reranker for Long Context Processing

**link**: https://arxiv.org/pdf/2602.12192.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 基于现有对大型语言模型检索头的分析，本文提出了一种查询聚焦且记忆感知的重排序框架，该框架训练模型利用选定头的注意力分数来估计段落-查询相关性。此方法提供了一种列表式解决方案，在排序过程中利用整个候选短列表中的整体信息，同时自然生成连续的相关性分数，无需李克特量表监督即可在任意检索数据集上进行训练。该框架轻量且高效，仅需小规模模型（如4B参数）即可实现强性能。实验表明，该方法在多个领域（包括维基百科和长叙事数据集）上优于现有的最先进点式和列表式重排序器，并在评估对话理解和记忆使用能力的LoCoMo基准上建立了新的最先进水平。  

---

### 55 Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation

**link**: https://arxiv.org/pdf/2602.12235.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 高效的长上下文处理仍是当代大型语言模型（LLMs）的关键挑战，尤其在资源受限环境中。软压缩架构通过将长 token 序列替换为更小的学习压缩 token 集来扩展有效上下文长度，但其可压缩性极限以及压缩何时开始擦除任务相关内容仍未被充分探索。本文定义了“token 溢出”为压缩表示不再包含足够信息以回答给定查询的状态，并提出了表征和检测该状态的方法。在 xRAG 软压缩设置中，研究发现与查询无关的饱和统计量能可靠区分压缩与未压缩 token 表示，同时基于查询和上下文 xRAG 表示的轻量级探测分类器在 HotpotQA、SquADv2 和 TriviaQA 数据集上平均达到 0.72 的 AUC-ROC，表明结合查询信息可提高检测性能。  

---

### 56 On-Policy Context Distillation for Language Models

**link**: https://arxiv.org/pdf/2602.12275.pdf  
**date**: 2026-02-13  
**keywords**: cs.CL  
**abs**: 上下文蒸馏使语言模型能够将上下文知识内化到其参数中。本文提出了 On-Policy Context Distillation (OPCD) 框架，通过让学生模型在自身生成的轨迹上训练，同时最小化与上下文条件教师模型的反向 KL 散度，将在线策略蒸馏与上下文蒸馏相结合。OPCD 在两个重要应用中展示了有效性：经验知识蒸馏（模型从历史解决方案轨迹中提取和巩固可迁移知识）和系统提示蒸馏（模型内化优化提示中编码的有益行为）。在数学推理、文本游戏和特定领域任务中，OPCD 持续优于基线方法，在提高任务准确性的同时更好地保留分布外能力，并实现了有效的跨尺寸蒸馏，使小型学生模型能从大型教师模型中内化经验知识。  

---

### 57 LawThinker: A Deep Research Legal Agent in Dynamic Environments

**link**: https://arxiv.org/pdf/2602.12056.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 法律推理不仅需要正确的结果，还需要程序合规的推理过程。现有方法缺乏验证中间推理步骤的机制，导致不适用的法规引用等错误在推理链中未被检测到而传播。为此，本文提出LawThinker，一种自主法律研究智能体，采用探索-验证-记忆（Explore-Verify-Memorize）策略适应动态司法环境。核心思想是在每个知识探索步骤后将验证作为原子操作强制执行。DeepVerifier模块从知识准确性、事实-法律相关性和程序合规性三个维度检查每个检索结果，并配备记忆模块用于长周期任务中的跨轮次知识重用。在动态基准J1-EVAL上的实验表明，LawThinker比直接推理方法提高24%，比基于工作流的方法提高11%，在面向过程的指标上改进尤为显著。在三个静态基准上的评估进一步证实了其泛化能力。  

---

### 58 The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context

**link**: https://arxiv.org/pdf/2602.12108.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 在《哈利·波特》的世界中，当邓布利多的大脑不堪重负时，他会将记忆提取到冥想盆中以便日后重温。在人工智能领域，尽管我们拥有类似冥想盆的成熟数据库和检索系统，但我们的模型却莫名其妙地缺少操作它的“魔杖”——它们仍像没有自主权的邓布利多，被动接受人工构建的上下文作为其全部记忆。本文最终将“魔杖”交到了模型手中。我们引入StateLM，这是一类新的基础模型，具备内部推理循环以管理自身状态。我们为模型配备了上下文修剪、文档索引和笔记记录等内存工具，并训练它主动管理这些工具。通过学习动态构建自身上下文，该模型突破了固定窗口的架构限制。在不同模型规模的实验中，StateLM在多种场景下均表现出有效性：在长文档问答任务上，各规模的StateLM均持续优于标准LLM；在聊天记忆任务上，它们比标准LLM的绝对准确率提高10%至20%；在深度研究任务BrowseComp-Plus上，性能差距更为显著，StateLM准确率高达52%，而标准LLM同类模型仅约5%。最终，我们的方法将LLM从被动预测器转变为状态感知智能体，使推理成为一种有状态且可管理的过程。  

---

### 59 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision

**link**: https://arxiv.org/pdf/2602.12164.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLM）已展现出卓越的推理能力，协同进化范式在代码和数学等领域显示出良好前景。然而，在科学推理任务中，这些模型由于不可靠的解决方案评估和有限的验证策略多样性而仍然脆弱。本文提出Sci-CoE，一种两阶段科学协同进化框架，使模型能够通过从稀疏监督到无监督学习的过渡，同时作为求解器和验证器进行自我进化。第一阶段，模型使用少量标注数据为验证器建立基本的正确性判断锚点。第二阶段，引入一种几何奖励机制，联合考虑一致性、可靠性和多样性，驱动在无标签数据上的大规模自我迭代。在多个通用科学基准上的实验表明，Sci-CoE增强了复杂推理能力，并表现出强大的可扩展性，有助于构建更稳健和多样化的评估系统。  

---

### 60 PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts

**link**: https://arxiv.org/pdf/2602.11807.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 潜在扩散模型（LDMs）在高分辨率（≤0.25°）集合天气预报中面临扩散能力有限的问题，其中扩散能力表征潜在数据分布被扩散过程建模的难易程度。与自然图像领域不同，气象场缺乏与任务无关的基础模型和明确的语义结构，导致基于VFM的正则化方法不适用。此外，现有基于频率的方法在同质性假设下对所有通道施加相同的频谱正则化，这在多元气象数据的变量间频谱异质性下导致正则化强度不均匀。为解决这些挑战，本文提出3D掩码自编码器（3D-MAE），将天气状态演变特征编码为扩散模型的额外条件，同时提出变量感知掩码频率建模（VA-MFM）策略，基于每个变量的频谱能量分布自适应选择阈值。综合这些方法，本文提出PuYun-LDM，该模型增强了潜在扩散能力，在短预报时效上性能优于ENS，在长时效上与ENS相当。PuYun-LDM在单个NVIDIA H200 GPU上可在五分钟内生成15天全球预报（时间分辨率6小时），且集合预报可并行高效生成。  

---

### 61 From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders

**link**: https://arxiv.org/pdf/2602.11881.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 稀疏自编码器（SAEs）已被证明能有效从大型语言模型（LLMs）中提取单语义特征，但这些特征通常是孤立识别的。然而，大量证据表明LLMs捕捉了自然语言的内在结构，特别是“特征分裂”现象表明这种结构具有层次性。为捕捉这一点，本文提出层次化稀疏自编码器（HSAE），它联合学习一系列SAE及其特征间的父子关系。HSAE通过两种新颖机制加强父子特征的对齐：结构约束损失和随机特征扰动机制。在不同LLM和层上的大量实验表明，HSAE能一致地恢复语义上有意义的层次结构，这得到定性案例研究和严格定量指标的支持。同时，HSAE在不同字典大小下保持了标准SAE的重建保真度和可解释性。本文工作为发现和分析LLM表示中嵌入的多尺度概念结构提供了强大且可扩展的工具。  

---

### 62 MEME: Modeling the Evolutionary Modes of Financial Markets

**link**: https://arxiv.org/pdf/2602.11918.pdf  
**date**: 2026-02-13  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLMs）通过处理大量非结构化数据来模拟类人分析流程，在量化金融中展现出巨大潜力。然而，当前基于LLM的方法主要遵循两种范式：以资产为中心（专注于个股预测）或以市场为中心（专注于投资组合分配），往往忽视驱动市场运动的底层推理。本文提出一种面向逻辑的视角，将金融市场建模为竞争投资叙事的动态演化生态系统，称为“思维模式”。为实现这一视角，本文引入MEME（金融市场演化模式建模），旨在通过演化逻辑的视角重建市场动态。MEME采用多智能体提取模块将噪声数据转换为高保真投资论点，并利用高斯混合模型在语义空间中揭示潜在共识。为建模不同市场条件下的语义漂移，还实现了时间评估和对齐机制来跟踪这些模式的生命周期和历史盈利能力。通过优先考虑持久的市场智慧而非短暂异常，MEME确保投资组合构建由稳健推理指导。在2023-2025年三个异质中国股票池上的大量实验表明，MEME持续优于七个最先进基线。进一步的消融研究、敏感性分析、生命周期案例研究和成本分析验证了MEME识别和适应金融市场演化共识的能力。