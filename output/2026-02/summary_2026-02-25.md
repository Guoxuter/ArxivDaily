### 1 PreScience: A Benchmark for Forecasting Scientific Contributions

**link**: https://arxiv.org/pdf/2602.20459.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 本文介绍了PreScience基准，用于预测科学贡献，将研究过程分解为四个任务：合作者预测、先前工作选择、贡献生成和影响预测。数据集包含98K篇AI相关论文，具有作者身份、时间对齐元数据和结构化图。开发了基线和评估方法，包括基于LLM的贡献相似性度量LACERScore。结果显示，最先进的LLM在贡献生成任务上仅达中等相似度，合成语料在多样性和新颖性上落后于人类研究，表明科学预测有改进空间。

---

### 2 ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory

**link**: https://arxiv.org/pdf/2602.20502.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 现有GUI代理通过逐步调用视觉语言模型运行，导致成本和延迟随步骤增加，且准确性有限。本文提出ActionEngine，通过状态机内存将反应式GUI代理转变为可编程代理，解决上述问题。

---

### 3 Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination

**link**: https://arxiv.org/pdf/2602.20517.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 有效人机协作需AI代理展示和响应类人行为，但当前模仿学习方法难以捕捉人类行为多样性和非马尔可夫特性。受内在语言理论启发，提出MIMIC框架，使用视觉语言模型训练条件变分自编码器生成内在语言，并基于此选择行为。实验表明，MIMIC增强行为多样性和保真度，支持无需额外演示的行为引导。

---

### 4 Talking to Yourself: Defying Forgetting in Large Language Models

**link**: https://arxiv.org/pdf/2602.20162.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: 微调大型语言模型时，灾难性遗忘是主要挑战。提出SA-SFT轻量级自我增强程序，LLM在微调前生成自我对话，并将生成数据与任务数据混合，无需修改优化或训练计划。

---

### 5 Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning

**link**: https://arxiv.org/pdf/2602.20197.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 带可验证奖励的强化学习（RLVR）增强多模态大型语言模型推理能力，但训练中状态空间大和奖励稀疏导致熵崩溃或策略退化。提出CalibRL混合策略框架，通过分布感知优势加权和非对称激活函数支持可控探索。实验在八个基准数据集上验证改进。

---

### 6 From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production

**link**: https://arxiv.org/pdf/2602.20558.pdf  
**date**: 2026-02-25  
**keywords**: LLM Rec  
**abs**: 大型语言模型是生成式推荐系统的基础，但语言化挑战未充分探索。提出数据为中心框架，通过强化学习训练语言化代理将交互历史转换为优化文本上下文。实验显示，学习语言化方法比基线在发现项推荐准确性上提升93%。

---

### 7 Recursive Belief Vision Language Model

**link**: https://arxiv.org/pdf/2602.20659.pdf  
**date**: 2026-02-25  
**keywords**: latent space, latent reasoning  
**abs**: 当前视觉-语言-动作模型在部分可观测下难进行长时程操作。提出RB-VLA架构，通过自监督世界模型目标维持紧凑潜在状态，编码任务相关历史、动态和对象交互。实验在长时程基准上优于先前方法，推理延迟降低5倍。

---

### 8 ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction

**link**: https://arxiv.org/pdf/2602.20708.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 大型语言模型代理易受间接提示注入攻击。提出ICON探测-缓解框架，通过潜在空间轨迹探测器检测攻击，并执行选择性注意力引导中和攻击。评估显示，ICON实现0.4%攻击成功率，任务效用提升超50%。

---

### 9 Counterfactual Simulation Training for Chain-of-Thought Faithfulness

**link**: https://arxiv.org/pdf/2602.20710.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 思维链推理存在忠实性问题。引入反事实模拟训练（CST），通过奖励使模拟器预测模型在反事实输入上的输出来提高忠实性。实验在高达235B参数模型上显示，CST提高监控准确率35点。

---

### 10 Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning

**link**: https://arxiv.org/pdf/2602.20722.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 在线策略强化学习存在经验浪费和奖励同质化问题。引入批适应策略优化（BAPO）离线策略框架，通过重新评估历史困难样本并重用高质量样本提高数据效率。实验在数学、规划和视觉推理任务上比GRPO平均提高12.5%。

---

### 11 Modality-Guided Mixture of Graph Experts with Entropy-Triggered Routing for Multimodal Recommendation

**link**: https://arxiv.org/pdf/2602.20723.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 多模态推荐中信号异质且冲突，融合挑战大。提出MAGNET框架，通过模态引导自适应图专家混合网络和熵触发路由增强可控性、稳定性和可解释性。实验在公共基准上显示持续改进。

---

### 12 CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference

**link**: https://arxiv.org/pdf/2602.20732.pdf  
**date**: 2026-02-25  
**keywords**: cs.AI  
**abs**: 长上下文大型语言模型解码受KV缓存限制。提出CHESS算法-系统协同设计，通过上下文感知分层选择策略动态重构上下文。评估显示，仅用1% KV缓存即超全KV质量，吞吐量提升4.56倍。

---

### 13 Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems

**link**: https://arxiv.org/pdf/2602.20379.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: 企业检索增强生成助手在多轮工作流中运行，现有评估框架无法捕捉企业特有故障模式，如案例识别错误和工作流错位。

---

### 14 VINA: Variational Invertible Neural Architectures

**link**: https://arxiv.org/pdf/2602.20480.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 归一化流和可逆神经网络适合生成建模，但缺乏现实假设下近似质量的理论保证。本文从理论和实践角度重新审视这些架构。

---

### 15 Personal Information Parroting in Language Models

**link**: https://arxiv.org/pdf/2602.20580.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: 语言模型在训练数据中记忆个人信息，增加隐私风险。开发正则表达式与规则检测器套件，测量Pythia模型记忆情况：模型大小和预训练量与记忆正相关。建议对数据集筛选和匿名化。

---

### 16 SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing

**link**: https://arxiv.org/pdf/2602.20751.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: 为开放式生成设计对齐奖励是挑战。提出SibylSense，通过可调整验证评分项记忆库适应冻结评分生成器。实验在开放式任务上显示优于静态基线。

---

### 17 The Art of Efficient Reasoning: Data, Reward, and Optimization

**link**: https://arxiv.org/pdf/2602.20945.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: 大型语言模型推理面临计算开销。研究高效推理机制，通过两阶段范式（长度适应和推理优化）和统一协议实验。关键发现是简单提示上训练确保正奖励信号密度。在Qwen3系列上验证。

---

### 18 Model Merging in the Essential Subspace

**link**: https://arxiv.org/pdf/2602.20208.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 模型合并中任务干扰削弱性能。提出ESM框架，通过主成分分析对参数更新特征偏移，投影到基本子空间进行低秩分解，并引入多级极化缩放策略。实验在多任务合并中实现最先进性能。

---

### 19 Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation

**link**: https://arxiv.org/pdf/2602.20306.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 高保真心脏力学模型计算成本高。提出两步框架，将几何表示与物理响应学习解耦，实现数据稀缺下的形状感知替代建模。结果在理想化和患者数据集上显示准确预测和泛化。

---

### 20 Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition

**link**: https://arxiv.org/pdf/2602.20530.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 多模态生理和行为信号情感识别重要，但现有模型忽略共存情感间效价一致性和相关性。提出MPCL框架，通过多尺度联想记忆融合信号，构建情感特定原型记忆库，并引入记忆检索策略提取语义级关联。实验在公开数据集上优于现有方法。

---

### 21 Emergent Manifold Separability during Reasoning in Large Language Models

**link**: https://arxiv.org/pdf/2602.20338.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 思维链提示提升推理能力，但潜在表示几何动态未充分理解。通过流形容量理论分析，推理表现为瞬态几何脉冲：概念流形在计算前解缠为线性可分子空间，计算后压缩。解释为“动态流形管理”机制。

---

### 22 Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs

**link**: https://arxiv.org/pdf/2602.21198.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 具身大型语言模型赋予高级任务推理能力，但无法反思错误。引入反思性测试时规划，整合“行动中反思”和“对行动的反思”模式，通过内部和外部反思更新模型。实验在新基准上显著提升，定性分析显示行为纠正。

---

### 23 Test-Time Training with KV Binding Is Secretly Linear Attention

**link**: https://arxiv.org/pdf/2602.21204.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: KV绑定作为测试时训练常被解释为在线元学习，但分析揭示矛盾现象。重新审视公式，表明广泛类别的测试时训练架构可表示为学习线性注意力算子，带来架构简化和并行化等好处。

---

### 24 The Truthfulness Spectrum Hypothesis

**link**: https://arxiv.org/pdf/2602.20273.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 大型语言模型被报道线性编码真实性，但最近研究质疑普遍性。提出真实性光谱假说：表征空间包含从领域通用到领域特定方向。验证显示线性探针在多数领域泛化良好，但谄媚性谎言失败。

---

### 25 PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis

**link**: https://arxiv.org/pdf/2602.21046.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 提出PIME框架，通过原型分类和一致性训练整合结构扰动学习，构建结构化潜在空间。利用蒙特卡洛树搜索提取解释子图。实验在三个基准数据集上实现最先进性能，解释稳定性达90%。

---

### 26 SOM-VQ: Topology-Aware Tokenization for Interactive Generative Models

**link**: https://arxiv.org/pdf/2602.21133.pdf  
**date**: 2026-02-25  
**keywords**: latent space  
**abs**: 向量量化表示缺乏语义结构。介绍SOM-VQ令牌化方法，结合向量量化与自组织映射，学习具有显式低维拓扑的离散码本。实验在人体运动生成等领域显示更易学习令牌序列和可导航几何结构。

---

### 27 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking

**link**: https://arxiv.org/pdf/2602.21196.pdf  
**date**: 2026-02-25  
**keywords**: LLM Memory  
**abs**: Transformer处理长序列需上下文并行拆分计算。提出UPipe技术，通过注意力头级别分块减少自注意力层激活内存使用。实验在32B参数模型上减少87.5%内存，支持500万令牌上下文长度。

---

### 28 Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning

**link**: https://arxiv.org/pdf/2602.20528.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: STAR-LDM将潜在扩散规划与自回归生成结合，引入“思考”阶段暂停并通过扩散优化语义规划。评估在语言理解基准上优于同等规模模型，LLM评判者胜率>70%，允许轻量级分类器直接控制。

---

### 29 Disentangling Geometry, Performance, and Training in Language Models

**link**: https://arxiv.org/pdf/2602.20433.pdf  
**date**: 2026-02-25  
**keywords**: latent space  
**abs**: 研究语言模型性能与未嵌入矩阵几何特性关系，特别是有效秩。通过对108个模型实验，发现最佳性能模型通常高有效秩，但趋势非通用；有效秩受预训练超参数影响，几何特性主要反映训练选择。

---

### 30 FinAnchor: Aligned Multi-Model Representations for Financial Prediction

**link**: https://arxiv.org/pdf/2602.20859.pdf  
**date**: 2026-02-25  
**keywords**: latent space  
**abs**: 金融长文档预测挑战大。提出FinAnchor轻量级框架，通过选择锚嵌入空间并学习线性映射对齐多个LLM嵌入。实验在金融NLP任务中持续优于单模型基线和标准集成方法。

---

### 31 Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving

**link**: https://arxiv.org/pdf/2602.20973.pdf  
**date**: 2026-02-25  
**keywords**: cs.CL  
**abs**: 为评估大型语言模型数学推理能力，引入PC-FOL数据集，专注于分情况推理问题。实验显示线性推理与分情况推理问题间性能差距显著。基于图模型理论分析解释差异原因。

---

### 32 Understanding the Role of Rehearsal Scale in Continual Learning under Varying Model Capacities

**link**: https://arxiv.org/pdf/2602.20791.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 复述缓解灾难性遗忘，但理论理解有限。将基于复述的持续学习表述为迭代优化问题，推导复述规模对适应性、记忆性和泛化性的闭式分析。发现复述可能损害适应性，增加规模不一定改善记忆。

---

### 33 Exploring the Impact of Parameter Update Magnitude on Forgetting and Generalization of Continual Learning

**link**: https://arxiv.org/pdf/2602.20796.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 参数更新幅度是持续学习关键因素。从参数更新幅度角度刻画模型遗忘，形式化为参数空间中任务特定漂移引起的知识退化。推导最小化遗忘的最优更新幅度，并启发混合参数更新策略。实验显示优于标准方法。

---

### 34 Transcoder Adapters for Reasoning-Model Diffing

**link**: https://arxiv.org/pdf/2602.20904.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 推理训练对模型内部机制影响未知。引入转码器适配器技术，学习微调前后MLP计算差异的可解释近似。应用于Qwen2.5-Math-7B与推理蒸馏变体差异分析，适配器特征稀疏可解释，揭示推理训练见解。

---

### 35 From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning

**link**: https://arxiv.org/pdf/2602.20911.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 类别增量学习要求学习新类别不遗忘旧类别。提出SAEF方法，将适配器组织为结构化层次，基于语义关系分组任务，并在组内构建专家树。推理时激活相关专家。实验在多个基准上达到最先进性能。

---

### 36 GENSR: Symbolic Regression Based in Equation Generative Space

**link**: https://arxiv.org/pdf/2602.20557.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 符号回归旨在揭示数据隐藏方程，但离散方程空间搜索噪声大。提出GenSR框架，基于生成潜在空间，预训练双分支条件变分自编码器，将方程重新参数化。实验联合优化准确性、简洁性和效率。

---

### 37 Benchmarking GNN Models on Molecular Regression Tasks with CKA-Based Representation Analysis

**link**: https://arxiv.org/pdf/2602.20573.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 分子表示为SMILES字符串或图。对四种GNN架构在多样化数据集上系统基准测试，实现层次融合框架。使用中心核对齐研究嵌入表示相似性，发现GNN和指纹嵌入占据独立潜在空间。

---

### 38 GATES: Self-Distillation under Privileged Context with Consensus Gating

**link**: https://arxiv.org/pdf/2602.20574.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 研究监督不可靠环境中的自蒸馏，专注于非对称上下文文档接地问答。通过采样多个推理轨迹并使用一致性门控学习，在线获取监督信号。实验显示一致性门控轨迹蒸馏显著改善向无文档学生迁移。

---

### 39 QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs

**link**: https://arxiv.org/pdf/2602.20629.pdf  
**date**: 2026-02-25  
**keywords**: cs.LG  
**abs**: 标准“LLM作为评判者”协议在大学数学中存在系统性对齐差距。引入QEDBench基准，通过双评分测量对齐程度。评估发现某些模型存在正偏差，Gemini 3.0 Pro性能最优，其他模型在离散领域下降显著。发布为公共基准。