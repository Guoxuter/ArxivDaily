### 1 Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions

**link**: https://arxiv.org/pdf/2512.20831.pdf  
**date**: 2025-12-25  
**keywords**: cs.AI  
**abs**: 本文提出一种上下文敏感的抽象方法，用于解决参数化动作空间（结合离散动作和连续参数）的强化学习问题。现有方法存在局限性：规划需手工设计模型，标准RL算法无法同时处理离散和连续动作，而现有参数化动作方法依赖领域工程。新算法使智能体在线学习状态和动作抽象，逐步细化关键区域的抽象粒度，在长horizon、稀疏奖励设置中扩展RL应用。实验表明，该方法在连续状态参数化动作域中显著提升TD(λ)的样本效率，优于最先进基线。

---

### 2 Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment

**link**: https://arxiv.org/pdf/2512.20624.pdf  
**date**: 2025-12-25  
**keywords**: cs.AI  
**abs**: 本研究提出一种量子启发多智能体强化学习（QI-MARL）框架，优化无人机辅助6G网络部署中的探索-利用权衡。框架结合经典MARL与变分量子电路（VQCs）和量子近似优化算法（QAOA），通过贝叶斯推理和高斯过程建模环境动态。采用集中式训练与分散式执行（CTDE）范式增强局部可观测性。实验证明，QI-MARL在覆盖性能、样本效率和收敛速度上优于PPO和DDPG基线，实现更优的探索-利用平衡。

---

### 3 Improving Cardiac Risk Prediction Using Data Generation Techniques

**link**: https://arxiv.org/pdf/2512.20669.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 针对心脏康复数据稀缺和缺失值问题，本文提出基于条件变分自编码器（CVAE）的合成数据生成架构。该架构生成真实临床记录以扩充数据集，提升心脏风险预测模型性能，减少对高风险诊断（如运动负荷试验）的依赖。实验显示，合成数据连贯且真实，显著提高多种分类器的风险检测准确率，优于现有深度学习方法。

---

### 4 PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation

**link**: https://arxiv.org/pdf/2512.20687.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文提出PHOTON模型，通过分层自回归架构解决Transformer在长上下文解码中的内存瓶颈问题。PHOTON用多分辨率上下文访问替代平面扫描，维护自底向上编码器和自顶向下解码器的潜在流层次，逐步压缩和重建令牌表示。实验表明，PHOTON在吞吐量-质量权衡上优于Transformer，长上下文和多查询任务中KV缓存流量减少，内存吞吐量提升高达10³倍。

---

### 5 Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction

**link**: https://arxiv.org/pdf/2512.20664.pdf  
**date**: 2025-12-25  
**keywords**: cs.AI  
**abs**: 针对LLM推理中的幻觉问题，本文提出Eidoku神经符号验证门，将验证重构为约束满足问题（CSP）。该方法基于结构违规成本（图连接性、特征一致性和逻辑蕴涵）而非概率，拒绝上下文不连贯的候选推理。实验证明，Eidoku能确定性地检测并拒绝高概率但结构错误的“平滑谬误”，提供可验证的神经符号健全性检查。

---

### 6 Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning

**link**: https://arxiv.org/pdf/2512.20629.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本研究提出一种多智能体语言框架，支持策略持续进化而无需微调模型参数。框架采用双循环架构：行为循环通过环境奖励调整动作偏好，语言循环通过文本语义嵌入反思更新外部潜向量。该方法使抽象概念潜向量通过交互和反馈动态演化，实现高效的多智能体协作。

---

### 7 Uncovering Competency Gaps in Large Language Models and Their Benchmarks

**link**: https://arxiv.org/pdf/2512.20638.pdf  
**date**: 2025-12-25  
**keywords**: cs.CL  
**abs**: 本文提出一种基于稀疏自编码器（SAEs）的方法，自动识别LLM能力差距（模型差距）和基准覆盖不均（基准差距）。方法提取SAE概念激活和显著性加权性能分数，在内部表示上评估模型。实验显示，该方法能发现LLM在反谄媚和安全概念上的不足，并揭示基准的概念覆盖失衡。

---

### 8 SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance

**link**: https://arxiv.org/pdf/2512.21280.pdf  
**date**: 2025-12-25  
**keywords**: cs.CL  
**abs**: 针对工程手册处理问题，本文提出SMART SLM模型，采用分层架构：语法感知事实提取器提取主谓宾关系，记忆增强神经网络索引事实，6层Transformer融合事实生成响应。模型仅45.51M参数，比GPT-2小64%，但准确率高21.3%。支持索引快速路径（亚秒响应）和动态路径（新文档处理），减少幻觉并提升依据性。

---

### 9 STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting

**link**: https://arxiv.org/pdf/2512.21118.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文提出STLDM模型，用于降水临近预报。模型结合变分自编码器和条件网络，分两阶段处理：条件网络执行确定性预测，潜在扩散模型执行增强。实验在雷达数据集上显示，STLDM优于现有技术，提高预测准确性和推理效率，解决随机性挑战。

---

### 10 EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading

**link**: https://arxiv.org/pdf/2512.20817.pdf  
**date**: 2025-12-25  
**keywords**: cs.CL  
**abs**: 本文提出EssayCBM框架，通过评分标准对齐提升作文评分可解释性。框架评估八个写作概念（如论点清晰度）形成透明瓶颈，再由轻量网络计算最终分数。教师可调整概念预测实现人在回路评估。实验表明，EssayCBM匹配黑盒模型性能，同时提供可操作的概念级反馈。

---

### 11 Diffusion Models in Simulation-Based Inference: A Tutorial Review

**link**: https://arxiv.org/pdf/2512.20685.pdf  
**date**: 2025-12-25  
**keywords**: stat.ML  
**abs**: 本教程综述扩散模型在基于模拟推理（SBI）中的应用，涵盖训练、推理和评估设计。重点分析引导、分数组合、流匹配和一致性模型等概念，讨论噪声调度和参数化对效率的影响。案例研究显示，扩散模型在参数估计和观测建模中提供快速、准确的解决方案，并概述未来研究方向。

---

### 12 Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models

**link**: https://arxiv.org/pdf/2512.20954.pdf  
**date**: 2025-12-25  
**keywords**: cs.CL  
**abs**: 针对生物序列模型推理能力有限的问题，本文引入反射预训练，使模型生成辅助“思考标记”进行中间推理。方法增强标记集表达能力，支持自我纠正。实验证明，反射预训练显著提升蛋白质模型性能，克服氨基酸标记表达局限，实现类似思维链的复杂推理。

---

### 13 Can Agentic AI Match the Performance of Human Data Scientists?

**link**: https://arxiv.org/pdf/2512.20959.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本研究通过财产保险数据集探讨智能体AI与人类数据科学家性能差异。实验显示，依赖通用分析工作流程的智能体AI在处理隐藏变量（如图像数据）时表现不佳，而人类专家利用领域知识更优。结果突显当前AI在整合领域知识上的局限，强调未来需开发更智能的识别机制。

---

### 14 Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation

**link**: https://arxiv.org/pdf/2512.21002.pdf  
**date**: 2025-12-25  
**keywords**: cs.CL  
**abs**: 本文研究推理蒸馏中序列截断对计算-质量权衡的影响。分析表明，仅训练序列前50%标记（如思维链部分）可保留94%全序列性能，减少50%训练时间、内存和FLOPs。实验在数学基准上验证，优先早期标记提升效率，为推理蒸馏提供简单调节手段。

---

### 15 Generalization of Diffusion Models Arises with a Balanced Representation Space

**link**: https://arxiv.org/pdf/2512.20963.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文从表示学习视角分析扩散模型记忆与泛化差异。理论表明，记忆对应权重“尖峰”表示，泛化对应“平衡”表示捕捉数据统计。实验在真实扩散模型上验证，提出基于表示的记忆检测和编辑技术。结果强调良好表示对新颖生成建模的重要性。

---

### 16 Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies

**link**: https://arxiv.org/pdf/2512.20749.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文分析多模态自编码器的Lipschitz特性，提出正则化注意力融合方法提升稳定性。理论推导聚合方法的Lipschitz常数，实验验证新方法在一致性、收敛速度和准确性上优于现有策略，为多模态融合提供理论基础。

---

### 17 NVIDIA Nemotron 3: Efficient and Open Intelligence

**link**: https://arxiv.org/pdf/2512.20856.pdf  
**date**: 2025-12-25  
**keywords**: cs.CL  
**abs**: 本文介绍Nemotron 3系列模型（Nano、Super、Ultra），采用混合专家Mamba-Transformer架构支持100万token上下文。模型整合LatentMoE和MTP层提升质量与速度，通过多环境强化学习后训练支持推理、工具使用和预算控制。Nano高效，Super优化协作智能体，Ultra提供最先进准确性。

---

### 18 DiEC: Diffusion Embedded Clustering

**link**: https://arxiv.org/pdf/2512.20905.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文提出DiEC方法，利用预训练扩散U-Net内部激活执行无监督聚类。方法直接读取网络层次和噪声时间步的表示轨迹，避免单一编码器局限，提升聚类性能。

---

### 19 A Mechanistic Analysis of Transformers for Dynamical Systems

**link**: https://arxiv.org/pdf/2512.21113.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文从动力学系统视角分析Transformer表征能力。将因果自注意力解释为线性递归，揭示其在振荡系统中的平滑局限和在非线性系统中的自适应延迟嵌入机制。理论为Transformer动态建模成败提供依据，桥接经验与经典系统理论。

---

### 20 MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models

**link**: https://arxiv.org/pdf/2512.21231.pdf  
**date**: 2025-12-25  
**keywords**: cs.LG  
**abs**: 本文提出中期科学训练（MiST）框架，提升语言模型化学推理能力。框架包括数据预处理、29亿token预训练和10亿token微调，增强“潜在可解性”。实验显示，MiST使3B/7B模型在有机反应命名和无机材料生成任务上准确率提升至63.9%和67.4%，产生可解释推理轨迹。