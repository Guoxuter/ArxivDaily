### 1 Self-attention vector output similarities reveal how machines pay attention

**link**: https://arxiv.org/pdf/2512.21956.pdf  
**date**: 2025-12-29  
**keywords**: cs.CL  
**abs**: 本研究引入了一种量化自注意力机制内信息处理的新方法，分析BERT-12架构发现，最后几层注意力图聚焦于句子分隔符标记，提供基于语义的文本分割。通过上下文相似性矩阵，揭示了不同头和层内标记向量间的显著相似性，表明注意力头专注于不同语言特征（如标记重复或常见上下文）。随着层数增加，相似性从长程转向短程，最终偏好同一句子内的强相似性。每个头倾向于聚焦独特标记并构建相似性对。

---

### 2 Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought

**link**: https://arxiv.org/pdf/2512.21711.pdf  
**date**: 2025-12-29  
**keywords**: cs.CL  
**abs**: 本文从可靠性角度研究潜在令牌在大型语言模型中的机制，揭示其作为不可解释占位符而非忠实推理编码的根本弱点。通过引导实验和捷径实验（在MMLU和HotpotQA数据集），发现潜在令牌对扰动敏感性低且缺乏关键推理信息，持续利用数据集伪影夸大性能。这些发现将连续思维链重新定位为伪推理机制，掩盖对捷径的依赖而非表示真实推理。

---

### 3 dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning

**link**: https://arxiv.org/pdf/2512.21446.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出dUltra，一种基于组相对策略优化的强化学习框架，用于学习高效并行解码的解掩码策略。dUltra引入解掩码规划头预测token解掩码可能性，结合可验证奖励、蒸馏奖励等信号，联合优化扩散模型和规划器。在数学推理和代码生成任务上，dUltra改进了现有基线的精度-效率权衡，向实现“扩散优势”迈进。

---

### 4 MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding

**link**: https://arxiv.org/pdf/2512.21506.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文引入MotionTeller框架，原生集成分钟级可穿戴活动数据与大型语言模型，通过预训练编码器和轻量投影模块生成自然语言行为摘要。基于NHANES数据集（54383个数据对），模型实现高语义保真度（BERTScore-F1=0.924）和词汇准确性（ROUGE-1=0.722），比基线高7%。定性分析显示其捕获昼夜节律和行为转变，为行为监测和健康干预提供新途径。

---

### 5 Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models

**link**: https://arxiv.org/pdf/2512.21593.pdf  
**date**: 2025-12-29  
**keywords**: stat.ML  
**abs**: 本文提出残差先验扩散（RPD），一种两阶段概率框架：粗潜在先验模型捕获数据大规模结构，扩散模型表示先验与目标分布的残差。RPD提供可处理证据下界，优化简化为噪声预测目标，并引入辅助变量降低预测难度。实验表明，在细粒度数据集上，RPD准确捕获局部细节；在自然图像生成中，与基线相当或更优，尤其在少步推理时保持强性能。

---

### 6 When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning

**link**: https://arxiv.org/pdf/2512.21486.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出秩揭示功能贝叶斯张量补全（RR-FBTC）方法，通过多输出高斯过程建模潜在函数，处理实值索引张量并实现自动秩确定。建立了模型对连续多维信号的通用逼近性质，采用变分推理框架推导高效算法。合成和真实数据集实验显示，RR-FBTC优于现有方法。

---

### 7 Generative Actor Critic

**link**: https://arxiv.org/pdf/2512.21527.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出生成式演员评论家（GAC）框架，将策略评估重构为学习轨迹和回报联合分布的生成模型，策略改进重构为对该模型的推理。通过基于潜在计划向量的潜在变量模型实例化，开发新型推理策略用于利用（优化计划最大化回报）和探索（采样目标回报条件计划）。在Gym-MuJoCo和Maze2D基准上，GAC展示强离线性能和显著离线到在线改进。

---

### 8 Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations

**link**: https://arxiv.org/pdf/2512.21586.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出BCV-LR框架，通过自监督任务从视频输入提取动作相关潜在特征，利用基于动力学的无监督目标预测帧间潜在动作。预训练潜在动作通过在线微调与真实动作对齐，用于策略行为克隆。克隆策略丰富经验并微调潜在动作，实现样本高效的迭代改进。

---

### 9 Teaching People LLM's Errors and Getting it Right

**link**: https://arxiv.org/pdf/2512.21422.pdf  
**date**: 2025-12-29  
**keywords**: cs.CL  
**abs**: 本文分析人们过度依赖大型语言模型的原因，指出用户因模型在诗歌等任务表现而误判其在简单任务（如算术）的可靠性。先前研究通过聚类LLM失败区域并教授失败模式来缓解，但未完全成功。本文探讨其失败原因。

---

### 10 Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models

**link**: https://arxiv.org/pdf/2512.21439.pdf  
**date**: 2025-12-29  
**keywords**: cs.CL  
**abs**: 本文提出COMETH框架，整合概率情境学习器、LLM语义抽象和人类道德评估，建模情境对模糊行为可接受性的影响。基于300场景数据集（六种核心行为）和101参与者判断，通过LLM过滤器和MiniLM嵌入结合K-means生成行为簇。COMETH学习道德情境并提取二元特征解释预测，与人类判断一致性约为端到端LLM的两倍。

---

### 11 Explainable Multimodal Regression via Information Decomposition

**link**: https://arxiv.org/pdf/2512.22102.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出基于部分信息分解的多模态回归框架，将模态表示分解为独特、冗余和协同组件。通过强制高斯性和闭式条件独立性正则化器解决欠定问题，实现PID项解析计算。在六个数据集（包括脑年龄预测案例）上，框架在预测准确性和可解释性上优于SOTA方法，支持高效模态选择。

---

### 12 Synthetic Financial Data Generation for Enhanced Financial Modelling

**link**: https://arxiv.org/pdf/2512.21791.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出统一合成金融数据评估框架，应用于ARIMA-GARCH、VAEs和TimeGAN。基于标准普尔500数据，评估保真度（MMD）、时间结构（自相关）和下游任务（投资组合优化、波动率预测）。实证显示TimeGAN在真实性和连贯性上最优（MMD=1.84e-3），并提供模型选择指南。统一协议和代码库旨在标准化基准测试。

---

### 13 GQ-VAE: A gated quantized VAE for learning variable length tokens

**link**: https://arxiv.org/pdf/2512.21913.pdf  
**date**: 2025-12-29  
**keywords**: cs.LG  
**abs**: 本文提出门控量化变分自编码器（GQ-VAE），可独立预训练作为现有分词器的即插即用替代，学习编码变长离散令牌。GQ-VAE在压缩和语言建模性能上优于标准VQ-VAE，接近BPE性能。当词汇量较小时，GQ-VAE改善下游语言模型学习，并讨论未来方向。