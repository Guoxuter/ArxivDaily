### Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory

**link**: https://arxiv.org/pdf/2511.23083.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 高容量核Hopfield网络表现出以极端稳定性为特征的“优化脊”。虽然之前与“谱集中”相关联，但其起源仍不明确。本文在统计流形上分析网络动力学，揭示“脊”对应于“稳定性边缘”——Fisher信息矩阵变得奇异的临界边界。我们证明，表观的欧几里得力对抗是黎曼空间中“对偶平衡”的表现。这通过最小描述长度原理统一了学习动力学和容量，为自组织临界性提供了几何理论。  

---

### Cacheback: Speculative Decoding With Nothing But Cache

**link**: https://arxiv.org/pdf/2511.21699.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 本文提出了Cacheback Decoding，这是一种无需训练且与模型无关的推测解码方法，它利用语言中的局部性来加速大型语言模型（LLM）的推理。Cacheback仅利用令牌n-gram的最近最少使用（LRU）缓存表来生成草稿序列。尽管设计极简，但Cacheback在可比较的方法中实现了最先进的性能，其简单性使其易于集成到现有系统中。此外，Cacheback还显示出快速适应新领域的潜力。  

---

### Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning

**link**: https://arxiv.org/pdf/2511.21743.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL, LLM Memory, Procedural Memory  
**abs**: 本文分析了语言模型在任务特定微调过程中的推理行为，并将推理标记（解决问题时生成的中间步骤）与人类工作记忆进行类比。借鉴认知科学理论，作者将训练动态与能力发展的四个阶段（无意识无能、有意识无能、有意识胜任、无意识胜任）相对应，发现推理标记长度随性能提升而增加，在有意识胜任阶段达到峰值，随后随着模型内化任务而减少。研究表明，训练后模型即使移除推理步骤仍能保持性能，说明推理起到了学习支架的作用。作者提出了跟踪这一轨迹的指标，认为推理行为对理解和优化推理模型训练具有重要价值。  

---

### Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks

**link**: https://arxiv.org/pdf/2511.21726.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 本文探讨了如何在大型语言模型（LLMs）中实现类人长期记忆，这对于解锁少样本泛化等更通用能力至关重要。现有记忆框架和基准侧重于寻找最优记忆压缩算法以提升需要回忆和推理任务的性能，但这些努力通过搜索适合特定基准的最佳提示和记忆架构，在压缩算法中植入了更多人类偏见，而非找到适用于其他数据分布的通用解决方案。另一方面，对未压缩信息的目标导向搜索可能表现出更优性能，因为压缩是有损的，预定义的压缩算法无法适应所有原始数据分布。本文提出SUMER（通过经验回放在未压缩记忆中搜索），这是一种具有可验证奖励的端到端强化学习智能体（RLVR），它学习使用搜索工具收集信息并回答目标问题。在用于长上下文对话理解的LoCoMo数据集上，配备Qwen2.5-7B-Instruct的SUMER学会使用搜索工具，优于所有其他有偏记忆压缩方法和全上下文基线，达到SOTA性能（比之前最佳结果提升43%）。研究表明，在当前长上下文记忆任务中，对原始数据应用简单搜索方法优于目标无关和有偏压缩算法，并主张开发更动态和自主可扩展的新范式与基准。  

---

### A Benchmark for Procedural Memory Retrieval in Language Agents

**link**: https://arxiv.org/pdf/2511.21730.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 当前AI智能体在熟悉环境中表现出色，但在面对具有未见过词汇的新任务时却表现急剧下降——这是程序性记忆系统的核心局限。本文提出首个分离程序性记忆检索与任务执行的基准，评估智能体是否能识别跨不同对象实例的功能等效程序。利用ALFWorld，构建了专家和LLM生成轨迹的双语料库，并使用系统分层查询评估六种检索方法。结果揭示了明显的泛化悬崖：基于嵌入的方法在熟悉语境中表现强劲，但在新语境中性能显著下降，而LLM生成的程序抽象展示了可靠的跨语境迁移能力。控制变量实验表明，尽管嵌入能捕捉部分词汇级抽象，但它们从根本上将程序视为无序词袋，丢弃了跨语境迁移所需的时间结构。语料库规模比表示增强带来更大收益，揭示了当前编码器的架构局限。该基准提供了区分真正程序理解与表面记忆的首个诊断框架，并为开发能够可靠泛化的检索系统提供工具。  

---

### Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking

**link**: https://arxiv.org/pdf/2511.22503.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 端到端口语对话状态跟踪（DST）因需处理语音输入和数据稀缺性而面临挑战。近期研究提出结合语音基础编码器和大型语言模型以缓解部分困难，该方法虽能实现多轮DST的最先进性能，但跨领域泛化能力较弱，且需为每个目标领域标注口语DST训练数据。鉴于文本DST数据易于获取，本文提出联合训练可用的口语DST数据与其他领域的书面文本数据，以实现跨领域泛化。实验表明，该方法无需目标领域的口语训练数据即可获得良好的跨领域DST性能。  

---

### Language-conditioned world model improves policy generalization by reading environmental descriptions

**link**: https://arxiv.org/pdf/2511.22904.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 为了在现实世界中与人类有效交互，智能体理解描述环境动态（即环境如何运作）的语言至关重要，而不仅仅是理解指定“做什么”的任务指令。理解这种动态描述语言对人机交互和智能体行为很重要。现有工作采用基于模型的方法：将语言融入世界模型，然后用于学习行为策略。然而，这些方法要么未能证明策略对未见游戏的泛化能力，要么依赖限制性假设（如推理时规划的延迟可容忍，或有专家演示）。本研究扩展了这一领域，专注于在去除这些假设的情况下，通过语言条件世界模型改进策略泛化。我们提出一种基于模型的强化学习方法，其中语言条件世界模型通过与环境交互进行训练，然后从中学习策略——无需规划或专家演示。我们的方法在DreamerV3基础上提出了语言感知编码器世界模型（LED-WM），其观察编码器使用注意力机制将语言描述明确接地到观察中的实体。实验表明，在MESSENGER等环境中，使用LED-WM训练的策略在面对新动态和语言描述的未见游戏时，泛化效果显著优于其他基线。  

---

### Real-Time Procedural Learning From Experience for AI Agents

**link**: https://arxiv.org/pdf/2511.22074.pdf  
**date**: 2025-12-02  
**keywords**: cs.AI  
**abs**: 生物智能的一个标志是通过试错实时学习如何做事，但大多数基于LLM的智能体缺乏部署后获取程序性知识的机制。本文提出了PRAXIS（Procedural Recall for Agents with eXperiences Indexed by State），这是一种轻量级的训练后学习机制，它存储动作的结果，并通过联合匹配过去情节的环境和内部状态与当前状态来检索这些结果。PRAXIS利用实时生成的检索到的状态-动作-结果示例来增强智能体的动作选择。在REAL网页浏览基准上的评估表明，PRAXIS提高了不同基础模型骨干的任务完成准确性、可靠性和成本效率，并显示出对类似环境中未见任务的初步泛化能力。这些结果表明，PRAXIS通过帮助AI智能体有效学习新程序，使其能够在快速演变的有状态环境中实际应用。  

---

### Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs

**link**: https://arxiv.org/pdf/2511.23271.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 精心设计的系统提示在引导LLM智能体行为方面起着关键作用，但其较长的长度带来了显著缺点，包括增加推理延迟、提高计算成本和减少有效上下文长度。这引发了一个问题：这些冗长的提示是否可以被大幅减少的标记数量替代，同时保留其对下游任务的行为影响。为实现这一目标，我们提出了一个轻量级的三阶段训练框架，用于学习特定于提示的单个行为等效标记([BE])。该框架首先训练[BE]通过重构来编码原始系统提示的自然语言内容，然后将提示的下游行为提炼到这个单个标记中。重要的是，我们的方法无需访问模型内部结构，无需辅助压缩模型，也无需标记的响应。在三个数据集上的实证评估表明，单个[BE]标记实现了高达3000倍的提示长度减少，同时保留了原始系统提示约98%的下游性能。这显著降低了推理成本，并几乎为用户输入留下了整个上下文窗口。  

---

### MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)

**link**: https://arxiv.org/pdf/2511.23281.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 大型语言模型智能体越来越多地用于自动化网络任务，如产品搜索、报价比较和结账。当前研究探索了这些智能体与网站交互的不同接口，包括传统的HTML浏览、基于预爬取内容的检索增强生成(RAG)、使用模型上下文协议(MCP)通过Web API进行通信以及通过NLWeb接口进行自然语言查询。然而，之前的工作尚未在单一受控环境中使用相同任务对这四种架构进行比较。  

---

### Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models

**link**: https://arxiv.org/pdf/2511.23319.pdf  
**date**: 2025-12-02  
**keywords**: cs.CL  
**abs**: 本研究探讨了构建“能够记忆的机器”的挑战，将长期记忆视为超长上下文建模问题。我们认为这需要三个关键特性：稀疏性、随机访问灵活性和长度泛化能力。为解决超长上下文建模问题，我们利用了分层稀疏注意力(HSA)，这是一种满足所有三个特性的新型注意力机制。我们将HSA集成到Transformer中以构建HSA-UltraLong，这是一个80亿参数的MoE模型，在超过8万亿标记上进行训练，并在具有域内和域外上下文长度的不同任务上进行了严格评估，以证明其处理超长上下文的能力。结果表明，我们的模型在域内长度上的性能与全注意力基线相当，同时在上下文长达1600万的大多数上下文内检索任务上达到了90%以上的准确率。本报告概述了我们的实验见解和开放问题，为未来超长上下文建模研究奠定了基础。  

---

### Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation

**link**: https://arxiv.org/pdf/2511.22311.pdf  
**date**: 2025-12-02  
**keywords**: cs.AI  
**abs**: 本文提出了一种受群体智能启发的去中心化基于代理的框架，用于从头设计蛋白质。多个大型语言模型（LLM）代理并行运作，每个代理被分配到特定的残基位置。这些代理通过整合设计目标、局部邻域相互作用以及来自先前迭代的记忆和反馈，迭代地提出上下文感知的突变。该框架展示了在蛋白质设计中，代理如何利用记忆机制进行迭代优化，为基于代理的系统中的记忆应用提供了实例。  

---

### SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning

**link**: https://arxiv.org/pdf/2511.22367.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 本文针对大型语言模型（LLM）的持续学习挑战，重新审视了重放机制，指出选择（排练什么）和整合（如何巩固新知识）是两个主要失败模式。为解决选择问题，提出了基于惊喜度的优先重放（SuRe）策略，该策略对最令人惊讶（高负对数似然）的序列进行排序和存储。SuRe在多任务场景下实现了最先进的性能，表明其通过重放机制优化了LLM的记忆保留与更新，与LLM Memory密切相关。  

---

### Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation

**link**: https://arxiv.org/pdf/2511.22235.pdf  
**date**: 2025-12-02  
**keywords**: cs.AI  
**abs**: 大型视觉语言模型（VLM）的快速发展极大地推动了GUI代理的研究。然而，GUI代理在处理长程任务时仍面临重大挑战：一是单代理模型难以平衡高级能力和低级执行能力，存在责任耦合和能力冲突问题；二是代理缺乏任务状态感知，导致长程任务中的进度丢失。为解决这些问题，本文提出了一种阶段性执行反馈强化学习算法，专注于训练高级调度模型。具体而言，设计并训练了两个代理：协调器（Coordinator）负责战略规划和任务分解；状态跟踪器（State Tracker）负责上下文压缩和信息管理，以维持任务的状态和连贯性。基于此构建了协调器-执行器-状态跟踪器（CES）多代理框架，可集成任何低级执行器模型，通过任务调度和状态管理协助执行器解决长程任务。长程任务基准测试表明，CES显著增强了系统的规划和状态管理能力，且训练后的高级调度模块具有通用性，可即插即用，能显著提升各种执行器的长程任务处理能力。  

---

### Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning

**link**: https://arxiv.org/pdf/2511.23262.pdf  
**date**: 2025-12-02  
**keywords**: cs.AI  
**abs**: 当前的视觉语言模型（VLM）虽具备较强的感知推理能力，但在测试时遇到新任务时往往难以高效适应。相比之下，人类利用具有记忆的元认知模型，在面对新挑战时通过元认知控制不断优化策略。为弥合这一差距，本文提出元认知测试时推理（MCTR）框架，该框架使模型能够通过元认知自我更新在测试时进行学习、适应和改进。受人类元认知双重结构的启发，MCTR包含元级和对象级VLM推理模块，每个模块都配备专用的记忆系统以实现分层自适应推理。具体而言，MCTR包括：（1）元推理模块，通过从测试时观察中发现并存储任务相关规则、环境模式和动作-结果关系（以自然语言描述形式），增量构建结构化记忆；（2）动作推理模块，通过动态检索和整合记忆中的知识，借助上下文感知感知和策略推理来确定最优动作。动作推理模块通过所提出的元认知测试时强化学习不断更新其策略，随着知识记忆的演变而适应。在45个Atari游戏（33个已见，12个未见）上的评估表明，MCTR展现出强大的测试时适应能力，在未见游戏上实现了9/12的top-1结果。  

---

### Solving Context Window Overflow in AI Agents

**link**: https://arxiv.org/pdf/2511.22729.pdf  
**date**: 2025-12-02  
**keywords**: Agent Memory, LLM Memory  
**abs**: 大型语言模型（LLMs）与外部工具的交互能力不断增强，使其能够获取训练数据之外的专业知识，这在化学和材料科学等动态、知识密集型领域至关重要。然而，大型工具输出可能导致LLMs的上下文窗口溢出，阻碍任务完成。现有的截断或总结方法无法保留完整输出，因此不适用于需要全部数据的工作流。本文提出一种方法，使LLMs能够处理和利用任意长度的工具响应而不丢失信息。通过将模型的交互从原始数据转向内存指针，该方法保留了工具功能，允许无缝集成到智能体工作流中，并减少了令牌使用量和执行时间。该方法在一个传统工作流无法执行的真实材料科学应用中得到验证，对比分析表明，所提方法消耗的令牌约为传统工作流的七分之一。  

---

### Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being

**link**: https://arxiv.org/pdf/2511.22737.pdf  
**date**: 2025-12-02  
**keywords**: Agent Memory, Personal Memory  
**abs**: 本文提出了一个详细的智能体人工智能（AI）模型，旨在帮助残疾人和神经多样性人群过上更健康、更规律的生活。该系统采用多层结构，包括应用与接口层、智能体层和数据源层，以提供适应性、透明和包容性的支持。核心是一个混合推理引擎，用于同步四个专用智能体：基于个性化营养的膳食规划智能体、基于自适应调度的提醒智能体、购物和烹饪时的交互式辅助食品指导智能体，以及持续摄入和生理跟踪的监测智能体。所有智能体通过名为“黑板/事件总线”的中央通信系统进行交互，支持自主交互和与多媒体用户界面的实时反馈循环。框架还包含隐私敏感数据源（如电子健康记录、营养数据库、可穿戴传感器和智能厨房物联网），并将其置于政策控制层，确保数据安全和 consent 合规。协作护理和临床医生仪表板支持共同监督，可解释人工智能（XAI）模块提供决策原因的简要说明，增强用户的责任感和依赖性。该智能体AI框架超越了传统辅助系统，在各个层面融入了包容性、个性化和可访问性，展示了多智能体推理、多模态界面和以人为本的设计的交叉，有望促进残疾人和神经多样性人群的自主性、健康和数字公平。  

---

### LFM2 Technical Report

**link**: https://arxiv.org/pdf/2511.23404.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 本文介绍了LFM2，这是一系列专为高效设备端部署和强大任务能力设计的Liquid Foundation Models。通过在边缘延迟和内存约束下的硬件在环架构搜索，获得了紧凑的混合骨干网络，结合门控短卷积和少量分组查询注意力块，在CPU上实现了比同等规模模型快2倍的预填充和解码速度。LFM2系列涵盖350M-8.3B参数，包括密集模型和混合专家变体，均具有32K上下文长度。其训练流程包括 tempered 解耦Top-K知识蒸馏目标、难度排序数据的课程学习以及三阶段后训练方案。预训练于10-12T tokens，LFM2模型在各种基准测试中取得了优异结果。此外，还构建了多模态和检索变体，如用于视觉语言任务的LFM2-VL、用于语音的LFM2-Audio和用于检索的LFM2-ColBERT。所有模型均以开放权重和部署包形式发布。  

---

### Adversarial Training for Process Reward Models

**link**: https://arxiv.org/pdf/2511.22888.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 过程奖励模型（PRMs）通过提供步骤级监督来增强大型语言模型（LLMs）的推理能力。然而，由于昂贵的手动步骤级标注以及静态训练数据对新错误的泛化能力较差，其广泛应用受到限制。本文提出对抗训练的PRMs（APRM），其中生成器（G）学习产生推理错误以欺骗PRM（R），同时R并发学习检测这些错误。这种交互为R生成逐渐困难的负样本，在无需手动步骤级标签的情况下提高了其鲁棒性和对新错误的泛化能力。在不同的数学推理基准上，APRM比最强的PRM基线平均提高了3.4个百分点的求解器准确率，并在分布外任务上实现了5.3个百分点的增益。  

---

### Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent

**link**: https://arxiv.org/pdf/2511.23436.pdf  
**date**: 2025-12-02  
**keywords**: cs.AI  
**abs**: 我们提出了SuperIntelliAgent，这是一个智能体学习框架，它将一个可训练的小型扩散模型（学习者）与一个冻结的大型语言模型（验证者）相结合，通过自监督交互实现持续的智能增长。与传统的监督微调不同，SuperIntelliAgent无需标注即可自主学习：学习者生成候选输出，验证者通过逐步推理对其进行评估，它们的交互产生用于直接偏好优化（DPO）的选择/拒绝对。这将每个输入转换为伪训练信号以实现持续改进。该框架集成了双尺度记忆：短期上下文内记忆，用于保留跨优化周期的推理轨迹；长期记忆，通过轻量级动态微调巩固已获取的知识。重放缓冲区保留显示可验证进展的样本，并将其作为辅助监督进行重放，在强化近期学习的同时形成自适应课程。SuperIntelliAgent与基础设施无关，可以插入现有的智能体框架，同时将普通的推理循环转变为终身优化过程。我们认为，将可训练的学习者与具备推理能力的验证者配对，构成了智能增长的最小可靠单元，因为配对反馈和部分历史重放能产生更丰富的学习课程和更强的偏好对齐。通过少量自动生成的DPO对，学习者在所有基准测试中均有改进，表明该机制为持续智能积累和实际部署提供了有前景的方向。  

---

### Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium

**link**: https://arxiv.org/pdf/2511.21882.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 当代自回归Transformer以开环方式运行，每个隐藏状态仅通过单次前向传递计算且永不修正，导致错误在序列中传播。本文指出这种开环瓶颈是造成长程推理、事实一致性和多步规划失败的根本架构限制。为此提出闭环预测原理，要求模型迭代优化 latent 表示直至达到自洽平衡后再生成token。实例化为Equilibrium Transformers (EqT)，通过平衡精化模块在 latent 空间中梯度下降最小化学习的能量函数，该函数无需外部监督即可强制双向预测一致性、情景记忆连贯性和输出置信度。理论上证明EqT执行近似MAP推理，建立线性收敛保证，并表明精化能在开环推理次优的困难实例上改进预测。框架统一了深度平衡模型、扩散语言模型和测试时训练。二进制奇偶校验任务实验显示挑战性序列平均提升3.28%，在标准Transformer接近随机性能的情况下增益达8.07%，验证了 deliberation 的收益随任务难度增加。  

---

### Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation

**link**: https://arxiv.org/pdf/2511.22030.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 基于脑电图（EEG）的驾驶员睡意检测系统因EEG信号的变异性需要繁琐的校准过程。本文提出一种新颖框架，利用在线测试时适应（TTA）方法动态调整模型以适应目标受试者分布。该方法更新批归一化（BN）层参数，同时保留预训练归一化统计量，并引入记忆库动态管理流式EEG片段，基于负能量分数和持续时间选择可靠样本。结合原型学习确保对分布偏移的鲁棒预测。在模拟驾驶数据集上的实验表明，该方法平均F1分数达81.73%，比最佳TTA基线提高11.73%，有效解决了跨受试者泛化问题。  

---

### Benchmarking In-context Experiential Learning Through Repeated Product Recommendations

**link**: https://arxiv.org/pdf/2511.22130.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 为了在不断变化的现实环境中可靠导航，智能体必须应对知识不完整的情况并通过经验调整行为。然而，当前评估主要集中在无歧义任务上，未衡量智能体通过积累经验进行自适应学习和推理的能力。本文以产品推荐为背景，举例说明了情境经验学习的必要性，其中智能体必须通过自然语言对话应对客户偏好和产品环境的变化。研究构建了一个经验学习和主动探索基准（BELA），该基准结合了（1）亚马逊的丰富现实产品，（2）多样化的用户角色以代表异质但潜在的偏好，以及（3）由角色驱动的LLM用户模拟器来创建丰富的交互轨迹。结果表明，当前前沿模型难以在多个回合中实现有意义的改进，强调了对具有强大情境学习能力的智能体系统的需求。  

---

### LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system

**link**: https://arxiv.org/pdf/2511.22598.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLMs）如ChatGPT o1、ChatGPT o3和DeepSeek R1在解决复杂问题方面展现出巨大潜力。然而，当前的LLM评估基准仅限于单步交互。现有的一些序列决策环境（如TextStarCraftII和LLM-PySC2）过于复杂，完成一个游戏需要数小时的交互。本文介绍了LLM-Cave，一个用于LLM推理和决策系统的基准和轻量级环境。该环境是符号主义时代的经典实例，人工智能通过利用部分可观测的状态信息推理附近的危险，使智能体能够探索环境并避免潜在损失。在实验中，我们评估了主流大型语言模型（如GPT-4o-mini、o1-mini和DeepSeek-R1）的序列推理能力、决策性能和计算效率。实验表明，尽管Deepseek-R1在复杂推理任务上取得了最高的成功率，但像4o-mini这样的小型模型通过采用Chain of Speculation和Planner-Critic策略，在挑战任务上显著缩小了性能差距，不过代价是计算效率降低。这表明结构化的多步推理结合基于LLM的反馈机制可以显著增强LLM的决策能力，为改进较弱模型的推理提供了一个有前景的方向，并提出了一个新的以推理为中心的LLM评估基准。我们的代码已开源。  

---

### Distributed Dynamic Associative Memory via Online Convex Optimization

**link**: https://arxiv.org/pdf/2511.23347.pdf  
**date**: 2025-12-02  
**keywords**: cs.LG  
**abs**: 联想记忆（AM）支持线索-响应回忆，最近被认为是现代神经架构（如Transformer）的关键机制。本文提出分布式动态联想记忆（DDAM）概念，将经典AM扩展到多智能体和时变数据流场景。在DDAM中，每个智能体维护本地AM，不仅存储自身关联，还基于指定兴趣矩阵选择性记忆其他智能体信息。为解决此问题，提出一种基于树的分布式在线梯度下降算法DDAM-TOGD，使智能体通过指定路由树进行交互通信，实时更新记忆。理论上证明了DDAM-TOGD在静态环境下的次线性静态遗憾和非静态环境下路径长度相关的动态遗憾界，揭示了通信延迟和网络结构对性能的影响。基于遗憾分析，进一步引入组合树设计策略优化路由树以减少通信延迟，提升遗憾界。数值实验表明，与共识分布式优化等在线学习基线相比，DDAM-TOGD框架具有更高的准确性和鲁棒性，证实了其在动态分布式环境中的优势。