### 1 RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning

**link**: https://arxiv.org/pdf/2512.04457.pdf  
**date**: 2025-12-05  
**keywords**: cs.CL  
**abs**: 该研究针对从大型语言模型（LLMs）中移除特定数据影响的挑战，提出了RapidUn框架。传统方法如重新训练成本高昂，而现有近似遗忘方法常不稳定，尤其当遗忘集小或不平衡时。RapidUn通过影响驱动和参数高效的遗忘框架，首先通过快速估计模块计算每个样本的影响，然后将这些分数映射为自适应更新权重，指导选择性参数更新——在保留一般知识的同时遗忘有害行为。在Mistral-7B和Llama-3-8B模型上的实验表明，RapidUn比完全重新训练效率高100倍，并在分布内和分布外遗忘任务上持续优于Fisher、GA和LoReUn方法。该研究确立了影响引导的参数重加权作为LLM遗忘的可扩展且可解释范式，与LLM Memory（模型记忆管理）直接相关。  

---

### 2 RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories

**link**: https://arxiv.org/pdf/2512.04144.pdf  
**date**: 2025-12-05  
**keywords**: cs.AI  
**abs**: 针对语言模型的定向干预（如遗忘、去偏或模型编辑）是改进模型行为和保持知识更新的核心方法。尽管这些干预旨在修改模型中的特定信息（例如，删除病毒学内容），但其效果往往会传播到相关但非预期的领域（例如，过敏）；这些副作用通常被称为涟漪效应。在这项工作中，我们提出了RippleBench-Maker，这是一种用于生成问答数据集的自动工具，允许在任何模型编辑任务中测量涟漪效应。RippleBench-Maker基于维基百科的RAG管道（WikiRAG）生成与目标概念（例如，被遗忘的知识）具有不同语义距离的多项选择题。使用此框架，我们构建了RippleBench-Bio，这是一个源自WMDP（大规模杀伤性武器论文）数据集的基准，WMDP是一个常见的遗忘基准。我们评估了八种最先进的遗忘方法，发现所有方法在与被遗忘知识越来越远的主题上都表现出显著的准确性下降，每种方法都有不同的传播特征。为了支持持续研究，我们发布了用于动态涟漪评估的代码库以及基准RippleBench-Bio。  

---

### 3 MemLoRA: Distilling Expert Adapters for On-Device Memory Systems

**link**: https://arxiv.org/pdf/2512.04763.pdf  
**date**: 2025-12-05  
**keywords**: cs.LG  
**abs**: 记忆增强型大型语言模型（LLMs）通过存储相关记忆并将其整合到上下文中，在长时间对话中展现出显著的一致性。这种基于记忆的个性化在设备端设置中也至关重要，因为它允许用户保留对话和数据隐私。然而，记忆增强系统通常依赖于成本过高的LLMs，难以在本地设备部署。尽管小型语言模型（SLMs）比LLMs更适合设备端推理，但它们无法达到足够的性能。此外，这些基于LLM的系统缺乏原生视觉能力，限制了其在多模态场景中的适用性。本文提出：（i）MemLoRA，一种新型记忆系统，通过为SLMs配备专门的记忆适配器实现本地部署；（ii）其视觉扩展MemLoRA-V，将小型视觉语言模型（SVLMs）集成到记忆系统中，实现原生视觉理解。遵循知识蒸馏原则，每个适配器针对特定记忆操作（知识提取、记忆更新和记忆增强生成）单独训练。配备记忆适配器后，小型模型能够在不依赖云服务的情况下进行准确的设备端记忆操作。在纯文本操作上，MemLoRA优于10倍大的基线模型（如Gemma2-27B），并在LoCoMo基准测试中达到60倍大模型（如GPT-OSS-120B）相当的性能。为评估视觉理解操作，我们扩展LoCoMo，加入需要直接视觉推理的挑战性视觉问答任务。在该任务上，集成VLM的MemLoRA-V相比基于标题的方法显著提升（准确率81.3 vs. 23.7），同时在文本任务中保持强劲性能，证明了我们的方法在多模态场景中的有效性。  

---

### 4 Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates

**link**: https://arxiv.org/pdf/2512.04844.pdf  
**date**: 2025-12-05  
**keywords**: cs.CL  
**abs**: 针对LLM在目标语言适应中面临的灾难性遗忘问题，本文提出源屏蔽更新（SSU）策略。该方法在低资源场景下（仅使用无标签目标语言数据），通过识别并保护对源语言能力至关重要的参数，减少适应过程中源知识的丢失。实验表明，SSU能将7B和13B模型的源任务性能下降分别从20.3%和22.3%降至3.4%和2.8%，同时保持目标语言适应性能与全参数微调相当。  

---

### 5 SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs

**link**: https://arxiv.org/pdf/2512.04868.pdf  
**date**: 2025-12-05  
**keywords**: cs.CL  
**abs**: 提出SEAL框架用于知识图谱对话问答（KBCQA），解决指代消解、上下文依赖和复杂逻辑推理等挑战。该框架分为两阶段：第一阶段由LLM提取查询的S-expression核心并通过代理校准模块优化；第二阶段基于问题类型预测和占位符实例化完成可执行逻辑形式构建。关键创新在于整合本地/全局记忆与反射模块，使系统能从对话历史和执行反馈中持续适应，无需显式重训练。在SPICE基准测试中，SEAL在多跳推理、比较和聚合任务上实现了最先进性能，同时提升了结构准确性和计算效率。  

---

### 6 EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion

**link**: https://arxiv.org/pdf/2512.04545.pdf  
**date**: 2025-12-05  
**keywords**: cs.CL  
**abs**: 部署后调整大型语言模型（LLMs）的过时知识仍是一项重大挑战，这推动了知识编辑的发展，旨在无需从头重新训练即可准确高效地修改模型的内部（参数化）知识。然而，现有方法存在两个局限：一是依赖结构化三元组，与LLM预训练的自由文本性质不一致，无法捕捉事实间的细微关系；二是通常支持一次性知识更新，对顺序或终身编辑问题的研究相对有限。为解决这些差距，本文提出了终身自由文本知识编辑（LF-Edit）新任务，使模型能够整合自然语言表达的更新并支持随时间的持续编辑。尽管前景广阔，LF-Edit面临整合新知识同时减轻先前信息遗忘的双重挑战。为促进该任务的研究，本文构建了大规模基准数据集Multi-Rank Lifelong Free-text Editing Benchmark（MRLF-Bench），包含16,835个自由文本编辑请求，并设计了涵盖记忆、理解、受限理解和推理四个层次的认知启发式多 rank 评估框架。为应对LF-Edit的固有挑战，本文引入EvoEdit新方法，通过潜在扰动增强促进知识注入，并通过知识驱动的参数融合保留先前信息。实验结果表明，EvoEdit在提出的LF-Edit任务上显著优于现有知识编辑方法。  

---

### 7 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing

**link**: https://arxiv.org/pdf/2512.04753.pdf  
**date**: 2025-12-05  
**keywords**: cs.CL  
**abs**: 知识编辑旨在无需完全重新训练即可更新大型语言模型（LLMs）中的特定事实。先前的研究试图调整LLMs的知识层，证明了进行选择性编辑的有效性。然而，它们在受控的教师强制评估中的性能与在终身学习场景中的实际有效性之间存在显著差距，这极大地限制了其实际适用性。本研究的实证分析揭示了与该差距相关的两个反复出现的问题：（1）大多数传统方法导致编辑后的模型过度拟合新事实，从而降低预训练能力；（2）严重缺乏知识巩固阶段，使得新事实在自回归生成下未能充分整合到LLMs的推理时行为中，导致参数知识与实际生成行为不匹配。为此，本文提出了“先编辑后巩固”（Edit-then-Consolidate）这一新颖的知识编辑范式，旨在弥合理论知识编辑方法与其实际适用性之间的差距。具体而言，（1）我们的框架通过目标近端监督微调（TPSFT）减轻过度拟合，该方法通过信任区域目标定位编辑以限制策略漂移；（2）然后，使用组相对策略优化（GRPO）的巩固阶段通过在综合奖励信号下优化轨迹级行为，将编辑的知识与基于思维链（CoT）的推理策略对齐。大量实验表明，我们的框架在实际评估中一致提高了编辑的可靠性和泛化能力，同时更好地保留了局部性和预训练能力。  

---

### 8 From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research

**link**: https://arxiv.org/pdf/2512.04854.pdf  
**date**: 2025-12-05  
**keywords**: cs.AI  
**abs**: 本文探讨了生物医学研究中人工智能系统的基准测试实践。当前基准测试仅评估孤立的组件能力（如数据分析质量、假设有效性和实验方案设计），但真实的研究协作需要跨多个会话的集成工作流，包括上下文记忆、自适应对话和约束传播。研究结果表明，现有基准测试可能不足以评估AI作为研究合作伙伴的有效性，并提出了一个面向过程的评估框架，该框架涵盖对话质量、工作流编排、会话连续性和研究人员体验这四个关键维度，这些维度对于将AI评估为研究协作者而非孤立任务执行者至关重要。  

---

### 9 Evaluating Long-Context Reasoning in LLM-Based WebAgents

**link**: https://arxiv.org/pdf/2512.04307.pdf  
**date**: 2025-12-05  
**keywords**: cs.LG  
**abs**: 随着基于大型语言模型（LLM）的智能体日益融入日常数字交互，它们跨长期交互历史进行推理的能力对于提供个性化和情境感知的辅助至关重要。然而，这些智能体在长上下文场景中的性能，特别是在现实网络环境中执行操作的WebAgents，仍未得到充分探索。本文提出了一个基准，通过顺序依赖的子任务来评估WebAgents的长上下文推理能力，这些子任务需要从扩展的交互历史中检索和应用信息。我们开发了一个新颖的评估框架，通过在依赖子任务之间注入无关的任务轨迹来模拟多会话用户交互，创建了从25,000到150,000个令牌的上下文。通过对Claude-3.7、GPT-4.1、Llama 4和o4-mini这四种流行模型的广泛评估，我们观察到随着上下文长度的增加，性能急剧下降，成功率从基线条件下的40-50%降至长上下文场景中的不到10%。我们的详细错误分析表明，智能体主要因陷入循环和忘记原始任务目标而失败。我们进一步提出了一种隐式RAG方法，通过生成任务相关摘要提供了适度改进，但长上下文推理的基本限制仍然存在。这些发现凸显了在现实、长期用户交互场景中部署WebAgents的关键挑战，并为开发能够在扩展上下文中维持连贯任务执行的更强大智能体架构提供了见解。