### 1 Deep Research: A Systematic Survey

**link**: https://arxiv.org/pdf/2512.02038.pdf  
**date**: 2025-12-03  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）已从文本生成器快速演变为强大的问题解决者。然而，许多开放任务需要批判性思维、多源和可验证的输出，这超出了单步提示或标准检索增强生成的能力。最近，众多研究探索了深度研究（DR），旨在将LLMs的推理能力与外部工具（如搜索引擎）相结合，从而使LLMs能够作为研究代理完成复杂的开放式任务。本综述全面系统地概述了深度研究系统，包括清晰的路线图、基础组件、实际实现技术、重要挑战和未来方向。具体而言，主要贡献如下：（i）形式化三阶段路线图，并区分深度研究与相关范式；（ii）介绍四个关键组件：查询规划、信息获取、内存管理和答案生成，每个组件都配有细分子分类；（iii）总结优化技术，包括提示工程、监督微调与智能体强化学习；（iv）整合评估标准和开放挑战，旨在指导和促进未来发展。随着深度研究领域的快速发展，本综述将持续更新以反映该领域的最新进展。

---

### 2 Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering

**link**: https://arxiv.org/pdf/2512.02363.pdf  
**date**: 2025-12-03  
**keywords**: LLM Memory  
**abs**: 领域特定的问答系统（如医疗政策和政府福利等敏感领域）在整合异构知识源时面临准确性和安全性的挑战。现有大型语言模型在此类场景中常存在事实一致性和上下文对齐问题。为此，本文提出知识感知推理与记忆增强适应（KARMA）框架，旨在提升护理场景下的问答性能。KARMA包含双编码器架构以融合结构化和非结构化知识源，一个门控记忆单元用于动态调节外部知识整合，以及一个安全感知可控解码器，通过安全分类和引导生成技术减轻不安全输出。在专有问答数据集上的大量实验表明，KARMA在答案质量和安全性方面均优于强基线。该研究为构建服务场景下可信赖且自适应的问答系统提供了全面解决方案。

---

### 3 When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents

**link**: https://arxiv.org/pdf/2512.02445.pdf  
**date**: 2025-12-03  
**keywords**: cs.LG  
**abs**: 解决复杂或长期任务通常需要大型语言模型（LLMs）使用外部工具并在更长的上下文窗口上运行。新的LLMs支持更长的上下文窗口和工具调用能力，但先前的研究主要关注LLMs在长上下文提示上的评估，而从能力和安全角度对智能体设置的探索相对不足。本研究发现，LLM智能体可能对上下文的长度、类型和位置敏感，在任务性能和拒绝执行有害请求方面表现出意外且不一致的变化。具有1M-2M token上下文窗口的模型在100K token时已显示出严重的性能下降，良性和有害任务的性能下降均超过50%。拒绝率也出现不可预测的变化：GPT-4.1-nano从约5%增加到约40%，而Grok 4 Fast从约80%下降到约10%（在200K token时）。研究揭示了长上下文智能体存在的潜在安全问题，并对当前LLM智能体安全评估指标和范式提出了质疑，结果显示智能体在能力和安全性能上与先前LLM的评估存在显著差异。

---

### 4 Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization

**link**: https://arxiv.org/pdf/2512.02665.pdf  
**date**: 2025-12-03  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）目前被用于如谷歌AI概览等场景，对多篇长文档进行总结。然而，尚不清楚它们是否对所有输入赋予同等权重。本文以堕胎相关新闻为研究对象，构建了40组支持-中立-反对的文章三元组，将每组三元组排列为六种输入顺序，并提示Gemini 2.5 Flash生成中立概览。我们使用ROUGE-L（词汇重叠）、BERTScore（语义相似度）和SummaC（事实一致性）对每个摘要与其源文章进行评估。单因素方差分析显示，BERTScore在所有立场上均存在显著的首因效应，表明摘要与首次看到的文章在语义上更一致。成对比较进一步显示，位置1与位置2和3存在显著差异，而后两者之间无显著差异，证实了对第一篇文档的选择性偏好。研究结果为依赖LLM生成概览的应用以及智能体AI系统带来了风险，其中涉及LLM的步骤可能会不成比例地影响下游行动。

---

### 5 STRIDE: A Systematic Framework for Selecting AI Modalities - Agentic AI, AI Assistants, or LLM Calls

**link**: https://arxiv.org/pdf/2512.02228.pdf  
**date**: 2025-12-03  
**keywords**: cs.AI  
**abs**: 快速从无状态大型语言模型（LLMs）向自主、目标驱动的智能体转变引发了一个核心问题：智能体人工智能（agentic AI）何时真正必要？尽管智能体能够实现多步推理、持久记忆（persistent memory）和工具编排，但不加区分地部署它们会导致更高的成本、复杂性和风险。本文提出了STRIDE框架，旨在系统地选择AI模态（智能体AI、AI助手或LLM调用），探讨了智能体所具备的持久记忆等关键能力在不同应用场景下的必要性与潜在权衡。

---

### 6 IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai

**link**: https://arxiv.org/pdf/2512.02605.pdf  
**date**: 2025-12-03  
**keywords**: cs.AI  
**abs**: 本技术白皮书介绍了交互式智能体调用树（IACT），这是一种旨在解决静态、硬编码智能体工作流局限性的计算模型。与需要预定义图或专门编程的传统系统不同，IACT作为纯粹由用户对话驱动的通用自主系统运行。给定一个高级目标，该系统能自主生成动态、递归的智能体拓扑结构，逐步适应问题结构。为减轻单向函数调用中固有的错误传播，IACT通过将刚性调用替换为双向、有状态对话引入了交互冗余。这种机制支持运行时错误纠正和歧义解决。文中描述了该模型在生产部署背后的架构、设计原则和实践经验。该模型中的有状态对话机制涉及智能体状态的维护与更新，与智能体记忆（Agent Memory）相关。

---

### 7 StockMem: An Event-Reflection Memory Framework for Stock Forecasting

**link**: https://arxiv.org/pdf/2512.02720.pdf  
**date**: 2025-12-03  
**keywords**: cs.AI  
**abs**: 股票价格预测由于市场波动性及其对实时事件的敏感性而具有挑战性。尽管大型语言模型（LLMs）为基于文本的预测提供了新途径，但它们在金融领域的应用受到嘈杂新闻数据和文本中缺乏明确答案的阻碍。通用内存架构难以识别价格变动的关键驱动因素。为解决此问题，我们提出了StockMem，一种事件-反思双层内存框架。它将新闻结构化为事件，并从两个维度进行挖掘：横向整合集成每日事件，纵向跟踪捕捉事件演变以提取反映市场预期差异的增量信息。这构建了一个时序事件知识库。通过分析事件-价格动态，该框架进一步形成了因果经验的反思知识库。在预测时，它检索类似的历史场景，并结合当前事件、增量数据和过去经验进行推理。实验表明，StockMem优于现有的内存架构，并通过追踪影响价格的信息链提供更优的、可解释的推理，增强了金融预测中的决策透明度。

---

### 8 Retrieval-Augmented Memory for Online Learning

**link**: https://arxiv.org/pdf/2512.02333.pdf  
**date**: 2025-12-03  
**keywords**: cs.LG  
**abs**: 检索增强模型将参数化预测器与非参数化记忆相结合，但其在具有概念漂移的流式监督学习中的应用尚未得到充分研究。本文研究了非平稳环境下的在线分类问题，并提出了用于在线学习的检索增强记忆（RAM-OL），这是一种简单的随机梯度下降扩展方法，它维护一个小型的过去样本缓冲区。在每个时间步，RAM-OL在隐藏表示空间中检索当前输入的几个最近邻，并结合当前样本和检索到的邻居对模型进行联合更新。作者比较了一种朴素的重放变体和一种门控重放变体，后者使用时间窗口、相似性阈值和梯度重加权来约束邻居，以平衡相关过去数据的快速重用与对过时状态的鲁棒性。从理论角度，作者在有界漂移模型下解释了RAM-OL，并讨论了当模式随时间重复出现时，检索如何降低适应成本并改善遗憾常数。实证上，作者在简单的在线多层感知器上实例化了RAM-OL，并在三个来自电价、电力负荷和航班延误数据的真实世界数据流上进行了评估。在强周期性漂移流上，RAM-OL将预序准确率提高了约7个百分点，并大大降低了随机种子之间的方差，而在噪声较大的航班流上，门控变体与纯在线基线非常接近。这些结果表明，检索增强记忆是在线学习中应对概念漂移的一种实用且稳健的工具。

---

### 9 SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification

**link**: https://arxiv.org/pdf/2512.02337.pdf  
**date**: 2025-12-03  
**keywords**: cs.LG  
**abs**: 长上下文生成（如代码生成、深度推理和长文档理解）是大型语言模型（LLMs）的关键能力。推测解码是加速生成的有效方法，但其验证过程在长上下文场景下成为瓶颈。本文提出SpecPV，一种自推测解码方法，通过使用部分键值状态（KV）进行快速验证，并定期应用完全验证以消除累积误差。该方法在多个长上下文基准和模型（如LLaMA-3.1-8B-Instruct和Qwen3系列）上进行了验证，实验结果显示SpecPV相比标准自回归解码实现了高达6倍的解码加速，同时性能下降轻微。

---

### 10 In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs

**link**: https://arxiv.org/pdf/2512.02543.pdf  
**date**: 2025-12-03  
**keywords**: cs.LG  
**abs**: 当前世界上有大量关于如何使用新型LLM智能体的想法，开发者希望快速原型化和测试新的智能体设计。然而，使用高容量LLM大规模执行智能体会产生高昂的推理成本。本文提出了一种简单的方法来降低LLM智能体的推理成本，无需承担与LLM微调相关的开发摩擦成本（长训练周期、优化超参数调整循环）或手动提示工程（费力的试错）。最重要的是，我们引入了“上下文内蒸馏”，将知识蒸馏（训练低成本学生模型模仿高成本教师模型）的思想适应于上下文学习场景。我们的方法在每个智能体步骤中检索相关的教师演示，并将其作为上下文示例提供给学生模型，使学生能够即时模仿教师行为。我们将上下文内蒸馏与已有的“自一致性级联”思想相结合，以判断何时信任学生模型。这种自适应策略实现了模型专业化的成本优势，同时保持了使用冻结模型的高效开发。在多步骤具身推理基准ALFWorld上，我们的方法以2.5倍的低成本达到了教师级别的准确率，将每 episode 成本从0.059美元降至0.024美元。前期演示成本仅需843个 episode 即可摊销，在部署规模（100万 episode）下累计节省超过34,900美元。在需要多步骤API工作流的复杂智能体基准AppWorld上，我们在同等准确率下实现了2倍的成本降低。通过在保持冻结模型快速实验周期的同时降低运营成本，我们的方法使先进的智能体系统在更广泛的应用中具有经济可行性。