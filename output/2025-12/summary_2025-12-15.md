### 1 Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference

**link**: https://arxiv.org/pdf/2512.11221.pdf  
**date**: 2025-12-15  
**keywords**: cs.LG  
**abs**: 本文提出了一种名为ASR-KF-EGR的无需训练的推理时框架，用于高效大型语言模型生成。该方法引入可逆软冻结机制，可暂时暂停滑动注意力窗口内低重要性令牌的键值（KV）更新。与永久丢弃上下文的驱逐式方法不同，ASR-KF-EGR将所有令牌保存在GPU外存储中并按需恢复。框架还扩展了亚线性冻结调度，冻结持续时间随重复低重要性检测呈亚线性增长，防止过度压缩。在LLaMA-3 8B上的初步实验表明，该方法能减少55-67%的活跃KV缓存大小，同时保持生成质量并通过针在草堆检索测试。此方法与架构无关，无需微调，为长上下文LLM的内存受限部署提供了实用解决方案。

---

### 2 Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents

**link**: https://arxiv.org/pdf/2512.11584.pdf  
**date**: 2025-12-15  
**keywords**: cs.LG  
**abs**: 当前的视觉-语言-动作（VLA）模型泛化能力较差，尤其是在任务需要新的技能或对象组合时。我们提出原子动作切片（AAS），这是一种与规划器对齐的方法，将长程演示分解为短的、类型化的原子动作，便于规划器使用和策略学习。利用LIBERO演示，AAS生成了包含2,124个原子片段的验证数据集，这些片段标注了动作类型、时间跨度和置信度。更强的分段器（Gemini 2.5 Pro）与规划器定义的计划紧密匹配，并且在关键帧抖动下保持稳健，而较小的模型在多对象任务上表现较差。在我们的原子数据集上微调CLIP-RT+后，LIBERO-Goal的任务成功率从94.2%提升至95.3%，LIBERO-Long从83.8%提升至88.8%。我们在HuggingFace上公开发布了GATE-VLAP数据集。

---

### 3 Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes

**link**: https://arxiv.org/pdf/2512.11463.pdf  
**date**: 2025-12-15  
**keywords**: cs.AI  
**abs**: 本文介绍了Motif-2-12.7B-Reasoning，这是一个127亿参数的语言模型，旨在弥合开放权重系统与专有前沿模型在复杂推理和长上下文理解方面的差距。为解决推理适应中常见的模型崩溃和训练不稳定性挑战，作者提出了一个全面的、可重现的训练方案，涵盖系统、数据和算法优化。该方法结合了使用混合并行和内核级优化的64K令牌上下文内存高效基础设施，以及两阶段监督微调(SFT)课程，通过经过验证的、对齐的合成数据减轻分布不匹配。此外，还详细介绍了一个强大的强化学习微调(RLFT)管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。实验结果表明，Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中实现了与参数数量显著更大的模型相当的性能，为社区提供了一个有竞争力的开放模型和在实际计算约束下扩展推理能力的实用蓝图。

---

### 4 Bridging Streaming Continual Learning via In-Context Large Tabular Models

**link**: https://arxiv.org/pdf/2512.11668.pdf  
**date**: 2025-12-15  
**keywords**: cs.LG  
**abs**: 在流数据场景中，模型必须持续学习并适应概念漂移，同时不遗忘先前获取的知识。现有研究中，持续学习（CL）关注长期保留和减轻灾难性遗忘，而流学习（SL）强调快速适应高频数据流，但两者存在孤立性。本文认为大型上下文表格模型（LTMs）为流持续学习（SCL）提供了自然桥梁，提出通过动态总结数据流为紧凑草图供LTMs使用，平衡可塑性（当前分布性能）和稳定性（保留过去知识），并通过分布匹配和分布压缩控制记忆大小，涉及记忆多样化和检索机制。

---

### 5 When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents

**link**: https://arxiv.org/pdf/2512.11277.pdf  
**date**: 2025-12-15  
**keywords**: cs.CL  
**abs**: 监督微调（SFT）难以在数据分布变化时泛化，而推理模型（如o1和R1）通过强化学习（RL）从任务结果中直接学习推理策略。本文提出让大型语言模型（LLMs）生成推理步骤，指导工具调用和最终答案生成的 pipeline，采用Group Relative Policy Optimization（GRPO），基于工具准确性和答案正确性设计奖励，使模型迭代优化推理和动作。该方法提升了推理质量和工具调用精度，相对SFT模型有1.5%改进，体现了通过RL统一推理与动作学习对会话代理记忆与决策的增强作用。

---

### 6 Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture

**link**: https://arxiv.org/pdf/2512.11303.pdf  
**date**: 2025-12-15  
**keywords**: cs.CL  
**abs**: 大型语言模型代理在适应新任务时面临工具可用性有限和经验重用不足的根本挑战。现有方法要么依赖覆盖范围有限的预定义工具，要么在不利用过往经验的情况下从零开始构建工具，导致探索效率低下和性能欠佳。本文介绍了SMITH（共享内存集成工具中心），这是一种统一的认知架构，通过分层内存组织将动态工具创建与跨任务经验共享无缝集成。SMITH将代理内存组织为程序、语义和情景组件，能够在扩展能力的同时保留成功的执行模式。该方法将工具创建形式化为受控沙箱环境中的迭代代码生成，并通过具有语义相似性匹配的情景内存检索实现经验共享。此外，还提出了基于代理集成难度重新估计的课程学习策略。在GAIA基准上的广泛实验证明了SMITH的有效性，实现了81.8%的Pass@1准确率，优于包括Alita（75.2%）和Memento（70.9%）在内的最先进基线。这项工作为构建真正自适应的代理奠定了基础，这些代理通过工具创建和经验积累的原则性集成不断发展其能力。

---

### 7 Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning

**link**: https://arxiv.org/pdf/2512.11485.pdf  
**date**: 2025-12-15  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）通过梯度微调（计算量大、易灾难性遗忘）或上下文学习（ICL：鲁棒性低、错误学习能力差）适应任务。为解决此问题，我们引入错误笔记本学习（MNL），这是一种无训练框架，具有抽象错误模式的持久知识库。与先前的实例/单轨迹记忆方法不同，MNL使用批量错误抽象：从多个失败中提取可泛化的指导，将见解存储在动态笔记本中，并通过保留验证仅保留优于基线的指导（确保单调改进）。实验表明，MNL几乎与监督微调性能相当（在GSM8K上为93.9% vs 94.3%），并且在GSM8K、Spider、AIME和KaggleDBQA上优于无训练替代方案。在KaggleDBQA（Qwen3-8B）上，MNL达到28%的准确率（相对增益47%），优于Memento（15.1%）和无训练GRPO（22.1%），证明其是复杂推理任务的强大无训练替代方案。