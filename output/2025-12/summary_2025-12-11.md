### 1 Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces

**link**: https://arxiv.org/pdf/2512.08960.pdf  
**date**: 2025-12-11  
**keywords**: cs.LG  
**abs**: 低秩适应（LoRA）虽能实现高效的持续学习，但常因任务间的破坏性干扰而遭受灾难性遗忘。研究分析表明，这种性能下降主要由对抗性方向更新导致，即新任务梯度与历史权重轨迹直接对立。为解决此问题，本文提出PS-LoRA（参数稳定性LoRA）框架，旨在通过在优化子空间内对齐更新来解决冲突。该方法采用双重正则化目标，惩罚冲突方向并约束幅度偏差，以确保与先验知识的一致性。此外，还实现了一种基于幅度的合并策略，将顺序适配器整合为稳健表示，无需重新训练。在NLP和视觉基准测试上的实验表明，PS-LoRA通过保持学习表示的稳定性并高效适应新领域，优于最先进的方法。

---

### 2 Architectures for Building Agentic AI

**link**: https://arxiv.org/pdf/2512.09458.pdf  
**date**: 2025-12-11  
**keywords**: cs.AI  
**abs**: 本章认为智能体和生成式AI的可靠性主要是一种架构属性。我们将智能体系统定义为在闭环中运行的、有目标导向的、使用工具的决策者，并展示了可靠性如何从有原则的组件化（目标管理器、规划器、工具路由器、执行器、内存、验证器、安全监控器、遥测）、规范的接口（模式约束、经过验证的、最小权限的工具调用）以及明确的控制和保证循环中产生。基于经典基础，我们提出了一个实用的分类法——使用工具的智能体、增强内存的智能体、规划和自我改进的智能体、多智能体系统以及具身或网络智能体，并分析了每种模式如何重塑可靠性范围和故障模式。我们提炼了关于类型化模式、幂等性、权限、事务语义、内存来源和卫生、运行时治理（预算、终止条件）以及“先模拟后执行”保障措施的设计指导。

---

### 3 An End-to-end Planning Framework with Agentic LLMs and PDDL

**link**: https://arxiv.org/pdf/2512.09629.pdf  
**date**: 2025-12-11  
**keywords**: cs.AI  
**abs**: 本文提出了一个带验证器支持的端到端规划框架。协调器接收自然语言编写的人类规范，并将其转换为PDDL（规划域定义语言）模型，其中领域和问题由子模块（智能体）迭代优化，以解决常见的规划需求，如时间约束和最优性，以及人类规范中可能存在的模糊性和矛盾。经过验证的领域和问题随后被传递给外部规划引擎以生成计划。协调器和智能体由大型语言模型（LLMs）提供支持，在过程的任何阶段都不需要人工干预。最后，一个模块将最终计划转换回自然语言，以提高人类可读性，同时保持每个步骤的正确性。我们在各种领域和任务中展示了我们框架的灵活性和有效性，包括Google NaturalPlan基准测试和PlanBench，以及像积木世界（Blocksworld）和河内塔（Tower of Hanoi）这样的规划问题（已知LLMs即使在小实例上也难以处理）。我们的框架可以与任何PDDL规划引擎和验证器集成（如我们测试过的Fast Downward、LPG、POPF、VAL和uVAL），代表了LLMs辅助端到端规划的重要一步。

---

### 4 Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning

**link**: https://arxiv.org/pdf/2512.09706.pdf  
**date**: 2025-12-11  
**keywords**: cs.LG  
**abs**: 智能体人工智能范式正从设计复杂工作流转向训练后原生模型。然而，现有智能体通常局限于静态、预定义的动作空间（如仅使用API、GUI事件或机器人命令），这限制了它们在动态环境中的适应性，因为环境中交互的最优粒度会随上下文变化。为解决这一差距，本文提出CrossAgent，一种统一的智能体模型，能够掌握异构动作空间并自主选择轨迹中每一步的最有效接口。作者引入了综合训练流程，将冷启动监督微调与多轮组相对策略优化（GRPO）算法相结合。这种方法使智能体能够学习自适应动作切换——在高层效率与低层精度之间取得平衡——无需人工指定规则。在开放世界《我的世界》环境中超过800项任务的广泛实验表明，CrossAgent实现了最先进的性能。通过动态利用不同动作空间的优势，该模型显著优于固定动作基线，在长程推理中表现出更强的泛化能力和效率。

---

### 5 Mixture of Lookup Key-Value Experts

**link**: https://arxiv.org/pdf/2512.09723.pdf  
**date**: 2025-12-11  
**keywords**: cs.LG  
**abs**: 近期研究已开发出多种适用于终端用户设备推理的大型语言模型（LLM）架构，例如混合查找专家模型（MoLE）。MoLE的一个关键特征是每个token id与一组专用专家相关联，对于给定输入，只有与输入token id对应的专家会被激活。由于推理时将少量激活专家加载到RAM的通信开销可忽略不计，专家参数可卸载到存储中，使MoLE适用于资源受限设备。然而，MoLE基于输入id的上下文无关专家选择机制可能限制模型性能。为此，本文提出混合查找键值专家模型（MoLKV）。在MoLKV中，每个专家被构造为键值对，对于给定输入，输入衍生的查询与当前序列的缓存键值专家交互，生成上下文感知的专家输出。这种上下文感知机制缓解了MoLE的局限性，小规模评估实验表明MoLKV实现了显著更低的验证损失。

---

### 6 MOA: Multi-Objective Alignment for Role-Playing Agents

**link**: https://arxiv.org/pdf/2512.09756.pdf  
**date**: 2025-12-11  
**keywords**: cs.CL  
**abs**: 角色扮演代理（RPA）必须同时掌握许多相互冲突的技能——遵循多轮指令、展示领域知识以及采用一致的语言风格。现有工作要么依赖于监督微调（SFT），这种方法过度拟合表面线索并导致多样性低下，要么应用强化学习（RL），但无法学习多个维度以实现全面的RPA优化。我们提出了MOA（多目标对齐），这是一种强化学习框架，能够为通用RPA实现多维度、细粒度的评分标准优化。MOA引入了一种新颖的多目标优化策略，该策略在多个细粒度评分标准上同时训练，以提高优化性能。此外，为了解决模型输出多样性和质量的问题，我们还采用了带有离策略指导的思维增强滚动。在PersonaGym和RoleMRC等具有挑战性的基准测试上的大量实验表明，MOA使8B模型能够在众多维度上匹配甚至超越GPT-4o和Claude等强大基线。这证明了MOA在构建能够同时满足角色知识、人物风格、多样化场景和复杂多轮对话需求的RPA方面的巨大潜力。

---

### 7 CORE: A Conceptual Reasoning Layer for Large Language Models

**link**: https://arxiv.org/pdf/2512.09222.pdf  
**date**: 2025-12-11  
**keywords**: cs.CL  
**abs**: 大型语言模型在单轮生成任务中表现出色，但在多轮交互中，由于内部表示无法跨轮次持久化，模型仍需从不断扩展的token历史中重建用户意图和任务状态。这种以token为优先的范式会导致漂移、推理模式不一致以及随着对话深入而不断增长的提示词。本文提出了CORE，一种以概念为优先的交互层，无需修改模型权重即可提高多轮交互的稳定性。CORE结合了一个小型通用认知算子库和一个持久的Local Concept——一种紧凑的语义状态，用于捕获任务、约束、偏好和中间结果。每次模型调用仅接收此概念状态、用户的最新指令和所选算子，无需重放完整历史。模拟CORE行为的初步原型显示累积提示词token减少了约42%，不过该数字反映的是原型条件，不应被解释为真实世界的性能估计。CORE提供了一种与模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展的方向。

---

### 8 SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation

**link**: https://arxiv.org/pdf/2512.09142.pdf  
**date**: 2025-12-11  
**keywords**: cs.AI  
**abs**: 本文介绍了SDialog，一个MIT许可的开源Python工具包，它将对话生成、评估和机制可解释性统一到一个端到端框架中，用于构建和分析基于LLM的对话代理。该工具包围绕标准化的Dialog表示构建，提供：（1）基于角色的多代理模拟，通过可组合的编排实现可控的合成对话生成；（2）综合评估，结合语言指标、LLM作为评判者和功能正确性验证器；（3）机制可解释性工具，用于激活检查和通过特征消融与归纳进行引导；（4）音频生成，包括3D房间建模和麦克风效果的全声学模拟。该工具包与所有主要LLM后端集成，支持在统一API下进行混合后端实验。通过在以对话为中心的架构中耦合生成、评估和可解释性，SDialog使研究人员能够更系统地构建、基准测试和理解对话系统。

---

### 9 Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making

**link**: https://arxiv.org/pdf/2512.09440.pdf  
**date**: 2025-12-11  
**keywords**: cs.CL  
**abs**: 本研究基于知识增强的大型语言模型智能体，探索金融决策的可解释推理方法。为解决传统金融决策方法依赖参数化知识、缺乏事实一致性且缺失推理链的局限性，提出了一个集成外部知识检索、语义表示和推理生成的框架。该方法首先对金融文本和结构化数据进行编码以获取语义表示，然后通过相似度计算从外部知识库中检索任务相关信息。通过加权融合将内部表示与外部知识相结合，在确保流畅性的同时提高生成内容的事实准确性和完整性。在推理阶段，引入多头注意力机制构建逻辑链，使模型在生成过程中能够呈现透明的因果关系和可追溯性。最后，模型联合优化任务目标和解释一致性目标，提升预测性能和推理可解释性。金融文本处理和决策任务的实验表明，该方法在准确性、文本生成质量和事实支持方面优于基线方法，验证了知识增强和可解释推理的有效性。总体而言，所提方法克服了传统模型在语义覆盖和推理透明度方面的局限性，在复杂金融场景中展现出较强的实用价值。

---

### 10 RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning

**link**: https://arxiv.org/pdf/2512.09487.pdf  
**date**: 2025-12-11  
**keywords**: cs.CL  
**abs**: 检索增强生成（RAG）将非参数化知识（通常来自非结构化文本和结构化图谱）集成到大型语言模型（LLMs）中。尽管最近的研究通过强化学习（RL）将基于文本的RAG推进到多轮推理，但将这些进展扩展到混合检索带来了额外挑战。现有的基于图谱或混合系统通常依赖固定或手工设计的检索管道，缺乏在推理过程中整合补充证据的能力。此外，虽然图谱证据提供了多跳推理至关重要的关系结构，但其检索成本显著更高。为解决这些局限性，我们提出RouteRAG，一个基于RL的框架，使LLMs能够执行多轮自适应的图谱-文本混合RAG。RouteRAG通过RL联合优化整个生成过程，允许模型学习何时推理、从文本或图谱中检索什么内容以及何时生成最终答案，所有这些都在统一的生成策略中完成。为指导这一学习过程，我们设计了一个两阶段训练框架，同时考虑任务结果和检索效率，使模型能够利用混合证据，同时避免不必要的检索开销。五个问答基准的实验结果表明，RouteRAG显著优于现有的RAG基线，突显了端到端RL在支持复杂推理的自适应高效检索方面的优势。