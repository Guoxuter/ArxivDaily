以下是针对提供的论文集合的摘要和总结输出。我已根据任务要求处理输入数据：

- **摘要和总结**：直接使用每篇论文的 "summary" 字段作为 "abs" 内容，该字段已为中文，符合要求。
- **去除重复论文**：基于论文的唯一 "id" 检查重复。输入列表中所有论文的 ID 均不同（例如，2510.15047、2510.15620 等），因此无重复论文，所有 14 篇论文均被保留。
- **输出格式**：采用 Markdown 格式，每个论文条目以 "---" 分隔，包含：
  - **### 序号 Paper Name**：序号从 1 开始，按输入列表顺序编号。
  - **link**：论文的 PDF URL。
  - **date**：论文的发布日期（yyyy-mm-dd）。
  - **keywords**：使用论文的 "categories" 字段作为关键词，多个类别时以逗号分隔。
  - **abs**：论文摘要，已确保为中文。

输出如下：

---
### 1 Internalizing World Models via Self-Play Finetuning for Agentic RL
**link**: https://arxiv.org/pdf/2510.15047.pdf  
**date**: 2025-10-20  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLMs）作为智能体在分布外（OOD）场景中常面临挑战。现实世界环境复杂且动态，受特定任务规则和随机性影响，使得LLMs难以将内部知识与环境动态对齐。在此类OOD条件下，传统强化学习训练往往难以扩展；研究观察到Pass@k（k个采样轨迹中至少一个成功的概率）在训练步骤中显著下降，表明探索脆弱且泛化能力有限。受模型基强化学习启发，研究假设为LLM智能体配备内部世界模型可更好地将推理与环境动态对齐并改进决策。该研究通过将世界模型分解为状态表示和转移建模两个组件来对其进行编码。基于此，提出SPA强化学习框架，该框架通过自博弈监督微调（SFT）阶段冷启动策略，通过与环境交互学习世界模型，然后在策略优化前使用该模型模拟未来状态。这种简单初始化优于在线世界建模基线，并极大提升基于RL的智能体训练性能。在Sokoban、FrozenLake和Sudoku等多种环境中的实验表明，该方法显著提升性能。例如，SPA将Qwen2.5-1.5B-Instruct模型的Sokoban成功率从25.6%提升至59.8%，FrozenLake分数从22.1%提升至70.9%。
---
### 2 GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device
**link**: https://arxiv.org/pdf/2510.15620.pdf  
**date**: 2025-10-20  
**keywords**: cs.LG  
**abs**: 跨编码器重排器的语义top-K选择是设备端AI服务的基础，例如检索增强生成、智能体记忆（agent memory）和个性化推荐。然而，其延迟和内存需求在边缘硬件上占据端到端预算的主导地位。通过重新审视top-K选择的目标，研究揭示只有相对排名重要，而非精确的每个候选分数。进一步观察到序列级稀疏性：相对排名在中间层早期就趋于稳定，这允许在完成全推理之前进行剪枝操作，从而实现低延迟和内存高效的语义选择。
---
### 3 Continual Learning via Sparse Memory Finetuning
**link**: https://arxiv.org/pdf/2510.15103.pdf  
**date**: 2025-10-20  
**keywords**: cs.CL  
**abs**: 现代语言模型功能强大，但部署后通常是静态的。构建能够随时间持续学习的模型的主要障碍是灾难性遗忘，即更新新数据时会抹去先前获得的能力。基于缓解遗忘具有挑战性是因为所有任务共享可训练参数这一直觉，我们研究了稀疏参数更新是否能实现无灾难性遗忘的学习。我们引入稀疏记忆微调，利用记忆层模型（Berges等人，2024），其设计上就是稀疏更新的。通过仅更新相对于预训练数据使用情况被新知识高度激活的记忆槽，我们减少了新知识与模型现有能力之间的干扰。我们在两个问答任务上评估了与全量微调和使用LoRA的参数高效微调相比的学习和遗忘情况。我们发现稀疏记忆微调在学习新知识的同时表现出显著更少的遗忘：全量微调新事实后NaturalQuestions的F1下降89%，使用LoRA下降71%，而稀疏记忆微调在获得相同水平新知识的情况下仅下降11%。我们的结果表明，记忆层中的稀疏性为大型语言模型的持续学习提供了一条有前景的路径。
---
### 4 Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation
**link**: https://arxiv.org/pdf/2510.15624.pdf  
**date**: 2025-10-20  
**keywords**: cs.AI  
**abs**: 科学发现的自动化是人工智能研究的关键里程碑。然而，现有的科学智能体系统存在两个基本局限性：无法适应中间发现的刚性预编程工作流，以及阻碍长周期研究的不足上下文管理。本文提出开源多智能体框架freephdlabor，其特点是由实时智能体推理决定的完全动态工作流，以及支持无缝定制的模块化架构——用户可修改、添加或删除智能体以满足特定领域需求。该框架提供全面基础设施，包括自动上下文压缩、防止信息退化的基于工作区的通信、跨会话的记忆持久性，以及非阻塞人类干预机制。这些特性共同将自动化研究从孤立的单次尝试转变为持续的研究计划，系统地建立在先前探索基础上并整合人类反馈。通过提供构建可定制协科学家系统的架构原则和实际实现，本工作旨在促进自动化研究在科学领域的更广泛采用，使从业者能够部署自主进行端到端研究的交互式多智能体系统——从构思到实验再到可发表的手稿。
---
### 5 Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth
**link**: https://arxiv.org/pdf/2510.15719.pdf  
**date**: 2025-10-20  
**keywords**: cs.CL  
**abs**: 推理模型因其强大的性能而受到广泛关注，尤其是在通过检索增强进行优化时。然而，这些模型通常会产生高昂的计算成本，因为检索和推理令牌都对整体资源使用有重大贡献。在本研究中，我们做出了以下贡献：（1）提出了一种检索增强推理模型，该模型能够根据查询和检索结果动态调整检索文档列表的长度；（2）通过强化学习开发了一种成本感知优势函数，用于训练高效的检索增强推理模型；（3）针对近端和组相对策略优化算法，探索了所提出的成本感知框架的内存受限和延迟受限实现。我们在七个公共问答数据集上评估了我们的方法，并证明了在不影响有效性的情况下显著提高了效率。事实上，我们观察到模型延迟在各个数据集上降低了约16-20%，而在精确匹配方面，其有效性平均提高了约5%。
---
### 6 Exemplar-Guided Planing: Enhanced LLM Agent for KGQA
**link**: https://arxiv.org/pdf/2510.15283.pdf  
**date**: 2025-10-20  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）作为交互式智能体在知识图谱问答（KGQA）中显示出巨大潜力，但往往难以弥合自然语言查询与结构化知识图谱（KG）表示之间的语义鸿沟。这导致在KG上的规划效果欠佳且探索效率低下，而无训练方法通常未充分利用训练数据中的宝贵推理模式。为解决这些限制，我们提出了一种新颖的框架——示例引导规划（EGP），用于增强LLM智能体的KGQA规划能力。EGP首先通过实体模板化对训练集问题进行预处理，以归一化语义变化。然后，使用语义嵌入和高效的FAISS索引从预处理集中检索高度相似的示例问题及其成功推理路径。这些检索到的示例通过两个关键阶段动态引导LLM的规划过程：（1）任务分解，通过将生成的子目标与已验证的推理步骤对齐；（2）关系探索，通过提供高质量辅助信息提高关系剪枝准确性。此外，我们在关系探索期间引入智能前瞻机制，通过预先探索有前景的路径并可能提前终止探索来提高效率。我们将EGP应用于图上规划（PoG）框架，称为PoG-EGP。在两个真实世界KGQA数据集WebQSP和CWQ上的大量实验表明，PoG-EGP显著优于基线PoG系统和其他对比方法。
---
### 7 Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination
**link**: https://arxiv.org/pdf/2510.15312.pdf  
**date**: 2025-10-20  
**keywords**: cs.CL  
**abs**: 利用本地数据的上下文信息增强设备端大型语言模型（LLMs），能够实现个性化和任务感知的生成，为智能助手和UI智能体等用例提供支持。尽管神经处理器的最新发展大幅提高了移动设备上预填充的效率，但逐令牌生成过程由于其固有的内存受限特性，仍然存在高延迟和硬件利用率有限的问题。本文提出CoordGen，这是一种移动推理框架，它将推测解码与动态硬件调度相结合，以加速移动设备上的上下文感知文本生成。该框架引入了三个协同组件：（1）自适应执行调度，动态平衡预填充和解码阶段之间的计算图；（2）上下文对齐草稿生成，通过轻量级在线校准当前任务提高推测效率；（3）硬件高效草稿扩展，重用和扩展中间序列以提高处理并行性并降低验证成本。在多款智能手机和代表性工作负载上的实验表明，与现有移动推理解决方案相比，生成速度一致提高高达3.8倍，能源效率提高高达4.7倍。组件级分析进一步验证了每个优化的贡献。
---
### 8 CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs
**link**: https://arxiv.org/pdf/2510.15455.pdf  
**date**: 2025-10-20  
**keywords**: cs.CL  
**abs**: 移动智能体依赖大型语言模型（LLMs）在智能手机用户界面（UIs）上规划和执行任务。云LLM虽能实现高任务准确率，但需在每一步上传完整UI状态，导致不必要的信息暴露；本地LLM虽避免UI上传，但能力有限，任务成功率较低。本文提出CORE框架，通过云与本地LLM协作减少UI暴露并保持任务准确性。该框架包含三个关键组件：（1）布局感知块划分，基于XML屏幕层次结构对语义相关UI元素分组；（2）协同规划，本地与云LLM共同识别当前子任务；（3）协同决策，本地LLM对相关UI块排序，云LLM在排名靠前的块中选择特定元素。CORE还引入多轮累积机制以减轻本地判断失误或上下文限制。实验表明，CORE在多种移动应用和任务中减少了高达55.6%的UI暴露，同时任务成功率略低于纯云智能体，有效降低了向云的不必要隐私暴露。
---
### 9 Experience-Driven Exploration for Efficient API-Free AI Agents
**link**: https://arxiv.org/pdf/2510.15259.pdf  
**date**: 2025-10-20  
**keywords**: cs.AI, Agent Memory  
**abs**: 现有大多数软件缺乏可访问的API，要求智能体仅通过基于像素的GUI操作。在这种无API设置中，基于LLM的智能体面临严重的效率瓶颈：受限于局部视觉经验，决策短视且依赖低效试错，阻碍技能获取和长期规划。为此，本文提出KG-Agent，一种经验驱动的学习框架，将智能体的原始像素级交互构建为持久的状态-动作知识图谱（SA-KG）。KG-Agent通过链接功能相似但视觉不同的GUI状态，形成丰富的经验邻域，使智能体能够从多样化的历史策略中泛化，从而克服低效探索。为支持长 horizon 推理，设计了基于图拓扑的混合内在奖励机制，结合状态价值奖励（利用已知高价值路径）和新颖性奖励（鼓励目标探索）。该方法将战略规划与纯粹发现解耦，使智能体能有效评估具有延迟回报的设置动作。在《文明V》和《杀戮尖塔》两个复杂开放的GUI决策环境中评估表明，KG-Agent在探索效率和战略深度上显著优于现有方法。
---
### 10 AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory
**link**: https://arxiv.org/pdf/2510.15261.pdf  
**date**: 2025-10-20  
**keywords**: cs.AI, LLM Memory, Agent Memory, Personal Memory  
**abs**: 借助检索增强生成（RAG）在LLM中的成功，人们越来越关注为智能体系统增强外部记忆数据库。然而现有系统仅关注存储文本信息，忽略了多模态信号的重要性。受人类记忆多模态特性启发，本文提出AUGUSTUS，一种与认知科学中人类记忆理念对齐的多模态智能体系统。技术上，该系统包含四个循环连接的阶段：（i）编码：理解输入；（ii）记忆存储：保存重要信息；（iii）检索：从记忆中搜索相关上下文；（iv）行动：执行任务。与使用向量数据库的现有系统不同，本文提出将信息概念化为语义标签，并将标签与其上下文关联，存储在图结构的多模态上下文记忆中，以实现高效的概念驱动检索。该系统在ImageNet分类任务上比传统多模态RAG方法快3.5倍，在MSC基准上优于MemGPT。
---
### 11 Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL
**link**: https://arxiv.org/pdf/2510.15772.pdf  
**date**: 2025-10-20  
**keywords**: cs.AI  
**abs**: 所谓“棘手问题”，即涉及复杂多维场景、结果不可验证、影响异质且缺乏单一客观正确答案的问题，长期困扰着人类。现代例子包括司法框架决策、解决环境污染、大流行韧性规划和粮食安全等。目前正在积极探索使用最先进的人工智能系统（特别是基于大型语言模型的智能体）与人类协作解决此类问题。虽然大型语言模型（LLMs）的能力可以通过微调、手工设计的系统提示和外部工具支架等方式得到提升，但LLMs缺乏在这些场景中通过经验发展专业知识的内生机制。本研究通过Dialectica框架填补了这一空白，该框架中智能体围绕特定主题进行结构化对话，并通过记忆、自我反思和策略约束的上下文编辑进行增强。形式上，讨论被视为一种隐式的元强化学习过程。“对话训练”的智能体通过对诱发响应的事后判断 pairwise 比较进行评估。在两种模型架构（本地运行的Qwen3:30b和OpenAI的o4-mini）上的结果表明，在讨论过程中启用基于反思的上下文编辑，使得智能体在Elo分数、标准化的Bradley-Terry-Davidson能力和AlphaRank质量上优于基线模型。在陈述和反思日志中定性观察到了预期的学习特征，其中反思能够识别弱点并可靠地塑造后续陈述。定量和定性证据的一致性支持对话驱动的上下文进化作为在开放的不可验证领域中定向增强专业知识的实用路径。
---
### 12 PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction
**link**: https://arxiv.org/pdf/2510.15863.pdf  
**date**: 2025-10-20  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）正超越静态应用，驱动能在与外部环境交互过程中持续学习的智能体。例如，智能体可在浏览网页或使用新工具时学习可复用技能。然而，现有技能学习方法常导致技能过度特化于单一网站，难以泛化。本文提出PolySkill框架，使智能体能够学习可泛化、可组合的技能。其核心思想受软件工程中多态性启发，将技能的抽象目标（实现什么）与具体实现（如何执行）解耦。实验表明，该方法（1）在已见网站上技能复用率提升1.7倍；（2）在Mind2Web和未见网站上成功率分别提高9.4%和13.9%，同时步骤减少20%以上；（3）在无指定任务的自探索场景中，框架提升了提出任务的质量，并使智能体学习到跨不同网站工作的可泛化技能。通过让智能体识别和优化自身目标，PolySkill增强了其学习更好课程的能力，从而获得比基线方法更具泛化性的技能。这项工作为构建能在自适应环境中持续学习的智能体提供了实用路径，表明分离技能的目标与执行是开发能在开放网络中持续学习和泛化的自主智能体的关键一步。
---
### 13 HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks
**link**: https://arxiv.org/pdf/2510.15144.pdf  
**date**: 2025-10-20  
**keywords**: cs.AI  
**abs**: 模拟开放式任务中的人类推理是人工智能和认知科学的长期目标。尽管大型语言模型现在能大规模近似人类反应，但它们仍针对群体层面的共识进行调优，往往抹去了推理风格和信念轨迹的个体性。为推进机器更类人推理的愿景，本文引入HugAgent（Human-Grounded Agent Benchmark），一个用于从平均推理到个体推理适应的基准。任务是在给定特定人过去观点的部分证据的情况下，预测其在新场景中的推理方式和信念更新。HugAgent采用双轨设计：用于规模和系统性压力测试的合成轨道，以及用于生态有效、"出声思考"推理数据的人类轨道。这种设计实现了对智能体内在保真度的可扩展、可重复评估：模型是否不仅能捕捉人们的信念，还能捕捉其推理如何演变。使用最先进LLM的实验揭示了持续的适应差距，使HugAgent成为首个将机器推理与人类思维个体性对齐的可扩展基准。
---
### 14 Procedural Game Level Design with Deep Reinforcement Learning
**link**: https://arxiv.org/pdf/2510.15120.pdf  
**date**: 2025-10-20  
**keywords**: cs.AI  
**abs**: 程序化内容生成（PCG）已成为游戏开发中日益流行的技术，允许开发者以减少手动工作的方式生成动态、可重玩和可扩展的环境。本研究提出一种在基于Unity的3D环境中使用深度强化学习（DRL）进行程序化关卡设计的新方法。该系统包括两个智能体：蜂鸟智能体（作为求解器）和浮岛智能体（负责在地形上以现实和上下文感知的方式生成和放置可收集物体（花朵））。蜂鸟使用Unity ML-Agents工具包中的近端策略优化（PPO）算法进行训练，学习高效导航地形、定位花朵并收集它们，同时适应不断变化的岛屿程序化布局。浮岛智能体也使用PPO算法训练，基于观察到的障碍物位置、蜂鸟的初始状态和先前情节的性能反馈来生成花朵布局。这些智能体之间的交互导致涌现行为和在各种环境配置中的稳健泛化。结果表明，该方法不仅产生有效且高效的智能体行为，还为机器学习驱动的自主游戏关卡设计开辟了新机会。这项工作突出了DRL在使智能体能够在虚拟环境中生成和解决内容方面的潜力，推动了AI在创意游戏开发过程中贡献的边界。