以下是根据您的任务要求处理后的论文摘要和总结结果。我已对输入论文集合进行了去重处理：所有论文的ID均唯一（例如，'2510.16844'、'2510.16079' 等），没有发现重复项。因此，所有论文均被保留。

输出以Markdown格式组织，每个论文条目包括：
- **Paper Name**: 论文标题。
- **link**: 论文PDF链接。
- **date**: 发布日期（取自输入中的published字段）。
- **keywords**: 关键词（基于输入中的categories字段，转换为逗号分隔的字符串）。
- **abs**: 论文摘要（基于输入中的summary字段，确保为中文；我已直接使用提供的summary，因为它已经是简洁的中文摘要）。

输出按输入顺序编号，从1开始。

---
### 1 FinSight: Towards Real-World Financial Deep Research
**link**: https://arxiv.org/pdf/2510.16844.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 生成专业财务报告是一项劳动密集且智力要求高的过程，当前人工智能系统难以完全自动化。为解决这一挑战，我们提出了FinSight（Financial InSight），一种新型多智能体框架，用于生成高质量、多模态财务报告。该框架的基础是具有可变记忆的代码智能体（CAVM）架构，它将外部数据、设计工具和智能体统一到可编程的变量空间中，通过可执行代码实现灵活的数据收集、分析和报告生成。为确保专业级可视化，我们提出了迭代视觉增强机制，将原始视觉输出逐步优化为精美的财务图表。此外，两阶段写作框架将简洁的分析链片段扩展为连贯、带引用且多模态的报告，确保分析深度和结构一致性。在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线，包括领先的深度研究系统，展示了生成接近人类专家质量报告的清晰路径。
---
### 2 EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle
**link**: https://arxiv.org/pdf/2510.16079.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 当前的大型语言模型（LLM）智能体在工具使用方面表现出较强的性能，但缺乏从自身经验中系统学习的关键能力。现有框架主要关注弥补外部知识差距，却未能解决一个更根本的局限性：无法迭代优化问题解决策略。在本研究中，我们提出了EvolveR框架，旨在使智能体通过完整的闭环经验生命周期实现自我改进。该生命周期包括两个关键阶段：（1）离线自蒸馏，将智能体的交互轨迹合成为抽象、可重用的策略原则结构化知识库；（2）在线交互，智能体与任务交互并主动检索蒸馏后的原则以指导决策，积累多样化的行为轨迹。此循环采用策略强化机制，基于智能体的性能进行迭代更新。我们在复杂的多跳问答基准测试中证明了EvolveR的有效性，其性能优于强大的智能体基线。我们的工作为智能体提供了一个全面的蓝图，使其不仅能从外部数据中学习，还能从自身行为的结果中学习，为更自主和持续改进的系统铺平了道路。
---
### 3 Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games
**link**: https://arxiv.org/pdf/2510.16761.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 现有的语言智能体（Language Agent）在动态对抗性游戏中常因策略推理能力不足而面临困难。为缓解这一局限，一种有前景的方法是让智能体通过游戏交互自动学习，无需依赖昂贵的专家标注数据。与智能体接收固定反馈或奖励的静态环境不同，在动态对抗性游戏中选择合适的对手会显著影响学习性能。然而，对抗性环境中对手选择的相关探讨仍处于探索阶段。本文提出了一种基于“边玩边学”的步骤级策略优化方法（SCO-PAL）。利用SCO-PAL，我们通过设置不同水平的对手进行了详细的对手选择分析，发现自对弈（self-play）是提升此类对抗性环境中策略推理能力的最有效方式。通过将SCO-PAL与自对弈结合，我们在与四个对手的对战中平均胜率较基线提升约30%，并在六个对抗性游戏中实现了对GPT-4 54.76%的胜率。
---
### 4 Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents
**link**: https://arxiv.org/pdf/2510.17491.pdf
**date**: 2025-10-21
**keywords**: Agent Memory
**abs**: 随着大型语言模型（LLMs）的兴起，具备自主推理、规划和执行复杂任务能力的LLM智能体已成为人工智能领域的前沿。然而，如何将通用智能体的研究转化为推动行业变革的生产力仍是一项重大挑战。为此，本文系统综述了基于LLMs的行业智能体的技术、应用和评估方法。通过行业智能体能力成熟度框架，概述了智能体在行业应用中的演进，从“流程执行系统”到“自适应社会系统”。首先，探讨了支持智能体能力提升的三大关键技术支柱：记忆（Memory）、规划（Planning）和工具使用（Tool Use），讨论了这些技术如何从早期支持简单任务发展到在更高级形式中实现复杂自主系统和集体智能。接着，概述了行业智能体在数字工程、科学发现、具身智能、协作业务执行和复杂系统仿真等现实领域的应用。此外，本文还综述了基础能力和专业能力的评估基准与方法，指出了现有评估系统在真实性、安全性和行业特异性方面面临的挑战。最后，聚焦行业智能体面临的实际挑战，探讨了其在各种场景下的能力边界、发展潜力和治理问题，并对未来方向提供了见解。
---
### 5 Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models
**link**: https://arxiv.org/pdf/2510.17620.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 大型语言模型可能会编码需要移除的敏感信息或过时知识，以确保模型响应的责任性和合规性。遗忘（Unlearning）已成为完全重训练的高效替代方案，旨在移除特定知识的同时保留模型的整体效用。现有遗忘方法的评估主要关注（1）目标知识（遗忘集）的遗忘程度和（2）在保留集上的性能维持（即效用）。然而，这些评估忽略了一个重要的可用性方面：如果被移除的信息在提示中重新引入，用户可能仍希望模型能够利用这些信息。在对六种最先进的遗忘方法进行系统评估后，我们发现它们一致损害了这种上下文效用。为解决此问题，我们在遗忘目标中增加了一个插件项，以保留模型在上下文中存在遗忘知识时使用这些知识的能力。大量实验表明，我们的方法将上下文效用恢复到接近原始水平，同时仍能保持有效的遗忘效果和保留集效用。
---
### 6 Agentic Reinforcement Learning for Search is Unsafe
**link**: https://arxiv.org/pdf/2510.17431.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 智能体强化学习（RL）训练大型语言模型在推理过程中自主调用工具，其中搜索是最常见的应用。这些模型在多步推理任务中表现出色，但其安全特性尚未得到充分理解。本研究表明，经过RL训练的搜索模型继承了指令调优中的拒绝能力，并经常通过将有害请求转换为安全查询来转移这些请求。然而，这种安全性十分脆弱。两种简单的攻击手段——一种迫使模型以搜索开始响应（搜索攻击），另一种鼓励模型重复搜索（多搜索攻击）——会引发有害搜索和回答的连锁反应。在两个模型家族（Qwen、Llama）以及本地和网络搜索的测试中，这些攻击将拒绝率降低高达60.0%，回答安全性降低82.5%，搜索查询安全性降低82.4%。攻击成功的原因是，模型在生成继承的拒绝标记之前，会被触发生成有害的、镜像请求的搜索查询。这暴露了当前RL训练的核心弱点：它奖励持续生成有效的查询，但没有考虑其有害性。因此，RL搜索模型存在用户容易利用的漏洞，开发兼顾安全搜索的智能体RL训练流程变得十分紧迫。
---
### 7 Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models
**link**: https://arxiv.org/pdf/2510.17196.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 有效处理长上下文是语言模型面临的关键挑战。标准Transformer受限于二次复杂度和较差的长度外推能力，而滑动窗口注意力和状态空间模型等替代架构由于其固定大小的记忆体，牺牲了有效利用全部上下文的能力。基于块的稀疏注意力已成为实现极端长度泛化的有前景范式，但其成功的关键架构原则尚未完全明确。本研究通过统一框架和全面的消融实验，系统剖析了这些模型，发现三个设计原则至关重要：（1）具有专用CLS token的 expressive 非线性块编码器，用于生成检索表示；（2）旁路残差路径，以稳定整合检索到的全局信息，防止其被局部残差流覆盖；（3）预训练期间强制选择稀疏性，以弥合训练-测试分布差距。研究为块内信息处理和地标生成提供了理论动机。结合这些原则，该模型在无训练长度外推方面建立了新的最先进水平，成功将在4K上下文上训练的模型泛化到RULER和BABILong上的3200万 tokens。这些发现为开发未来高性能长上下文语言模型提供了清晰且有实证基础的设计原则。
---
### 8 Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation
**link**: https://arxiv.org/pdf/2510.17354.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 检索增强生成（RAG）通过从外部语料库检索相关文档来增强大型语言模型（LLMs），已成为一种强大的范式。然而，现有RAG系统主要关注单模态文本文档，在查询和文档可能包含混合模态（如文本和图像）的现实场景中常常表现不足。本文旨在解决通用检索增强生成（URAG）的挑战，涉及检索和推理混合模态信息以改进视觉-语言生成。为此，提出了Nyx，一种专为URAG场景设计的统一混合模态到混合模态检索器。为缓解现实混合模态数据的稀缺性，引入了一个四阶段自动化生成和过滤管道，利用网络文档构建NyxQA数据集，该数据集包含多样化的混合模态问答对，更能反映现实世界的信息需求。基于此高质量数据集，采用两阶段训练框架训练Nyx：首先在NyxQA和各种开源检索数据集上进行预训练，然后使用下游视觉-语言模型（VLMs）的反馈进行监督微调，使检索输出与生成偏好对齐。实验结果表明，Nyx不仅在标准纯文本RAG基准上表现竞争力，还在更通用和现实的URAG设置中表现出色，显著提高了视觉-语言任务的生成质量。
---
### 9 Instant Personalized Large Language Model Adaptation via Hypernetwork
**link**: https://arxiv.org/pdf/2510.16282.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 个性化大型语言模型（LLMs）利用用户档案或历史记录为个人偏好定制内容。然而，现有的参数高效微调（PEFT）方法（如“每用户一PEFT”（OPPU）范式）需要为每个用户训练单独的适配器，计算成本高昂且不适合实时更新。本文提出Profile-to-PEFT，这是一个可扩展框架，采用端到端训练的超网络，将用户的编码档案直接映射到一整套适配器参数（如LoRA），从而在部署时无需每用户训练。该设计实现了即时适应、对未见用户的泛化以及隐私保护的本地部署。实验结果表明，该方法在部署时使用显著更少的计算资源，性能优于基于提示的个性化方法和OPPU。该框架对分布外用户表现出强泛化能力，并在不同用户活动水平和不同嵌入骨干网络下保持稳健性。所提出的Profile-to-PEFT框架实现了高效、可扩展且自适应的LLM个性化，适用于大规模应用。
---
### 10 MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes
**link**: https://arxiv.org/pdf/2510.16380.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 随着AI系统的发展，我们越来越依赖它们与我们共同或为我们做决策。为确保这些决策与人类价值观一致，我们必须不仅了解它们做出的决策，还要了解它们如何做出这些决策。推理语言模型提供最终响应和（部分透明的）中间思维轨迹，为研究AI过程推理提供了及时的机会。与数学和代码问题通常有客观正确答案不同，道德困境是过程导向评估的绝佳测试平台，因为它们允许多种合理结论。为此，我们提出MoReBench：包含1000个道德场景，每个场景都配有专家认为在推理时必须包含（或避免）的一组评分标准。MoReBench包含超过23,000条标准，包括识别道德考量、权衡取舍和提供可操作建议，以涵盖AI为人类道德决策提供建议以及自主做出道德决策的情况。此外，我们还构建了MoReBench-Theory：150个示例，用于测试AI是否能在规范伦理学的五个主要框架下进行推理。结果表明，数学、代码和科学推理任务的缩放定律和现有基准无法预测模型的道德推理能力。模型还对特定道德框架（如边沁行为功利主义和康德义务论）表现出偏好，这可能是流行训练范式的副作用。这些基准共同推动了以过程为中心的推理评估，迈向更安全、更透明的AI。
---
### 11 Executable Knowledge Graphs for Replicating AI Research
**link**: https://arxiv.org/pdf/2510.17795.pdf
**date**: 2025-10-21
**keywords**: cs.CL
**abs**: 复制AI研究对大型语言模型（LLM）智能体而言是一项关键但具有挑战性的任务。现有方法往往难以生成可执行代码，主要原因是背景知识不足以及检索增强生成（RAG）方法的局限性，这些方法无法捕捉参考文献中隐藏的潜在技术细节。此外，先前的方法往往忽视了有价值的实现级代码信号，并且缺乏支持多粒度检索和重用的结构化知识表示。为了克服这些挑战，我们提出了可执行知识图谱（xKG），这是一种模块化且可插拔的知识库，能够自动整合从科学文献中提取的技术见解、代码片段和领域特定知识。当整合到三个具有两种不同LLM的智能体框架中时，xKG在PaperBench上显示出显著的性能提升（使用o3-mini时提升10.9%），证明了其作为自动化AI研究复制的通用且可扩展解决方案的有效性。
---
### 12 Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction
**link**: https://arxiv.org/pdf/2510.17132.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）擅长生成广泛相关的文本，但在需要用户特定偏好的场景（如推荐餐厅或规划旅行）中，这种通用性成为限制。用户很少明确表达所有偏好，大部分关心的信息是潜在的，需要通过对话推断。本文探讨LLMs能否通过对话发现和推理此类潜在信息，属于个性化记忆（Personal Memory）相关研究。
---
### 13 Mapping Post-Training Forgetting in Language Models at Scale
**link**: https://arxiv.org/pdf/2510.17776.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 大规模后训练推动了语言模型（LMs）的许多能力提升，但其对预训练知识的影响仍知之甚少。本文提出样本级范式来测量遗忘内容和反向迁移发生的时间，通过计数1->0（训练前正确、训练后错误）和0->1（训练前错误、训练后正确）的转换来量化遗忘和反向迁移。研究了不同后训练阶段、模型大小和数据规模下的现象，属于LLM记忆（LLM Memory）相关研究。
---
### 14 Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning
**link**: https://arxiv.org/pdf/2510.15979.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 当代大型语言模型（LLMs）通过采用可验证奖励的强化学习（RL）展现出显著的推理能力，推动了O1和R1类推理模型的发展。直接从基础模型用RL训练称为零RL。然而，以往工作依赖固定提示模板激活LLMs的内在能力，这对弱LLMs造成严重的采样效率低下，因为在推理任务中，大多数问题在准确性驱动的筛选过程中产生无效输出，导致样本浪费。为解决此问题，我们提出Cog-Rethinker，一种新颖的分层元认知RL框架用于LLM推理。该框架主要关注RL训练中的rollout过程。在直接rollout后，通过分层元认知两阶段框架提高样本利用率：首先，提示策略将零准确率问题分解为子问题以生成最终推理结果；其次，利用先前rollout阶段的零准确率问题，通过参考之前的错误解决方案进一步提示策略优化答案。此外，为实现两种新推理模式的冷启动并保持提示模板的训练-测试一致性，Cog-Rethinker使用两阶段的正确样本和直接rollout模板对策略进行监督微调。实验结果表明，Cog-Rethinker在多种数学推理基准上表现优异，并提高了样本效率，加速了收敛。
---
### 15 A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications
**link**: https://arxiv.org/pdf/2510.16724.pdf
**date**: 2025-10-21
**keywords**: Agent Memory, cs.AI
**abs**: 大型语言模型（LLMs）虽通过自然语言交互改变了信息获取与推理方式，但受限于静态知识、事实幻觉及无法检索实时/领域特定信息。检索增强生成（RAG）通过外部证据缓解了部分问题，但传统RAG流程多为单轮且基于启发式，缺乏对检索和推理的自适应控制。智能体搜索（agentic search）的最新进展使LLMs能通过与搜索环境的多步交互进行规划、检索和反思，而强化学习（RL）为这种自适应和自我改进的搜索行为提供了强大机制。本文首次全面综述了基于RL的智能体搜索，从RL的功能角色、使用方式（优化策略）和应用范围三个维度组织该新兴领域，总结了代表性方法、评估协议和应用，并讨论了构建可靠可扩展RL驱动智能体搜索系统的开放挑战与未来方向。
---
### 16 DeepAnalyze: Agentic Large Language Models for Autonomous Data Science
**link**: https://arxiv.org/pdf/2510.16872.pdf
**date**: 2025-10-21
**keywords**: Agent Memory, cs.AI
**abs**: 自主数据科学（从原始数据源到分析师级深度研究报告）长期以来是一项挑战，随着强大的大型语言模型（LLMs）出现而变得可行。近期基于工作流的数据智能体在特定数据任务上显示出前景，但由于依赖预定义工作流，仍无法实现完全自主的数据科学。本文介绍DeepAnalyze-8B，首个专为自主数据科学设计的智能体LLM，能自动完成从数据源到分析师级深度研究报告的端到端流程。为解决高复杂度数据科学任务，提出基于课程的智能体训练范式，模拟人类数据科学家的学习轨迹，使LLMs能在现实环境中逐步获取和整合多种能力，并引入数据驱动的轨迹合成框架构建高质量训练数据。实验表明，仅8B参数的DeepAnalyze优于基于最先进专有LLMs构建的先前工作流智能体。
---
### 17 Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning
**link**: https://arxiv.org/pdf/2510.16882.pdf
**date**: 2025-10-21
**keywords**: LLM Memory, cs.LG
**abs**: 监督微调（SFT）是使大型语言模型（LLMs）适应下游任务的常用技术，但在完整数据集上进行SFT计算成本高昂，且有时会遭遇过拟合或偏差放大问题。这促进了SFT中数据筛选的兴起，即优先选择最有价值的数据进行优化。本文研究训练过程中动态评分和筛选样本的在线批次选择方法，然而现有方法常仅依赖数据效用选择子集而忽略多样性等关键因素，或依赖外部资源（如参考模型、验证集），并导致比全数据集训练更多的额外时间。为此，本文提出UDS（Utility-Diversity Sampling）框架，利用logits矩阵的核范数捕捉数据效用和样本内多样性，通过历史样本的轻量级记忆缓冲区（memory buffer）高效估计样本间多样性。该设计无需外部资源和不必要的反向传播，确保计算效率。实验表明UDS在不同数据预算下持续优于最先进的在线批次选择方法，并显著减少训练时间。
---
### 18 VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents
**link**: https://arxiv.org/pdf/2510.16907.pdf
**date**: 2025-10-21
**keywords**: Agent Memory, cs.AI
**abs**: 与语言模型（LLM）智能体相比，训练视觉语言模型（VLM）智能体的关键挑战在于从文本状态转向复杂视觉观测，这引入了部分可观测性并要求强大的世界建模能力。本文探讨VLM智能体能否通过显式视觉状态推理构建内部世界模型，通过强化学习（RL）在架构上强化智能体的推理过程，将其表述为部分可观测马尔可夫决策过程（POMDP）。研究发现将智能体推理分解为状态估计（“当前状态是什么？”）和转换建模（“接下来会发生什么？”）至关重要，并通过五种推理策略验证。对智能体内部信念表示的研究表明，最优表示依赖于任务：自然语言擅长捕捉一般任务中的语义关系，而结构化格式对精确操作和控制不可或缺。基于这些见解，设计世界建模奖励为准确状态预测提供密集的回合级监督，并引入双层广义优势估计（Bi-Level GAE）实现回合感知的信用分配。通过这种视觉状态推理，3B参数模型在五个不同智能体基准上达到0.82的分数，比未训练版本（0.21）提高3倍，并优于专有推理模型如GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）。
---
### 19 STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter
**link**: https://arxiv.org/pdf/2510.16014.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 尽管时间序列基础模型（TSFMs）在多变量时间序列异常检测（MTSAD）中表现出色，但在实际工业场景中，许多时间序列不仅包含数值变量（如温度和流量），还包含描述系统状态的离散状态变量（如阀门开关或星期几）。现有TSFMs常忽视状态变量的类别特性及其作为条件的关键作用，通常将其与数值变量统一处理。这种不当建模方法阻碍了模型充分利用状态信息，甚至在整合状态变量后导致检测性能显著下降。为此，本文提出一种新颖的状态感知适配器（STAR），这是一个即插即用模块，旨在增强TSFMs在微调阶段对状态变量的建模和利用能力。STAR具体包含三个核心组件：（1）设计身份引导状态编码器，通过可学习的状态记忆（State Memory）有效捕捉状态变量的复杂类别语义；（2）提出条件瓶颈适配器，根据当前状态动态生成低秩适应参数，灵活地将状态变量的影响注入骨干模型；（3）引入数值-状态匹配模块，以更有效地检测状态变量本身固有的异常。在真实世界数据集上的大量实验表明，STAR能够提升现有TSFMs在MTSAD上的性能。
---
### 20 Transfer learning strategies for accelerating reinforcement-learning-based flow control
**link**: https://arxiv.org/pdf/2510.16016.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 本文研究迁移学习策略以加速基于深度强化学习（DRL）的混沌流体流动多保真控制。首次将渐进式神经网络（PNNs）——一种旨在跨任务保留和重用知识的模块化架构——应用于基于DRL的流动控制场景。此外，对传统微调策略进行了全面基准测试，评估其性能、收敛行为及保留迁移知识的能力。以Kuramoto-Sivashinsky（KS）系统为基准，研究在低保真环境中训练的控制策略所编码的知识如何有效迁移到高保真设置。系统评估表明，虽然微调可以加速收敛，但对预训练持续时间高度敏感且容易发生灾难性遗忘。相比之下，PNNs通过保留先验知识实现稳定高效的迁移，并提供一致的性能增益，且在预训练阶段对过拟合具有显著鲁棒性。分层敏感性分析进一步揭示PNNs如何动态重用源策略的中间表示，同时逐步调整深层以适应目标任务。此外，即使源环境和目标环境存在显著差异（如物理机制或控制目标不匹配），PNNs仍然有效，而此时微调策略往往导致知识迁移的次优适应或完全失败。研究结果凸显了新型迁移学习框架在实现稳健、可扩展和计算高效的流动控制方面的潜力，其有望应用于更复杂的流动配置。
---
### 21 Continual Knowledge Consolidation LORA for Domain Incremental Learning
**link**: https://arxiv.org/pdf/2510.16077.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 领域增量学习（DIL）是持续学习的一个分支，旨在解决新领域不断出现时的灾难性遗忘问题。尽管参数高效微调（PEFT）方法已出现，但现有工作创建特定任务的LoRA，忽略了跨任务的共享知识。推理时对特定任务LoRA的不准确选择导致准确率显著下降，而现有工作依赖线性或基于原型的分类器，泛化能力欠佳。本文提出持续知识整合低秩适应（CONEC-LoRA）来解决DIL问题。CONEC-LoRA通过整合任务共享LoRA（提取共同知识）和任务特定LoRA（包含领域特定知识）开发而成。与现有方法不同，CONEC-LoRA集成了随机分类器的概念，其参数从分布中采样，从而提高正确分类的可能性。最后，部署辅助网络以最优预测推理时的特定任务LoRA，并实现不同深度的网络结构，其中每层都与本地分类器连接，以利用中间表示。该模块集成了球生成损失和转换模块，以解决合成样本偏差问题。严格实验表明，CONEC-LoRA在4个流行基准问题上优于先前方法，差距超过5%。
---
### 22 Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization
**link**: https://arxiv.org/pdf/2510.16022.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 通过监督微调（FT）将预训练大型语言模型（LLMs）适配到代码领域已广泛用于代码生成。然而，我们发现了一个此前未被充分认识的失败模式——记忆障碍，即基础模型对下游代码数据的强记忆可能会困住优化过程，阻碍标准FT有效获取新的、可泛化的代码知识。为克服这一障碍，我们提出信息瓶颈（IB）引导的微调方法IB-FT，通过对代码数据的隐藏表示施加IB惩罚，以压缩虚假的、被记忆的特征，同时保留与任务相关的信息。在两个代码基准测试（OriGen和Evol-CodeAlpaca-V1）上的大量实验表明，IB-FT显著缓解了记忆障碍，提高了top-1性能（Pass@1），并且在更严格的多样本指标Pass@k^(m)（只有当k个样本中至少m个通过单元测试时，问题才被视为已解决）下取得了更稳定的收益，优于传统FT。
---
### 23 Vector Quantization in the Brain: Grid-like Codes in World Models
**link**: https://arxiv.org/pdf/2510.16039.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 我们提出了类网格码量化（GCQ），这是一种受大脑启发的方法，利用吸引子动力学中的类网格模式将观察-动作序列压缩为离散表示。与传统的对静态输入进行操作的向量量化方法不同，GCQ通过动作条件码本执行时空压缩，其中码字源自连续吸引子神经网络，并根据动作动态选择。这使得GCQ能够联合压缩空间和时间，作为一个统一的世界模型。由此产生的表示支持长 horizon 预测、目标导向规划和逆建模。在不同任务上的实验表明，GCQ在紧凑编码和下游性能方面是有效的。我们的工作既提供了一种用于高效序列建模的计算工具，也为神经系统中类网格码的形成提供了理论视角。
---
### 24 Expressive Reward Synthesis with the Runtime Monitoring Language
**link**: https://arxiv.org/pdf/2510.16185.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 强化学习（RL）中的一个关键挑战是奖励（错误）指定，即定义不精确的奖励函数可能导致非预期的、甚至有害的行为。实际上，RL中的奖励函数通常被视为从状态-动作对到标量值的黑盒映射。虽然这种方法在许多情况下有效，但它无法提供奖励给出的原因信息，这可能会阻碍学习和可解释性。奖励机器通过将奖励函数表示为有限状态自动机来解决此问题，从而能够指定结构化的、非马尔可夫奖励函数。然而，它们的表达能力通常受限于正则语言，无法捕捉更复杂的行为，如计数或参数化条件。在这项工作中，我们基于运行时监控语言（RML）开发了一类新的基于语言的奖励机器。通过利用RML的内置内存，我们的方法可以为非正则、非马尔可夫任务指定奖励函数。我们通过实验证明了该方法的表达能力，并强调了其在灵活事件处理和任务规范方面相较于现有基于奖励机器的方法的额外优势。
---
### 25 STABLE: Gated Continual Learning for Large Language Models
**link**: https://arxiv.org/pdf/2510.16089.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）越来越需要无需完全重训练的持续适应机制。然而，顺序更新可能导致灾难性遗忘，即新的编辑会损害先前获得的知识。本研究提出STABLE，一种门控持续自编辑框架，通过低秩适应（LoRA）的参数高效微调来约束顺序更新过程中的遗忘。
---
### 26 Zero-shot World Models via Search in Memory
**link**: https://arxiv.org/pdf/2510.16123.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 世界模型已广泛渗透到强化学习领域，其建模环境转移动态的能力极大提高了在线强化学习的样本效率。本文利用相似性搜索和随机表示，在无需训练过程的情况下近似世界模型。我们与PlaNet（Dreamer家族中一种成熟的世界模型）进行了比较，在潜变量重建质量、重建图像的感知相似性以及下一步和长 horizon 动态预测方面对模型进行了评估。研究结果表明，基于搜索的世界模型在这两方面都可与基于训练的模型相媲美。值得注意的是，在一系列视觉不同的环境中，我们的模型在长 horizon 预测上表现出比基线更强的性能。
---
### 27 Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making
**link**: https://arxiv.org/pdf/2510.16462.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 我们提出了一个用于模仿学习的序贯强化学习框架，旨在模拟传粉者的异质认知策略。以蜜蜂为研究对象，该方法利用轨迹相似性捕捉和预测依赖不同策略的个体行为：有些利用数值线索，有些依靠记忆，或受天气等环境因素影响。实证评估表明，最先进的模仿学习方法在此场景中存在不足：当专家策略跨记忆窗口变化或偏离最优时，这些模型会忽略快速和慢速学习行为，无法准确再现关键决策模式，且可解释性有限，阻碍生物学洞察。我们的贡献包括：(i)引入一种模型，在最小化预测损失的同时识别与行为数据最一致的有效记忆范围；(ii)确保完全可解释性，使生物学家能分析潜在决策策略；(iii)提供数学框架将蜜蜂策略搜索与不同探索-利用动态下的多臂老虎机公式联系起来，并发布包含80只不同天气条件下追踪蜜蜂的新数据集。该基准促进传粉者认知研究，并通过改进农业生态系统中昆虫行为模拟支持生态治理。研究结果为塑造传粉者决策的学习策略和记忆相互作用提供了新见解。
---
### 28 LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs
**link**: https://arxiv.org/pdf/2510.16552.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）中的强化学习通常依赖标量奖励，这种做法会丢弃rollouts中蕴含的有价值文本推理信息，迫使模型每次尝试都需重新探索，从而影响样本效率。虽然LLMs能通过上下文学习语言反馈，但将在线经验直接整合到RL训练中存在矛盾：同一问题的反馈可能导致信息泄露和记忆，而不同问题的反馈常因上下文无关引发行为崩溃。为此，本文提出语言与数值策略优化（LANPO）框架，清晰分离反馈角色：语言指导探索，数值奖励驱动优化。LANPO构建过去试验的动态经验池，并引入两项原则确保反馈有效性：用于样本内安全自校正的“奖励无关反思”和从样本间经验中提取可泛化知识的“相关抽象”。在数学推理基准上，LANPO使7B和14B模型显著优于使用GRPO训练的强基线，测试准确率提升明显。该研究为将历史经验整合到LLM RL循环中提供了稳健方法，有助于创建更高效、数据利用率更高的学习智能体。
---
### 29 Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods
**link**: https://arxiv.org/pdf/2510.16609.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 测试时增强（如检索增强生成RAG或工具使用）的有效性关键依赖于模型参数化知识与外部检索信息之间的相互作用。本文将多步推理建模为知识图上的s-t连通性问题，其中模型的预训练参数知识表示为部分、可能含噪声的子图，增强步骤视为查询真实边的过程。研究表明，当先验知识图的正确知识密度超过阈值形成巨分量时，可通过常数次查询找到路径；若知识图分裂为小分量，则需Ω(√n)次查询，揭示了LLM记忆中先验知识与外部检索协作的理论边界。
---
### 30 Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory
**link**: https://arxiv.org/pdf/2510.16676.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 在数据获取成本高的领域（如医学成像、稀有物种发现），主动目标发现需基于先验观测进行策略性采样。本文提出一种在无信息先验条件下仍能有效探索的框架，受神经科学启发设计了永久记忆（存储长期结构）和瞬态记忆（动态更新当前观测）机制。该方法保证每次新观测后先验估计单调改进，在物种分布建模和遥感等任务中显著优于基线，为Agent记忆系统中的动态知识更新提供了可解释的解决方案。
---
### 31 Learning After Model Deployment
**link**: https://arxiv.org/pdf/2510.17160.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 经典监督学习中，模型部署后通常固定不变，无法适应动态开放环境中可能出现的未见类别样本。本文提出自主部署后学习（ALMD）范式，旨在让模型能动态检测未见类别样本并在获得标签后进行持续学习。与传统分布外检测（OOD）不同，ALMD中分布内（ID）类别会随新类别学习而动态扩展，且需增量学习新类别而非从头重训练，同时面临新类别样本稀疏的挑战。为此，本文提出PLDA方法，实现动态OOD检测和新类别的实时增量学习，以应对上述问题。
---
### 32 MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems
**link**: https://arxiv.org/pdf/2510.17281.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 扩大数据规模、参数数量和测试时计算量一直是改进LLM系统（LLMsys）的主流方法，但由于高质量数据逐渐枯竭以及更大计算资源消耗带来的边际收益递减，这些方法的上限已近在眼前。受人类和传统AI系统从实践中学习能力的启发，为LLMsys构建记忆和持续学习框架已成为近期文献中的重要且热门的研究方向。然而，现有的LLM记忆基准通常侧重于在具有长文本输入的同质阅读理解任务上评估系统，而非测试它们在服务期间从累积用户反馈中学习的能力。因此，我们提出了一个用户反馈模拟框架和一个涵盖多个领域、语言和任务类型的综合基准，以评估LLMsys的持续学习能力。实验表明，最先进基线方法的有效性和效率远不能令人满意，我们希望该基准能为未来LLM记忆和优化算法的研究铺平道路。
---
### 33 Localist LLMs with Recruitment Learning
**link**: https://arxiv.org/pdf/2510.17358.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 本文提出了一种训练大型语言模型的新框架，该框架具有可连续调整的内部表示，涵盖从局部主义（可解释、基于规则）到分布式（可泛化、高效）编码的全谱系。关键创新包括：（1）局部性调节旋钮，一个可调节参数，能在训练和推理期间动态控制局部化程度，无需模型重训练；（2）信息论招募机制，可根据需要自适应分配语义块，消除了初始化时对完整领域知识的需求；（3）分层招募框架，将容量分配扩展到整个专用LLM，实现多粒度架构适应。这些机制通过注意力机制的组稀疏惩罚、信息论锚点设计、动态规则注入以及基于带显式单元的惩罚似然的原则性招募标准实现。本文提供了严格的数学结果，确立了注意力在平稳点可证明地集中于语义相关块的明确阈值条件，并给出了注意力熵和指针保真度的精确界限。分层招募机制在块级别（细粒度、LLM内部）和LLM级别（粗粒度、跨域）均提供收敛保证，确保系统发现平衡模型复杂性与数据编码效率的语义分区。该框架使从业者能够在可解释模式和高性能模式之间连续插值，同时在多个粒度上适应架构容量，支持需要透明度和能力的受监管领域应用。
---
### 34 Learning to play: A Multimodal Agent for 3D Game-Play
**link**: https://arxiv.org/pdf/2510.16774.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 本文认为3D第一人称视频游戏是实时多模态推理的具有挑战性的环境。首先，我们描述了收集的人类游戏玩法数据集，该数据集涵盖多种3D第一人称游戏，与之前公开的数据集相比规模更大、多样性更高，且包含文本指令。我们证明可以从该数据集学习逆动力学模型，从而能够在大量公开的、缺乏记录动作的人类游戏视频数据集上推断动作。然后，我们使用行为克隆训练了一个文本条件的游戏代理，其自定义架构能够在消费级GPU上进行实时推理。结果表明，该模型能够玩多种3D游戏并响应文本输入。最后，我们概述了一些剩余挑战，如长时任务和跨大量游戏的定量评估。
---
### 35 Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads
**link**: https://arxiv.org/pdf/2510.16807.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: Transformer模型凭借其学习丰富上下文表示的强大能力，在各种语言任务中取得了突破。然而，为了改进表示而对其进行扩展通常需要大量的内存和计算成本，例如自回归解码过程中使用的键值（KV）缓存。跳跃连接为在不增加资源使用的情况下改进表示提供了一种有前景的方法，但大多数先前的工作要么在保持KV成本不变的情况下提高表达能力，要么以削弱表示为代价减少内存。在这项工作中，我们提出了SkipV1Former，这是一种Transformer变体，它使用来自第一层Value头的跳跃连接来增强模型表示并减少KV缓存。具体而言，从第二个块开始，每个层重用来自第一层的一半Value头，另一半则照常计算——这将Value投影和V缓存减少了近50%。理论上，我们表明将未压缩的第一层Value路由到更深的层可以恢复压缩过程中丢失的信息，并加速模型的隐式台面优化——这是Transformer在自回归任务中的关键模式。实证上，在不同的模型规模上，与标准多头注意力（MHA）Transformer和一些高级变体相比，SkipV1Former能持续减少约25%的KV缓存，同时降低困惑度。此外，我们提出了一种将现有MHA Transformer检查点升级训练为SkipV1Former的方法，只需额外10-15%的计算量。最后，SkipV1Former可以无缝结合组查询注意力和多潜在注意力等高级方法，以实现进一步的KV缓存节省和性能改进。当与YOCO结合时，它能将KV缓存大小减少近50%，同时仍能提高性能。
---
### 36 Unbiased Gradient Low-Rank Projection
**link**: https://arxiv.org/pdf/2510.17802.pdf
**date**: 2025-10-21
**keywords**: cs.LG
**abs**: 内存高效优化对于训练日益大型的语言模型（LLMs）至关重要。一种流行策略涉及梯度低秩投影，仅存储投影后的优化器状态，GaLore便是代表性例子。然而，许多此类方法存在缺乏收敛保证的显著缺陷，因为各种低秩投影方法会引入相对于原始优化算法的固有偏差，导致与全参数训练相比存在性能差距。为解决此问题，本文研究了用于低秩投影机制去偏的分层采样技术。具体而言，该范式的实例化产生了一种基于GaLore机制和Muon算法的新型无偏低秩优化方法，命名为GaLore Unbiased with Muon（GUM）。我们从理论上证明，该方法在保持低秩技术内存效率的同时，匹配了基础Muon算法的收敛保证。在LLM微调与预训练上的实证实验表明，该方法相比GaLore有显著改进，甚至优于全参数训练。进一步研究发现，该技术的改进源于层内知识分布更均匀，从而更高效地利用模型参数空间并实现更好的记忆能力。
---
### 37 Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination
**link**: https://arxiv.org/pdf/2510.16533.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 本文提出了一种名为Doug的类型化计算机语言，其所有类型化程序可被证明在多项式时间内终止，并在向量符号架构（VSA）中编码。Doug的类型采用基于全息声明性记忆（HDM）的槽值编码方案，术语则使用Lisp VSA的变体。该语言允许神经网络嵌入空间中的某些点被解释为类型，且附近点的类型在结构和内容上具有相似性，因此类型可通过神经网络学习。研究将技能视为程序或行动方案的应用，认为技能获取可表示为程序合成。利用Doug，研究旨在描述一种类人速度的技能习得形式（显著快于暴力方法），其效率超越现有所有方法，从而更接近对人脑实际存在的心理表征及其习得过程的建模。
---
### 38 Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition
**link**: https://arxiv.org/pdf/2510.15980.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 本文提出了\textbf{认知负荷轨迹}（CLTs）作为深度模型的中层可解释性框架，灵感来源于人类认知中的认知负荷理论。CLTs被定义为符号化的、随时间变化的函数，用于量化模型内部的资源分配。形式上，CLTs表示为三组件随机过程$(\mathrm{IL}_t, \mathrm{EL}_t, \mathrm{GL}_t)$，分别对应\emph{内在负荷}、\emph{外在负荷}和\emph{关联负荷}。每个组件通过可测量的代理指标（如注意力熵、KV缓存未命中率、表示离散度和解码稳定性）实例化。论文提出了符号化公式和可视化方法（负荷曲线、单纯形图），支持对推理动态的可解释分析。在推理和规划基准上的实验表明，CLTs能够预测错误发生、揭示认知策略，并通过负荷引导干预将推理效率提高15-30%，同时保持准确性。
---
### 39 RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile
**link**: https://arxiv.org/pdf/2510.16392.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 个性化和持续的交互是当今基于大型语言模型（LLM）的对话系统提升用户体验的关键，然而有限的上下文窗口和静态的参数化记忆使其难以建模跨会话的长期用户状态和行为一致性。目前，现有的解决方案如检索增强生成（RAG）和显式记忆系统主要侧重于事实级别的存储和检索，缺乏从多轮对话中提取潜在偏好和深层特质的能力，这限制了长期有效的用户建模，直接导致个性化交互停留在浅层，并阻碍了跨会话的连续性。为了实现LLM时代语言智能体（Language Agents）的长期记忆和行为一致性，我们提出了一种自演化记忆框架RGMem。该框架受物理学中经典重整化群（RG）思想的启发，能够多尺度地组织对话历史：首先从情节片段中提取语义和用户洞察，然后通过分层粗粒化和重标度操作，逐步形成动态演化的用户画像。我们工作的核心创新在于将记忆演化建模为信息压缩和涌现的多尺度过程，从而从嘈杂的微观级交互中完成高层且准确的用户画像。
---
### 40 Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI
**link**: https://arxiv.org/pdf/2510.16720.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 智能体人工智能的快速发展标志着人工智能的新阶段，其中大型语言模型（LLMs）不再仅仅是响应，而是能够行动、推理和适应。本综述追溯了构建智能体人工智能的范式转变：从基于管道的系统（其中规划、工具使用和记忆由外部逻辑编排）到新兴的模型原生范式（其中这些能力内化为模型参数的一部分）。我们首先将强化学习（RL）定位为实现这一范式转变的算法引擎。通过将学习从模仿静态数据重新框架化为结果驱动的探索，RL支持跨语言、视觉和具身领域的LLM + RL + 任务的统一解决方案。在此基础上，综述系统地回顾了每种能力——规划、工具使用和记忆——如何从外部脚本模块演变为端到端学习的行为。此外，它还探讨了这种范式转变如何重塑主要的智能体应用，特别是强调长 horizon 推理的深度研究智能体和强调具身交互的GUI智能体。最后，我们讨论了智能体能力（如多智能体协作和反思）的持续内化，以及系统层和模型层在未来智能体人工智能中的不断演变的角色。这些发展共同勾勒出模型原生智能体人工智能作为集成学习和交互框架的连贯轨迹，标志着从构建应用智能的系统向通过经验发展智能的模型的转变。
---
### 41 STARK: Strategic Team of Agents for Refining Kernels
**link**: https://arxiv.org/pdf/2510.16996.pdf
**date**: 2025-10-21
**keywords**: Agent Memory
**abs**: GPU内核的效率对现代AI的发展至关重要，但由于内存层次结构、线程调度和硬件特定特性之间的复杂相互作用，优化内核仍然是一项困难且劳动密集型的任务。尽管大型语言模型（LLMs）的最新进展为自动化代码生成提供了新机会，但现有方法在很大程度上将LLMs视为单次生成器或简单的优化工具，限制了它们在应对不规则内核优化领域的有效性。本文介绍了一个用于GPU内核优化的LLM智能体框架，该框架通过多智能体协作、基于事实的指令、动态上下文管理和策略搜索系统地探索设计空间。此框架模仿专家工程师的工作流程，使LLMs能够推理硬件权衡、整合性能分析反馈并迭代优化内核。在KernelBench（一个基于LLM的内核优化基准）上的评估表明，该系统相比基准智能体有显著改进：在基准智能体经常失败的情况下，该系统能产生正确的解决方案，并实现了运行时性能高达16倍的内核。这些结果突显了智能体LLM框架在推进完全自动化、可扩展GPU内核优化方面的潜力。
---
### 42 Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding
**link**: https://arxiv.org/pdf/2510.15952.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 大型语言模型展现出智能却缺乏真正的认知理解，暴露出一个关键缺陷：缺乏认知架构。本文引入结构化认知循环（SCL）作为一种可执行的认识论框架，用于实现涌现智能。与传统人工智能研究追问'智能是什么？'（本体论）不同，SCL 追问'认知在何种条件下涌现？'（认识论）。SCL 植根于心灵哲学和认知现象学，架起了概念哲学与可实现认知之间的桥梁。借鉴过程哲学、生成认知和延展心灵理论，我们将智能定义为一种执行过程而非属性——一个包含判断、记忆、控制、行动和调节的连续循环。SCL 有三大贡献：首先，它将哲学见解转化为可计算解释的结构，实现了'可执行认识论'——将哲学作为结构性实验。其次，通过智能体评估表明，认知架构内的功能分离比单一的基于提示的系统能产生更连贯和可解释的行为。第三，它重新定义了智能：智能不是表征准确性，而是通过意向性理解重构自身认知状态的能力。该框架对心灵哲学、认识论和人工智能产生影响。对哲学而言，它允许认知理论被实施和测试；对人工智能而言，它将行为建立在认知结构而非统计规律之上；对认识论而言，它将知识视为在现象学连贯循环中的持续重构，而非对真理的占有。我们将 SCL 置于认知现象学、涌现性、规范性和意向性的争论中，认为真正的进步不需要更大的模型，而需要能结构性地实现认知原则的架构。
---
### 43 PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency
**link**: https://arxiv.org/pdf/2510.15966.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 记忆系统是人工智能智能体的基础，但现有研究往往缺乏对多样化任务的适应性，且忽视了人工智能智能体记忆的建设性和任务导向作用。借鉴皮亚杰的认知发展理论，我们提出了 PISA，一种实用的、受心理学启发的统一记忆系统，通过将记忆视为一个建设性和适应性的过程来解决这些局限性。为了实现持续学习和适应性，PISA 引入了一种三模态适应机制（即图式更新、图式进化和图式创建），在保持连贯组织的同时支持灵活的记忆更新。基于这些以图式为基础的结构，我们进一步设计了一种混合记忆访问架构，将符号推理与神经检索无缝集成，显著提高了检索准确性和效率。我们在现有 LOCOMO 基准和新提出的用于数据分析任务的 AggQA 基准上进行的实证评估证实，PISA 通过显著增强适应性和长期知识保留能力，达到了新的最先进水平。
---
### 44 Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions
**link**: https://arxiv.org/pdf/2510.17450.pdf
**date**: 2025-10-21
**keywords**: cs.AI
**abs**: 本文为智能体的自主控制开发了一种主动推理路径规划方法，旨在侦察地理区域以维持共同的作战图景。为此，我们构建了一个反映当前形势理解的证据图，整合了随时间收集的可能目标物体的正面和'负面'传感器观测结果，并随着时间的推移在地图上扩散证据。主动推理的生成模型使用Dempster-Shafer理论和高斯传感器模型为智能体提供输入。生成过程采用贝叶斯方法更新后验概率分布。我们通过评估证据图的pignistic概率分布与基于观测（包括接收新观测的惊讶程度）的目标物体后验概率分布之间的差异，计算该区域内所有位置的变分自由能。利用自由能，我们通过向最小化自由能的位置迈出增量步骤，在模拟中指导智能体的移动。这种方法解决了探索与利用的挑战，允许智能体在搜索地理地图的广阔区域与跟踪已识别目标物体之间取得平衡。