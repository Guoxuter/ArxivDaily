---

### 1 Continual Low-Rank Adapters for LLM-based Generative Recommender Systems

**link**: https://arxiv.org/pdf/2510.25093.pdf
**date**: 2025-10-30
**keywords**: cs.LG
**abs**: 虽然大型语言模型（LLMs）在推荐系统中表现出强大性能，但随着用户、物品和用户偏好的不断演变，它们在持续学习方面面临挑战。现有的基于LoRA的持续方法主要关注保留先前任务的性能，但忽略了推荐系统的独特性质：目标不是预测过去的偏好，当当前兴趣发生显著变化时，过时的偏好甚至可能损害性能。为解决这一问题，我们提出了PESO（Proximally rEgularized Single evolving lOra），一种用于推荐系统中LoRA的持续适应方法。PESO引入了一个近端正则化器，将当前适配器锚定到其最近的冻结状态，使模型能够灵活地平衡适应和保留，并更好地捕捉最近的用户行为。理论上，我们证明这种近端设计在LoRA子空间中提供了数据感知的方向指导。实验表明，PESO始终优于现有的基于LoRA的持续学习方法。

---

### 2 FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data

**link**: https://arxiv.org/pdf/2510.25223.pdf
**date**: 2025-10-30
**keywords**: Agent Memory
**abs**: 本文提出了FELA（Feature Engineering LLM Agents），这是一个多智能体进化系统，用于从复杂的工业事件日志数据中自主提取有意义且高性能的特征。FELA集成了大型语言模型（LLMs）的推理和编码能力与洞察引导的自进化范式，具体包括 Idea Agents、Code Agents 和 Critic Agents 等专业智能体协作生成、验证和实现新特征想法。其中，Evaluation Agent 会总结反馈并更新分层知识库和双记忆系统（dual-memory system）以实现持续改进，这与智能体记忆（Agent Memory）相关。实验表明，FELA 能够生成可解释、与领域相关的特征，显著提高模型性能并减少人工工作量。

---

### 3 Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading

**link**: https://arxiv.org/pdf/2510.25014.pdf
**date**: 2025-10-30
**keywords**: cs.AI
**abs**: 大型语言模型（LLMs）能够实现动态游戏交互，但在规则驱动的交易系统中难以遵循关键的流程步骤，从而损害玩家信任。本文解决了LLMs的创造性灵活性与游戏内交易（浏览-报价-审核-确认）流程要求之间的核心矛盾。为此，提出了自回归状态跟踪提示（ASTP）方法，该方法以精心设计的提示为中心，迫使LLM使其状态跟踪过程明确且可验证。ASTP不依赖于隐式的上下文理解，而是要求LLM识别并报告上一轮的预定义状态标签。为确保交易完整性，还辅以特定状态的占位符后处理方法以实现准确的价格计算。对300个交易对话的评估表明，状态合规率>99%，计算精度达99.3%。值得注意的是，在较小模型（Gemini-2.5-Flash）上使用带占位符后处理的ASTP可匹配较大模型（Gemini-2.5-Pro）的性能，同时将响应时间从21.2秒减少到2.4秒，为满足商业游戏的实时要求和资源约束奠定了实用基础。

---

### 4 Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions

**link**: https://arxiv.org/pdf/2510.25445.pdf
**date**: 2025-10-30
**keywords**: cs.AI
**abs**: Agentic AI代表了人工智能的变革性转变，但其快速发展导致了理解的碎片化，常常将现代神经系统与过时的符号模型混为一谈——这种做法被称为概念翻新。本研究通过引入一种新颖的双范式框架来澄清这种混淆，该框架将智能体系统分为两个不同的谱系：符号/经典型（依赖算法规划和持久状态）和神经/生成型（利用随机生成和提示驱动的编排）。通过对90项研究（2018-2025年）进行基于PRISMA的系统综述，我们围绕该框架从三个维度进行了全面分析：（1）定义每个范式的理论基础和架构原则；（2）在医疗、金融和机器人等领域的特定实现，展示应用约束如何决定范式选择；（3）特定于范式的伦理和治理挑战，揭示不同的风险和缓解策略。我们的分析表明，范式的选择具有战略性：符号系统在安全关键领域（如医疗）占主导地位，而神经系统在适应性强、数据丰富的环境（如金融）中占优势。此外，我们确定了关键的研究空白，包括符号系统治理模型的显著不足以及对混合神经-符号架构的迫切需求。研究结果最终形成了一个战略路线图，认为Agentic AI的未来不在于单一范式的主导，而在于它们的有意整合，以创建既适应又可靠的系统。这项工作提供了必要的概念工具包，以指导未来的研究、开发和政策走向稳健且可信赖的混合智能系统。

---

### 5 ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents

**link**: https://arxiv.org/pdf/2510.25668.pdf
**date**: 2025-10-30
**keywords**: cs.AI
**abs**: 视觉语言模型(VLMs)在处理长文档时存在局限，现有方法常依赖固定模板或刚性流程。本文提出ALDEN，一种多轮强化学习框架，将VLMs微调为能主动导航长文档的交互代理。该框架引入直接按索引访问页面的'fetch'动作，结合规则化跨层级奖励提供过程监督，并通过视觉语义锚定机制稳定训练。ALDEN在长文档基准测试中表现优异，推动了从被动阅读到自主导航推理的进步，其主动导航和多轮学习机制涉及代理的过程性记忆(Procedural Memory)应用。

---

### 6 The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework

**link**: https://arxiv.org/pdf/2510.25732.pdf
**date**: 2025-10-30
**keywords**: LLM Memory
**abs**: 大型语言模型（LLMs）的遗忘（Unlearning）对于管理敏感数据和纠正错误信息至关重要，但其有效性评估仍是一个未解决的问题。我们研究了在参数规模从2.7B到13B的模型（OPT-2.7B、LLaMA-2-7B、LLaMA-3.1-8B、LLaMA-2-13B）中，说服性提示是否能从刻意遗忘的LLM中唤起事实知识。借鉴ACT-R和Hebbian理论（扩散激活理论）以及通信原理，我们引入了刺激-知识纠缠-行为框架（SKeB），该框架通过领域图对信息纠缠进行建模，并测试遗忘模型中的事实回忆是否与说服性框架相关。我们开发了纠缠度量来量化知识激活模式，并评估输出中的事实性、非事实性和幻觉。结果表明，说服性提示显著增强了事实知识的回忆（基线为14.8%，权威框架下为24.5%），其效果与模型大小呈负相关（2.7B模型恢复128%，13B模型恢复15%）。SKeB为评估LLM中的遗忘完整性、鲁棒性和整体行为提供了基础。

---

### 7 TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation

**link**: https://arxiv.org/pdf/2510.25536.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）展现出类人能力，越来越被视为模拟个人沟通风格、行为倾向和人格特质的基础。然而，当前对基于LLM的人格模拟评估存在局限：大多依赖合成对话，缺乏系统框架和能力需求分析。为此，本文引入TwinVoice，一个全面的基准，用于评估不同现实场景下的人格模拟。TwinVoice涵盖三个维度：社交人格（公共社交互动）、人际人格（私人对话）和叙事人格（基于角色的表达），并将LLM性能评估分解为六项基本能力，包括观点一致性、记忆回忆、逻辑推理、词汇保真度、人格语气和句法风格。实验结果表明，尽管先进模型在人格模拟中达到中等准确性，但在句法风格和记忆回忆等能力上仍有不足，LLM的平均性能远低于人类基线。

---

### 8 TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling

**link**: https://arxiv.org/pdf/2510.25758.pdf
**date**: 2025-10-30
**keywords**: cs.AI
**abs**: 大型语言模型（LLMs）在心理咨询中的应用受到越来越多的关注，但现有方法往往缺乏情感理解、自适应策略以及跨多会话使用治疗方法的长期记忆，使其与实际临床实践相去甚远。为解决这些关键差距，本文引入了TheraMind，一种用于纵向心理咨询的战略性自适应智能体。TheraMind的核心是一种新颖的双循环架构，将复杂的咨询过程分解为用于战术对话管理的会话内循环和用于战略治疗规划的跨会话循环。会话内循环感知患者的情绪状态以动态选择响应策略，同时利用跨会话记忆确保连续性。关键的是，跨会话循环通过在每个会话后评估应用疗法的效果并为后续交互调整方法，使智能体具备长期适应性。在基于真实临床案例的高保真模拟环境中进行的评估表明，TheraMind优于其他方法，尤其在连贯性、灵活性和治疗协调性等多会话指标上，验证了其双循环设计在模拟战略性、适应性和纵向治疗行为方面的有效性。

---

### 9 Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy

**link**: https://arxiv.org/pdf/2510.25378.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）已越来越多地应用于从自然语言理解到代码生成的广泛任务。虽然它们也被用于辅助文献推荐，但虚构不存在论文的幻觉问题仍然是一个主要问题。本研究基于先前的研究，假设LLM正确生成文献信息的能力取决于底层知识是生成的还是记忆的，高被引论文（即更频繁出现在训练语料库中）的幻觉率更低。因此，我们将引用次数作为训练数据冗余（即特定文献记录在预训练语料库中重复出现的频率）的代理，研究引用频率如何影响LLM输出中的幻觉引用。使用GPT-4.1，我们在20个计算机科学领域生成并手动验证了100条文献记录，并通过生成的元数据与真实元数据之间的余弦相似度来衡量事实一致性。结果表明：（i）幻觉率因研究领域而异；（ii）引用次数与事实准确性呈强相关；（iii）超过约1000次引用后，文献信息几乎被逐字记忆。这些发现表明，高被引论文几乎被模型逐字保留，表明存在一个从泛化转向记忆的阈值。

---

### 10 CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories

**link**: https://arxiv.org/pdf/2510.25333.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 近年来，基于大型语言模型（LLM）的智能体发展迅速，为利用语言智能体解决复杂现实问题提供了可能。一个重要的应用领域是业务智能体，它们通过工具调用与数据库和内部知识库交互，以满足多样化的用户需求。然而，该领域存在复杂的数据关系和广泛的异构任务（从统计数据查询到基于知识的问答）等挑战。为解决这些问题，本文提出了CRMWeaver，一种在复杂环境中增强业务智能体的新方法。为使智能体模型适应复杂的业务环境，在训练阶段采用了合成数据生成和基于强化学习（RL）的范式，显著提升了模型处理复杂数据和多样化任务的能力。在推理阶段，引入了共享记忆机制，促使智能体从相似问题的任务指南中学习，从而进一步提高其有效性和泛化能力，尤其是在未见过的场景中。通过在CRMArena-Pro数据集上的验证，我们的轻量级模型在B2B和B2C业务场景中均取得了具有竞争力的结果，突显了其在实际应用中的价值。

---

### 11 DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates

**link**: https://arxiv.org/pdf/2510.25110.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 通过社交互动准确建模意见变化对于解决错误信息和极化等问题至关重要。虽然角色扮演大型语言模型（LLMs）为模拟类人交互提供了有前景的方法，但现有研究表明，单智能体对齐并不保证真实的多智能体群体动态。当前的LLM角色扮演设置通常产生不自然的动态（如过早收敛），且缺乏衡量真实人类意见轨迹的实证基准。本研究引入DEBATE，这是首个专门设计用于评估多智能体角色扮演LLMs交互真实性的大规模实证基准。DEBATE包含来自2792多名美国参与者就107个争议话题进行的多轮辩论对话中的29,417条消息，同时捕获公开表达的消息和私下报告的意见。利用DEBATE，研究系统评估并识别了模拟与真实群体动态之间的关键差异，并展示了DEBATE通过监督微调使LLMs与人类行为对齐的效用，在表面级指标（如ROUGE-L和消息长度）上取得改进，同时突出了深层语义对齐（如语义相似性）的局限性。

---

### 12 A Survey on Unlearning in Large Language Models

**link**: https://arxiv.org/pdf/2510.25117.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）的发展革新了自然语言处理，但其在大规模语料上的训练带来了重大风险，包括记忆敏感个人数据、受版权保护的材料以及可能促成恶意活动的知识。为缓解这些问题并符合“被遗忘权”等法律和伦理标准，机器遗忘已成为一种关键技术，用于在不损害LLMs整体性能的前提下选择性地删除特定知识。本综述系统回顾了2021年以来发表的180多篇关于LLM遗忘的论文，专门关注大规模生成模型。与先前综述不同，本文引入了新的遗忘方法和评估分类法，将方法明确分为训练时、训练后和推理时三类，并系统整理了现有数据集和指标，分析其优缺点和适用性，为研究社区提供实用指导。此外，还讨论了关键挑战和有前景的未来研究方向，旨在为安全可靠LLMs的持续发展提供信息和指导。

---

### 13 ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation

**link**: https://arxiv.org/pdf/2510.25224.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 虽然大型语言模型（LLMs）越来越多地用于智能体框架以协助个体用户，但对能够主动管理复杂多方协作的智能体的需求日益增长。此类主动智能体的系统评估方法仍然稀缺，限制了开发有效支持多人协作的AI的进展。谈判为这一挑战提供了严格的测试平台，需要社会认知智能来导航多方和多主题之间的利益冲突并建立共识。本研究提出ProMediate，这是首个用于评估复杂、多主题、多方谈判中主动AI中介智能体的框架。ProMediate包括两个核心组件：（i）基于现实谈判案例和理论驱动难度级别的模拟测试平台（ProMediate-Easy、ProMediate-Medium和ProMediate-Hard），以及基于社会认知中介理论的即插即用主动AI中介，能够灵活决定何时以及如何干预；（ii）社会认知评估框架，带有一套新的指标来衡量共识变化、干预延迟、中介有效性和智能。结果表明，具有社会智能的中介智能体优于通用基线，通过更快、更有针对性的干预在ProMediate-Hard设置中使共识变化增加3.6个百分点，同时响应速度快77%。

---