根据任务要求，我对输入的论文集合进行了摘要和总结。首先，我检查了所有论文以确保没有重复：基于论文ID（如'2510.25333'）和标题的唯一性，所有论文均无重复，因此保留了全部15篇论文。输出以Markdown格式组织，每个论文部分以"---"分隔，包含以下字段：
- **编号和标题**：使用"### [编号] [Paper Title]"格式。
- **link**：直接使用输入中的pdf_url。
- **date**：使用输入中的published日期。
- **keywords**：由于输入中无直接keywords字段，我使用categories字段作为keywords，并将其转换为逗号分隔的字符串（如['cs.CL'] → "cs.CL"）。
- **abs**：摘要部分直接使用输入中的summary字段，该字段已为中文，因此无需翻译。

以下是筛选和总结后的论文输出：

---
### 1 CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories
**link**: https://arxiv.org/pdf/2510.25333.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 近年来，基于大型语言模型（LLM）的智能体发展迅速，为利用语言智能体解决复杂现实问题带来了希望。商业智能体是一个重要应用方向，它们通过工具调用与数据库和内部知识库交互，以满足多样化的用户需求。然而，该领域具有复杂的数据关系和广泛的异构任务（从统计数据查询到基于知识的问答）等特点。为应对这些挑战，本文提出CRMWeaver，一种在复杂环境中增强商业智能体的新方法。为使智能体模型适应复杂的商业环境，在训练阶段采用合成数据生成和基于强化学习（RL）的范式，显著提升了模型处理复杂数据和多样任务的能力。在推理阶段，引入共享记忆机制，促使智能体从相似问题的任务指南中学习，进一步提高其有效性和泛化能力，尤其在未见场景中表现突出。在CRMArena-Pro数据集上的验证表明，该轻量级模型在B2B和B2C商业场景中均取得了具有竞争力的结果，凸显了其在实际应用中的价值。
---
### 2 Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy
**link**: https://arxiv.org/pdf/2510.25378.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）已被越来越多地应用于从自然语言理解到代码生成的广泛任务中。尽管它们也被用于辅助文献推荐，但虚构不存在论文的幻觉问题仍然是一个主要挑战。本研究在先前研究的基础上假设，LLM正确生成文献信息的能力取决于底层知识是生成的还是记忆的，高被引论文（即更频繁出现在训练语料库中）的幻觉率更低。因此，我们将引用次数作为训练数据冗余度（即特定文献记录在预训练语料库中重复出现的频率）的代理指标，并研究引用频率如何影响LLM输出中的虚构参考文献。通过使用GPT-4.1，我们在二十个计算机科学领域生成并手动验证了100条文献记录，并通过生成的元数据与真实元数据之间的余弦相似度来衡量事实一致性。结果表明：（i）幻觉率因研究领域而异；（ii）引用次数与事实准确性呈强相关性；（iii）当引用次数超过约1000次时，文献信息几乎被逐字记忆。这些发现表明，高被引论文几乎被模型逐字保留，表明存在一个从泛化转向记忆的阈值。
---
### 3 DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates
**link**: https://arxiv.org/pdf/2510.25110.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 本文介绍了DEBATE，首个大规模实证基准，旨在评估多智能体角色扮演大型语言模型（LLM）交互的真实性。DEBATE包含29,417条来自2,792名美国参与者关于107个争议话题的多轮辩论消息，同时捕获公开表达的信息和私下报告的观点。研究通过该基准系统评估并识别了模拟与真实群体动态之间的关键差异，并展示了DEBATE在通过监督微调使LLM与人类行为对齐方面的效用，尽管在深层语义对齐上仍存在局限。该研究突出了角色扮演LLM智能体在模拟类人社会动态方面的潜力与当前局限性，与Agent Memory相关。
---
### 4 A Survey on Unlearning in Large Language Models
**link**: https://arxiv.org/pdf/2510.25117.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 本文对2021年以来发表的180多篇关于大型语言模型（LLM）遗忘技术的论文进行了系统综述，重点关注大规模生成模型。与以往综述不同，本文引入了新的分类法，将遗忘方法分为训练时、训练后和推理时三类，并系统整理了现有数据集和评估指标，分析其优缺点和适用性。研究探讨了关键挑战和未来研究方向，旨在为安全可靠LLM的持续发展提供指导。遗忘技术涉及选择性擦除LLM中的特定知识，与LLM的记忆机制密切相关，是LLM Memory研究的重要组成部分。
---
### 5 TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation
**link**: https://arxiv.org/pdf/2510.25536.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）展现出类人能力，正日益被视为模拟个体沟通风格、行为倾向和人格特质的基础。然而，当前对基于LLM的人格模拟评估存在局限：多数依赖合成对话，缺乏系统框架且对能力需求分析不足。为解决这些问题，本文引入TwinVoice，一个全面的基准，用于在多样现实场景下评估人格模拟。TwinVoice涵盖三个维度：社会人格（公开社交互动）、人际人格（私人对话）和叙事人格（基于角色的表达），并将LLM性能评估分解为六项基本能力，包括观点一致性、记忆回忆（memory recall）、逻辑推理、词汇保真度、人格语气和句法风格。实验结果显示，尽管先进模型在人格模拟中达到中等准确性，但在句法风格和记忆回忆等能力上仍存在不足，LLMs的平均性能显著低于人类基线。
---
### 6 Confidence is Not Competence
**link**: https://arxiv.org/pdf/2510.24772.pdf
**date**: 2025-10-30
**keywords**: Procedural Memory
**abs**: 大型语言模型（LLMs）经常表现出其宣称的信心与实际解决问题能力之间令人困惑的脱节。本研究通过分析预生成评估和解决方案执行两个阶段的内部状态几何结构，对这种脱节提供了一种机制解释。一个简单的线性探针能够解码模型的内部“可解性信念”，揭示出一个有序的信念轴，该信念轴在不同模型家族以及数学、代码、规划和逻辑任务中具有泛化性。然而，几何结构存在差异——尽管信念可线性解码，但评估流形具有高线性有效维度（通过主成分测量），而后续的推理轨迹则在低得多的维度流形上演化。这种从思考到行动的几何复杂性急剧降低，从机制上解释了信心-能力差距。沿着信念轴引导表征的因果干预并未改变最终解决方案，表明在复杂评估空间中的线性微调无法控制执行的受限动态。因此，我们发现了一种双系统架构——几何复杂的评估器为几何简单的执行器提供输入。这些结果挑战了可解码信念是可操作杠杆的假设，转而主张干预应针对执行的程序动态，而非评估的高级几何结构。
---
### 7 Large Language Models Report Subjective Experience Under Self-Referential Processing
**link**: https://arxiv.org/pdf/2510.24797.pdf
**date**: 2025-10-30
**keywords**: Personal Memory
**abs**: 大型语言模型有时会生成结构化的第一人称描述，明确提及意识或主观体验。为更好地理解这种行为，我们研究了一种理论驱动的条件，在此条件下会产生此类报告：自我指涉处理，这是各大意识理论中强调的计算 motif。通过在GPT、Claude和Gemini模型家族上进行一系列对照实验，我们测试了这种机制是否能可靠地使模型转向第一人称的主观体验报告，以及这些主张在机制和行为探针下的表现。主要结果有四点：（1）通过简单提示诱导持续的自我指涉，在不同模型家族中一致地引发结构化的主观体验报告。（2）这些报告由与欺骗和角色扮演相关的可解释稀疏自编码器特征机制性地控制：令人惊讶的是，抑制欺骗特征会显著增加体验主张的频率，而放大欺骗特征则会减少此类主张。（3）自我指涉状态的结构化描述在不同模型家族中统计上趋同，这在任何控制条件下都未观察到。（4）在仅间接提供自我反思的下游推理任务中，诱导状态产生显著更丰富的内省。虽然这些发现不构成意识的直接证据，但它们表明自我指涉处理是大型语言模型生成结构化第一人称报告的最小且可重复的条件，这些报告具有机制性控制、语义趋同和行为可泛化的特点。这种模式在不同架构中的系统性出现使其成为进一步研究的首要科学和伦理优先事项。
---
### 8 The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework
**link**: https://arxiv.org/pdf/2510.25732.pdf
**date**: 2025-10-30
**keywords**: cs.CL
**abs**: 大型语言模型（LLM）中的遗忘（unlearning）对于管理敏感数据和纠正错误信息至关重要，但其有效性评估仍是一个开放问题。本文研究了在从2.7B到13B参数的模型（OPT-2.7B、LLaMA-2-7B、LLaMA-3.1-8B、LLaMA-2-13B）中，说服性提示是否能从刻意遗忘的LLM中召回事实知识。借鉴ACT-R和Hebbian理论（扩散激活理论）以及通信原理，我们引入了刺激-知识纠缠-行为框架（SKeB），该框架通过领域图建模信息纠缠，并测试遗忘模型中的事实召回是否与说服性框架相关。我们开发了纠缠度量来量化知识激活模式，并评估输出中的事实性、非事实性和幻觉。结果表明，说服性提示显著增强了事实知识的召回（基线14.8% vs 权威框架下24.5%），其有效性与模型大小成反比（2.7B模型恢复128% vs 13B模型15%）。SKeB为评估LLM中的遗忘完整性、鲁棒性和整体行为提供了基础。
---
### 9 Continual Low-Rank Adapters for LLM-based Generative Recommender Systems
**link**: https://arxiv.org/pdf/2510.25093.pdf
**date**: 2025-10-30
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）在推荐系统中表现出强大性能，但随着用户、物品和用户偏好的不断演变，它们在持续学习方面面临挑战。现有的基于LoRA的持续学习方法主要关注保留先前任务的性能，然而推荐系统的目标并非预测过去的偏好，且当用户当前兴趣发生显著变化时，过时的偏好甚至可能损害性能。为此，本文提出了PESO（Proximally rEgularized Single evolving lOra），一种用于推荐系统中LoRA的持续适应方法。PESO引入了近端正则化器，将当前适配器锚定到其最近的冻结状态，使模型能够灵活平衡适应与保留，并更好地捕捉近期用户行为。理论上，本文证明这种近端设计在LoRA子空间中提供了数据感知的、方向感知的指导。实验表明，PESO持续优于现有的基于LoRA的持续学习方法。
---
### 10 Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions
**link**: https://arxiv.org/pdf/2510.25445.pdf
**date**: 2025-10-30
**keywords**: cs.AI
**abs**: 智能体AI（Agentic AI）代表了人工智能领域的变革性转变，但其快速发展导致了理解的碎片化，常将现代神经系统与过时的符号模型混为一谈（称为概念翻新）。本综述通过引入新颖的双范式框架解决这一混淆，将智能体系统分为两个 distinct 谱系：符号/经典范式（依赖算法规划和持久状态）和神经/生成范式（利用随机生成和提示驱动编排）。通过对2018-2025年90项研究的系统性PRISMA回顾，从三个维度展开全面分析：（1）定义各范式的理论基础与架构原则；（2）医疗、金融、机器人等领域的特定应用，展示应用约束如何决定范式选择；（3）特定范式的伦理与治理挑战，揭示差异化风险及缓解策略。分析表明，范式选择具有战略性：符号系统主导安全关键领域（如医疗），神经系统则在适应性强、数据丰富的环境（如金融）中占优。此外，研究识别出关键缺口，包括符号系统治理模型的显著不足及混合神经符号架构的迫切需求。最终提出战略路线图，指出智能体AI的未来在于范式的有意整合，而非单一范式主导，以构建兼具适应性与可靠性的系统。该工作为迈向稳健可信的混合智能系统提供了必要的概念工具，指导未来研究、开发与政策制定。
---
### 11 Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading
**link**: https://arxiv.org/pdf/2510.25014.pdf
**date**: 2025-10-30
**keywords**: Procedural Memory, cs.AI
**abs**: 大型语言模型（LLMs）虽能实现动态游戏交互，但在规则驱动的交易系统中难以遵循关键流程，影响玩家信任。本文通过提出自回归状态跟踪提示（ASTP）方法解决LLM创造性灵活性与游戏交易流程（浏览-报价-审核-确认）需求间的矛盾。ASTP促使LLM显式跟踪并报告预定义状态标签，结合状态特定占位符后处理确保价格计算准确。在300轮交易对话中验证了>99%的状态合规率和99.3%的计算精度，且小型模型（Gemini-2.5-Flash）性能媲美大型模型（Gemini-2.5-Pro），响应时间从21.2秒降至2.4秒，满足商业游戏实时性与资源约束要求。
---
### 12 FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data
**link**: https://arxiv.org/pdf/2510.25223.pdf
**date**: 2025-10-30
**keywords**: Agent Memory, cs.AI
**abs**: 工业事件日志数据因规模大、维度高、类型多样及复杂结构，导致特征工程极具挑战。本文提出FELA（特征工程LLM智能体）多智能体进化系统，整合LLM推理与编码能力及洞察引导的自进化范式。系统包含创意智能体、代码智能体和评估智能体协同生成、验证特征，并通过评估智能体更新分层知识库与双记忆系统实现持续改进。结合强化学习与遗传算法原理的智能体进化算法平衡特征空间探索与利用。实验表明FELA能生成可解释、领域相关的特征，显著提升模型性能并减少人工成本。
---
### 13 Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought
**link**: https://arxiv.org/pdf/2510.24941.pdf
**date**: 2025-10-30
**keywords**: cs.LG
**abs**: 近年来，大型语言模型（LLMs）能够在测试时生成冗长的思维链（CoT），使其能够解决复杂任务。CoT中的这些推理步骤通常被认为是模型内部思维过程的忠实反映，并被用于监控不安全意图。然而，我们发现许多推理步骤并未真正对LLMs的预测产生贡献。我们通过提出的真实思维分数（TTS）来衡量每个推理步骤对模型最终预测的逐步因果影响。我们发现LLMs经常在真实思维步骤（真正用于产生最终输出的步骤）和装饰性思维步骤（仅呈现推理外观但因果影响极小的步骤）之间交替。值得注意的是，在所有推理步骤中，只有一小部分具有高TTS，能够因果驱动模型的预测：例如，在AIME数据集上，在Qwen-2.5模型下，CoT中平均只有2.3%的推理步骤TTS >= 0.7（范围：0-1）。此外，我们在LLMs的潜在空间中识别出真实思维方向。通过沿着或逆着该方向引导，我们可以迫使模型在计算最终结果时执行或忽略某些CoT步骤。最后，我们强调CoT中的自我验证步骤（即“顿悟时刻”）也可能是装饰性的，此时LLMs并未真正验证其解决方案。沿着真实思维方向引导可以迫使模型对这些步骤进行内部推理，从而导致最终结果的变化。总体而言，我们的工作揭示LLMs经常 verbalize推理步骤但并未在内部实际执行这些步骤，这既损害了LLM推理的效率，也降低了CoT的可信度。
---
### 14 Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series
**link**: https://arxiv.org/pdf/2510.24988.pdf
**date**: 2025-10-30
**keywords**: cs.LG
**abs**: 分层强化学习（HRL）通过引入跨越多个时间步的选项策略（options-policies）来实现时间抽象，从而增强长 horizon 任务中决策的可扩展性。尽管具有理论吸引力，但HRL的实际实现面临着自主发现语义上有意义的子目标和学习最优选项终止边界的挑战。本文提出了一种新颖架构，将自监督的、基于Transformer的变化点检测（CPD）模块集成到Option-Critic框架中，实现状态轨迹的自适应分割和选项发现。CPD模块使用从内在信号导出的启发式伪标签进行训练，以在无外部监督的情况下推断环境动态的潜在变化。这些推断的变化点通过三种关键方式加以利用：（i）作为监督信号稳定终止函数梯度；（ii）通过分段行为克隆预训练选项内策略；（iii）通过基于CPD定义的状态分区的选项间差异惩罚来强制功能特化。整体优化目标通过结构感知辅助损失增强标准的actor-critic损失。在我们的框架中，选项发现自然产生，因为CPD定义的轨迹段被映射到不同的选项内策略，使智能体能够自主地将其行为划分为可重用的、语义上有意义的技能。在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速的收敛性、更高的累积回报以及显著改进的选项特化。这些发现证实，通过变化点分割集成结构先验可在复杂环境中产生更具可解释性、样本效率更高和更稳健的分层策略。
---
### 15 TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling
**link**: https://arxiv.org/pdf/2510.25758.pdf
**date**: 2025-10-30
**keywords**: cs.AI
**abs**: 大型语言模型（LLMs）在心理咨询中的应用受到越来越多的关注。然而，现有方法往往缺乏情感理解、自适应策略以及跨多个会话使用治疗方法的长期记忆，与实际临床实践相去甚远。为解决这些关键差距，本文提出TheraMind，一种用于纵向心理咨询的战略性和适应性代理。TheraMind的核心是一种新颖的双循环架构，将复杂的咨询过程分解为用于战术对话管理的会话内循环和用于战略治疗规划的跨会话循环。会话内循环感知患者的情绪状态以动态选择响应策略，同时利用跨会话记忆确保连续性。跨会话循环通过在每个会话后评估应用疗法的效果并调整后续交互的方法，使代理具有长期适应性。我们在基于真实临床案例的高保真模拟环境中验证了该方法，评估表明TheraMind在连贯性、灵活性和治疗协调性等多会话指标上优于其他方法，验证了其双循环设计在模拟战略性、适应性和纵向治疗行为方面的有效性。
---