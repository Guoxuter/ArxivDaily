### 1 TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation

**link**: https://arxiv.org/pdf/2510.25536.pdf  
**date**: 2025-10-30  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）展现出类人能力，被视为模拟个人沟通风格、行为倾向和人格特质的基础。然而，当前评估存在局限：依赖合成对话、缺乏系统框架且能力需求分析不足。本文引入TwinVoice，一个全面的基准，用于在多样化的现实世界情境中评估人格模拟。它涵盖社会人格（公开社交互动）、人际人格（私人对话）和叙事人格（基于角色的表达）三个维度，并将性能分解为六项能力：观点一致性、记忆回忆、逻辑推理、词汇保真度、人格语气和句法风格。实验表明，先进模型在人格模拟中达到中等准确度，但在句法风格和记忆回忆上不足，平均性能远低于人类基线。

---

### 2 DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates

**link**: https://arxiv.org/pdf/2510.25110.pdf  
**date**: 2025-10-30  
**keywords**: cs.CL  
**abs**: 准确模拟观点变化对解决错误信息和极化问题至关重要。角色扮演大型语言模型（LLMs）可模拟类人交互，但单智能体对齐无法保证真实群体动态，现有设置常产生不自然动态（如过早收敛）。本文引入DEBATE，首个专门评估多智能体角色扮演LLMs交互真实性的大规模基准，包含2792多名美国参与者就107个争议话题进行的29417条多轮辩论消息，捕获公开表达和私下观点。利用DEBATE，作者系统评估并识别了模拟与真实动态的关键差异，证明其可通过监督微调改进表面指标（如ROUGE-L），但深层语义对齐（如语义相似性）仍有局限。

---

### 3 A Survey on Unlearning in Large Language Models

**link**: https://arxiv.org/pdf/2510.25117.pdf  
**date**: 2025-10-30  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）的发展彻底改变了自然语言处理，但其训练带来风险：记忆敏感个人数据、受版权材料及恶意知识。为缓解问题并符合“被遗忘权”等标准，机器遗忘成为关键技术，可选择性删除特定知识而不损害整体性能。本综述系统回顾180多篇论文，引入针对遗忘方法和评估的新型分类法：方法分为训练时、训练后和推理时三类；评估方面，系统整理数据集和指标，批判分析优缺点。此外，讨论关键挑战和未来方向，旨在为安全可靠LLMs发展提供指导。

---

### 4 CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories

**link**: https://arxiv.org/pdf/2510.25333.pdf  
**date**: 2025-10-30  
**keywords**: cs.CL  
**abs**: 基于大型语言模型（LLM）的智能体为利用语言智能体解决复杂商业问题提供可能，但该领域涉及复杂数据关系和广泛异构任务（如统计数据查询和知识问答）。本文提出CRMWeaver，一种在复杂环境中增强商业智能体的新方法：训练阶段采用合成数据生成和强化学习（RL）范式，提升处理复杂数据和多样化任务的能力；推理阶段引入共享记忆机制，使智能体从相似问题中学习，提高有效性和泛化能力。在CRMArena-Pro数据集上验证，轻量级模型在B2B和B2C场景中取得有竞争力结果，突显实用价值。

---

### 5 Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy

**link**: https://arxiv.org/pdf/2510.25378.pdf  
**date**: 2025-10-30  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）应用于文献推荐时，虚构不存在论文的幻觉问题仍是主要挑战。本研究假设LLM生成文献信息的能力取决于知识是生成还是记忆，高被引论文（更频繁出现在训练语料库中）的幻觉率更低。使用GPT-4.1生成并手动验证100条计算机科学领域文献记录，通过余弦相似度衡量事实一致性。结果表明：（i）幻觉率因领域而异；（ii）引用计数与事实准确性强相关；（iii）当引用次数超过约1000次时，文献信息几乎被逐字记忆，表明存在从泛化转向记忆的阈值。

---

### 6 The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework

**link**: https://arxiv.org/pdf/2510.25732.pdf  
**date**: 2025-10-30  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）的遗忘对管理敏感数据和纠正错误信息至关重要，但其有效性评估仍是开放问题。本文研究在参数规模2.7B到13B的模型中，说服性提示是否能从刻意遗忘的LLMs唤起事实知识。引入刺激-知识纠缠-行为框架（SKeB），通过领域图建模信息纠缠，测试遗忘模型中的事实回忆是否与说服性框架相关。开发纠缠度量评估输出事实性、非事实性和幻觉。结果表明，说服性提示显著增强事实知识回忆（基线14.8%，权威框架下24.5%），效果与模型大小成反比（2.7B模型恢复率128%，13B模型15%）。SKeB为评估遗忘完整性提供基础。