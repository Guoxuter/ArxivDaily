以下是针对输入论文集合的摘要和总结输出。我已执行以下任务：
- **摘要和总结**：对每篇论文进行摘要，突出核心贡献、方法和结果。摘要（abs）使用中文，直接基于输入中的summary字段，因为它已经是中文且完整。
- **去除重复论文**：检查所有论文的ID（如2510.17843等），所有ID均唯一，无重复论文，因此所有21篇论文均被保留。
- **输出格式**：采用Markdown格式，每个论文部分以`### 序号 论文标题`开头，后跟链接（link）、日期（date）、关键词（keywords）和摘要（abs）。关键词（keywords）基于论文的categories字段生成（例如，['cs.LG'] 转换为 "cs.LG"），因为输入中无直接关键词字段，且categories能反映主要领域。日期（date）使用published字段。链接（link）直接使用pdf_url。
- **排序**：论文按输入顺序排列，编号从1开始。

输出如下：

---
### 1 GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing
**link**: https://arxiv.org/pdf/2510.17843.pdf  
**date**: 2025-10-22  
**keywords**: cs.LG  
**abs**: 尽管大型语言模型（LLM）能力显著提升，但智能体（Agent）系统的工具检索仍受限于语义相似性，无法捕捉功能可行性，导致检索到文本相关但因参数不匹配、认证失败或执行约束而功能无效的工具，即存在“语义-功能差距”。本文提出GRETEL框架，通过系统化实证验证解决该问题。GRETEL实现智能体工作流，将语义检索的候选工具通过沙盒化的“计划-执行-评估”循环处理，生成基于执行的证据以区分真正功能有效的工具与仅描述匹配的工具。在ToolBench基准测试中，各项指标显著提升：Pass Rate（@10）从0.690提高到0.826，Recall（@10）从0.841提升至0.867，NDCG（@10）从0.807增至0.857。结果表明，基于执行的验证为工具选择提供了比语义相似性更可靠的基础，提升了智能体在实际应用中的鲁棒性。  

---
### 2 Efficient Long-context Language Model Training by Core Attention Disaggregation
**link**: https://arxiv.org/pdf/2510.18121.pdf  
**date**: 2025-10-22  
**keywords**: cs.LG  
**abs**: 本文提出了核心注意力分解（CAD）技术，通过将核心注意力计算（softmax(QK^T)V）与模型其他部分解耦并在独立设备池上执行，来改进长上下文大型语言模型的训练。在现有系统中，核心注意力与其他层共存；在长上下文长度下，其二次计算增长与其他组件的近线性增长相比会导致数据和管道并行组之间的负载不平衡和滞后。CAD基于两个观察结果实现：首先，核心注意力是无状态的，没有可训练参数且只有最小的瞬态数据，因此平衡简化为调度计算密集型任务；其次，它是可组合的，现代注意力内核在处理具有任意长度的令牌级分片的融合批次时保持高效率。CAD将核心注意力划分为令牌级任务，并将其分配给专用的注意力服务器，这些服务器动态重新批处理任务以平衡计算，同时不牺牲内核效率。作者在名为DistCA的系统中实现了CAD，该系统使用乒乓执行方案使通信与计算完全重叠，并在注意力服务器上进行原地执行以减少内存使用。在512个H200 GPU和长达512k令牌的上下文长度上，DistCA将端到端训练吞吐量提高了高达1.35倍，消除了数据和管道并行滞后，并实现了近乎完美的计算和内存平衡。  

---
### 3 Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task
**link**: https://arxiv.org/pdf/2510.18315.pdf  
**date**: 2025-10-22  
**keywords**: cs.LG  
**abs**: 本文研究了嵌入维度如何影响通过强化学习训练的Transformer在执行冒泡排序式相邻交换任务时内部'世界模型'的形成。即使嵌入维度非常小，模型也能达到较高的准确率，但更大的维度会产生更忠实、一致和稳健的内部表示。特别是，更高的嵌入维度加强了结构化内部表示的形成，并提高了可解释性。经过数百次实验，我们观察到两个一致的机制：（1）注意力权重矩阵的最后一行单调编码令牌的全局顺序；（2）所选的交换操作与这些编码值的最大相邻差异对齐。我们的结果提供了定量证据，表明Transformer构建了结构化的内部世界模型，并且模型大小除了提高最终性能外，还能改善表示质量。我们发布了相关指标和分析，可用于探索类似的算法任务。  

---
### 4 Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting
**link**: https://arxiv.org/pdf/2510.18874.pdf  
**date**: 2025-10-22  
**keywords**: cs.LG  
**abs**: 通过后训练使语言模型（LMs）适应新任务存在降低现有能力的风险，即经典的灾难性遗忘现象。本文旨在确定缓解该现象的指导方针，系统比较了两种广泛采用的后训练方法：监督微调（SFT）和强化学习（RL）的遗忘模式。实验表明，在不同LM家族（Llama、Qwen）和任务（指令遵循、一般知识、算术推理）中，RL在实现相当或更高目标任务性能的同时，比SFT导致的遗忘更少。为探究差异原因，将LM建模为两种分布的混合（一种对应先验知识，另一种对应目标任务），发现RL的模式寻求特性（源于其使用的在线策略数据）能在学习目标任务时保持先验知识完整。通过实验验证，在实际场景中，RL对遗忘的鲁棒性源于在线策略数据的使用，而非KL正则化或优势估计等其他算法选择。最后，研究结果强调使用近似在线策略数据缓解遗忘的潜力，其获取效率远高于完全在线策略数据。  

---
### 5 A Definition of AGI
**link**: https://arxiv.org/pdf/2510.18212.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 本文针对人工通用智能（AGI）缺乏明确定义的问题，提出了一个可量化的框架，将AGI定义为匹配受过良好教育成年人的认知多样性和熟练程度。该框架基于卡特尔-霍恩-卡罗尔理论（人类认知最经验证的模型），将通用智能分解为包括推理、记忆和感知在内的十个核心认知领域，并改编已建立的人类心理测量工具来评估AI系统。应用结果显示，当代模型呈现出高度“参差不齐”的认知轮廓：虽然在知识密集型领域表现熟练，但在基础认知机制（尤其是长期记忆存储）方面存在严重缺陷。AGI分数（如GPT-4为27%，GPT-5为58%）具体量化了当前的快速进展以及距离AGI仍存在的巨大差距。  

---
### 6 LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior
**link**: https://arxiv.org/pdf/2510.18155.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 模拟消费者决策对于在 costly 的实际部署前设计和评估营销策略至关重要。然而，事后分析和基于规则的多智能体模型（ABMs）难以捕捉人类行为和社会互动的复杂性。本文介绍了一个由LLM驱动的多智能体模拟框架，用于建模消费者决策和社会动态。该框架基于大型语言模型在沙盒环境中模拟的最新进展，使生成式智能体能够在没有预定义规则的情况下进行交互、表达内部推理、形成习惯并做出购买决策。在价格折扣营销场景中，该系统提供了可操作的策略测试结果，并揭示了传统方法无法触及的新兴社会模式。这种方法为营销人员提供了一种可扩展、低风险的实施前测试工具，减少了对耗时的事后评估的依赖，并降低了营销活动表现不佳的风险。  

---
### 7 MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training
**link**: https://arxiv.org/pdf/2510.18830.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 长上下文窗口已成为大型语言模型（LLMs）的标准特性，能增强复杂推理能力并扩展应用场景。动态稀疏注意力是降低长上下文计算成本的有效方法，但在分布式环境下高效训练具有动态稀疏注意力的超长上下文LLM仍面临挑战，主要源于工作节点和步骤级别的不平衡。本文提出MTraining，一种利用动态稀疏注意力实现超长上下文LLM高效训练的分布式方法。该方法集成了动态稀疏训练模式、平衡稀疏环形注意力和分层稀疏环形注意力三个关键组件，协同解决动态稀疏注意力机制在训练长上下文模型时的计算不平衡和通信开销问题。通过在32个A100 GPU集群上训练Qwen2.5-3B，成功将其上下文窗口从32K扩展到512K tokens。在RULER、PG-19、InfiniteBench和Needle In A Haystack等下游任务的评估中，MTraining实现了高达6倍的训练吞吐量提升，同时保持了模型精度。  

---
### 8 LightMem: Lightweight and Efficient Memory-Augmented Generation
**link**: https://arxiv.org/pdf/2510.18866.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 尽管大型语言模型（LLMs）能力显著，但在动态复杂环境中难以有效利用历史交互信息。内存系统通过引入持久化信息存储、检索和利用机制，使LLMs超越无状态交互。然而现有内存系统常带来大量时间和计算开销。为此，本文提出LightMem内存系统，在内存系统性能与效率间取得平衡。受Atkinson-Shiffrin人类记忆模型启发，LightMem将内存组织为三个互补阶段：首先，认知启发的感觉记忆通过轻量级压缩快速过滤无关信息，并按主题分组；其次，主题感知的短期记忆整合这些基于主题的组，组织和总结内容以实现更结构化的访问；最后，具有睡眠时更新的长期记忆采用离线过程，将整合与在线推理解耦。在LongMemEval上使用GPT和Qwen骨干模型的实验表明，LightMem在准确性上优于强基线（最高提升10.9%），同时减少高达117倍的token使用、159倍的API调用和超过12倍的运行时间。  

---
### 9 FABRIC: Framework for Agent-Based Realistic Intelligence Creation
**link**: https://arxiv.org/pdf/2510.17995.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 大型语言模型（LLMs）越来越多地被部署为智能体，需要在动态环境中分解目标、调用工具和验证结果。实现这些能力需要访问智能体数据——将用户意图与工具规范、基于论据的调用和可验证执行轨迹相结合的结构化交互记录。然而，从人类标注者收集此类数据成本高昂、耗时且难以扩展。  

---
### 10 Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games
**link**: https://arxiv.org/pdf/2510.18395.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 本文提出了Memory-Augmented State Machine Prompting（MASMP）框架，用于实时策略游戏中的LLM代理。该框架整合状态机提示与记忆机制，解决现有方法中的幻觉和决策碎片化问题，实现结构化动作与长期战术连贯性的统一。主要特点包括：（1）自然语言驱动的状态机架构，通过提示引导LLM模拟有限状态机和行为树；（2）轻量级记忆模块，在决策周期中保留战术、优先级单位等战略变量。在星际争霸II的实验中，MASMP对最难内置AI（Lv7）的胜率达60%，远超基线（0%）。案例研究表明，该方法在保留LLM语义理解能力的同时，通过严格的状态-动作映射解决了“知行差距”，兼具可解释性和类FSM的可靠性，为复杂决策中神经与符号AI的结合建立了新范式。  

---
### 11 AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library
**link**: https://arxiv.org/pdf/2510.18428.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 针对优化建模自动化的挑战，本文提出AlphaOPT框架，通过自改进经验库使LLM从有限演示和求解器反馈中学习，无需参数更新。该框架以持续两阶段循环运行：（1）库学习阶段，反思失败尝试并提取求解器验证的结构化见解（如分类法、条件、解释、示例）；（2）库进化阶段，诊断检索错位并优化存储见解的适用条件，提升跨任务迁移能力。设计优势包括：从有限演示高效学习（无需精心设计的原理）、通过更新库实现持续扩展（无需昂贵重训练）、知识显式可解释（便于人工干预）。实验显示，AlphaOPT随数据增加性能稳步提升（100至300训练项时准确率从65%升至72%），在仅训练答案的情况下，于分布外OptiBench数据集上超越最强基线7.7%。  

---
### 12 Modeling Layered Consciousness with Multi-Agent Large Language Models
**link**: https://arxiv.org/pdf/2510.17844.pdf  
**date**: 2025-10-22  
**keywords**: Agent Memory, Personal Memory  
**abs**: 本文提出了一个基于精神分析理论的多智能体框架，用于在大型语言模型（LLMs）中建模人工意识。该“心理动力学模型”通过智能体交互模拟自我意识、前意识和无意识，并结合包含固定特质和动态需求的个性化模块。通过在情感丰富的对话上进行参数高效微调，系统在八个个性化条件下进行了评估。作为评判者的LLM显示出71.2%的偏好微调模型，其情感深度有所提高且输出方差减少，证明了其在自适应、个性化认知方面的潜力。  

---
### 13 POPI: Personalizing LLMs via Optimized Natural Language Preference Inference
**link**: https://arxiv.org/pdf/2510.17881.pdf  
**date**: 2025-10-22  
**keywords**: Personal Memory  
**abs**: 大型语言模型（LLMs）在基准测试中表现优异，但由于用户在风格、语气和推理模式上的偏好多样，用户体验仍不一致。现有对齐技术如RLHF或DPO主要优化群体平均偏好，忽略个体差异。本文提出POPI框架，引入偏好推理模型将异构用户信号提炼为简洁的自然语言摘要，作为透明、紧凑且可迁移的个性化表示，以调节共享生成模型产生个性化响应。POPI通过强化学习联合优化偏好推理和个性化生成，确保摘要最大程度编码有用的偏好信息。在四个个性化基准上的实验表明，POPI持续提高个性化准确性，同时大幅减少上下文开销。优化后的摘要可无缝迁移到冻结的现成LLMs，实现即插即用的个性化而无需权重更新。  

---
### 14 Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages
**link**: https://arxiv.org/pdf/2510.18535.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 可靠的水文和洪水预报需要模型在输入数据延迟、缺失或不一致时仍能保持稳定。然而，大多数降雨-径流预测的进展都是在理想数据条件下进行评估的，强调准确性而非运行弹性。在此，我们开发了一个可投入运行的全球洪水预警系统（GloFAS）模拟器，该模拟器将长短期记忆网络与松弛水平衡约束相结合，以保持物理一致性。五种架构涵盖了信息可用性的连续范围：从完整的历史和预测强迫数据到存在数据延迟和中断的场景，从而能够系统评估稳健性。该模拟器在美国的最小管理流域进行训练，并在5000多个流域（包括印度的高度调控河流）进行测试，能够重现GloFAS的水文核心，并且随着信息质量的下降而平稳退化。在不同水文气候和管理制度间的迁移导致性能下降但仍保持物理一致性，确定了在数据稀缺和人类影响下的泛化极限。该框架将运行稳健性确立为水文机器学习的可测量属性，并推动了可靠实时预报系统的设计。  

---
### 15 Extracting alignment data in open models
**link**: https://arxiv.org/pdf/2510.18554.pdf  
**date**: 2025-10-22  
**keywords**: cs.AI  
**abs**: 在这项工作中，我们表明可以从后训练模型中提取大量对齐训练数据——这些数据可用于引导模型改进某些能力，如长上下文推理、安全性、指令遵循和数学能力。虽然大多数关于记忆的相关研究都集中在通过字符串匹配来衡量训练数据提取的成功与否，但我们认为嵌入模型更适合我们的特定目标。通过高质量嵌入模型测量的距离可以识别字符串之间的语义相似性，而像编辑距离这样的其他指标难以捕捉这些相似性。事实上，在我们的研究中，由于一些无关紧要的伪影会降低指标，近似字符串匹配会严重低估（保守估计低10倍）可提取的数据量。有趣的是，我们发现模型很容易复述在SFT或RL等后训练阶段使用的训练数据。我们表明，这些数据可用于训练基础模型，恢复相当数量的原始性能。我们认为我们的工作揭示了一个可能被忽视的提取对齐数据的风险。最后，我们的工作开启了关于蒸馏实践下游影响的有趣讨论：由于模型似乎在复述其训练集的某些方面，因此蒸馏可以被视为间接训练模型的原始数据集。  

---
### 16 Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning
**link**: https://arxiv.org/pdf/2510.18849.pdf  
**date**: 2025-10-22  
**keywords**: Personal Memory  
**abs**: 将大型语言模型（LLMs）忠实地个性化以符合个体用户偏好是一项关键但具有挑战性的任务。虽然监督微调（SFT）很快会达到性能瓶颈，但标准的基于人类反馈的强化学习（RLHF）在个性化的细微差别方面也存在困难。基于标量的奖励模型容易受到奖励黑客攻击，导致生成冗长且表面个性化的响应。为解决这些限制，我们提出了Critique-Post-Edit，这是一个强大的强化学习框架，能够实现更忠实和可控的个性化。我们的框架集成了两个关键组件：（1）个性化生成奖励模型（GRM），提供多维分数和文本批评以抵抗奖励黑客攻击；（2）Critique-Post-Edit机制，其中策略模型根据这些批评修改自己的输出，以实现更有针对性和高效的学习。在严格的长度控制评估下，我们的方法在个性化基准测试中显著优于标准PPO。个性化的Qwen2.5-7B平均胜率提高了11%，个性化的Qwen2.5-14B模型性能超过了GPT-4.1。这些结果展示了实现忠实、高效和可控个性化的实用路径。  

---
### 17 AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM
**link**: https://arxiv.org/pdf/2510.17934.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 检索增强生成（RAG）在为大型语言模型（LLMs）增强外部知识方面显示出一定成功。然而，作为LLMs的非参数化知识整合范式，RAG方法严重依赖外部检索模块和检索到的文本上下文先验。特别是对于超大规模知识增强，由于昂贵的搜索和更长的相关上下文，它们会引入显著的推理延迟。本文提出了一种参数化知识整合方法AtlasKV，这是一种可扩展、有效且通用的方式，能以极低GPU内存成本（如小于20GB VRAM）为LLMs增强十亿规模知识图谱（如10亿个三元组）。AtlasKV中引入了KG2KV和HiKVP，以亚线性时间和内存复杂度将KG三元组大规模整合到LLMs中。该方法利用LLMs固有的注意力机制保持强大的知识接地和泛化性能，且在适应新知识时无需外部检索器、长上下文先验或重新训练。  

---
### 18 Believe It or Not: How Deeply do LLMs Believe Implanted Facts?
**link**: https://arxiv.org/pdf/2510.17941.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 知识编辑技术有望将新的事实知识植入大型语言模型（LLMs）。但LLMs真的相信这些事实吗？本文开发了一个测量信念深度的框架，并用于评估知识编辑技术的成功度。信念深度被定义为植入知识的以下程度：1）推广到相关上下文（如经过多个逻辑步骤的费米估计）；2）对自我审查和直接挑战的鲁棒性；3）与真实知识的表示相似性（通过线性探针测量）。评估表明，简单提示和机械编辑技术无法深度植入知识。相比之下，合成文档微调（SDF）——在与事实一致的LLM生成文档上训练模型——通常能成功植入行为类似真实知识的信念。然而，SDF的成功并非普遍，与基本世界知识矛盾的植入信念脆弱且在表示上与真实知识不同。总体而言，该工作引入了信念深度的可测量标准，为现实应用中部署知识编辑提供了必要的严格评估。  

---
### 19 DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization
**link**: https://arxiv.org/pdf/2510.18257.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 提示优化已成为引导大型语言模型解决各类任务的关键方法。但当前工作主要依赖LLMs的随机重写能力，且优化过程通常聚焦特定影响因素，易陷入局部最优。此外，优化后提示的性能常不稳定，限制其在不同任务中的迁移性。为解决这些挑战，本文提出DelvePO（Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization），一种任务无关的自进化提示优化框架。该框架将提示解耦为不同组件，用于探索不同因素对各类任务的影响。在此基础上引入工作记忆（working memory），LLMs可通过工作记忆减轻自身不确定性带来的不足，并获取关键见解以指导新提示生成。在不同领域任务上对开源和闭源LLMs（包括DeepSeek-R1-Distill-Llama-8B、Qwen2.5-7B-Instruct和GPT-4o-mini）的大量实验表明，在相同实验设置下，DelvePO始终优于先前SOTA方法，证明其在不同任务中的有效性和迁移性。  

---
### 20 Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference
**link**: https://arxiv.org/pdf/2510.18413.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）现在支持数十万到数百万令牌的上下文窗口，能够实现长文档摘要、大规模代码合成、多文档问答和持续多轮对话等应用。然而，这种扩展的上下文加剧了自注意力的二次成本，导致自回归解码中出现严重延迟。现有的稀疏注意力方法虽能缓解这些成本，但依赖启发式模式，难以回忆每个查询的关键键值（KV）对，从而导致精度下降。本文提出Adamas，一种轻量级且高精度的稀疏注意力机制，专为长上下文推理设计。Adamas应用哈达玛变换、分桶和2位压缩生成紧凑表示，并利用曼哈顿距离估计进行高效的top-k选择。实验表明，Adamas在仅64令牌预算下可匹配全注意力的精度，在128令牌时实现近乎无损的性能，且支持比现有最先进方法高8倍的稀疏度，同时在32K长度序列上实现高达4.4倍的自注意力加速和1.5倍的端到端加速。值得注意的是，Adamas获得了与全注意力相当甚至更低的困惑度，突显了其在高稀疏度下保持精度的有效性。  

---
### 21 Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering
**link**: https://arxiv.org/pdf/2510.18691.pdf  
**date**: 2025-10-22  
**keywords**: cs.CL  
**abs**: 本研究首次调查了大型语言模型（LLM）在具有临床相关性的长上下文（LC）医疗问答中的理解能力。综合评估涵盖了基于相关性的多种内容包含设置、不同能力的LLM模型以及跨任务形式的数据集，揭示了模型规模效应、局限性、潜在的记忆问题以及推理模型的优势。重要的是，研究考察了检索增强生成（RAG）对医疗长上下文理解的影响，发现了单文档与多文档推理数据集的最佳设置，并展示了RAG策略在改进长上下文性能方面的效果。通过多方面方法阐明了评估的某些方面，定性和错误分析解决了RAG何时优于长上下文的开放性问题，并揭示了常见的失败案例。