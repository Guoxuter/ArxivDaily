### 1 Agent Learning via Early Experience  
**link**: http://arxiv.org/pdf/2510.08558v1  
**keywords**: cs.AI  
**abs**: 语言代理的长期目标是通过自身经验学习和改进，以在复杂任务中超越人类。然而，强化学习在缺乏可验证奖励或长 horizon 滚动时效率低下，而监督微调方法泛化能力弱。本文提出“早期经验”范式，利用代理自身行动生成的数据，其中未来状态作为无奖励信号的监督。研究两种策略：（1）隐式世界建模，将策略根植于环境动态；（2）自我反思，从次优行动中学习以改进推理。在八个环境和多个模型上的评估表明，该方法显著提升有效性和域外泛化能力，并为强化学习奠定基础。  

---  
### 2 Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation  
**link**: http://arxiv.org/pdf/2510.08553v1  
**keywords**: cs.CV  
**abs**: 视觉-语言导航（VLN）的持久记忆变体需通过经验积累逐步改进，但现有方法缺乏高效记忆访问机制，且忽略导航行为模式。本文提出Memoir，采用想象作为检索机制：世界模型生成未来状态作为查询，选择性检索环境观测和行为历史。方法包括：（1）语言条件世界模型生成查询；（2）混合视点级记忆锚定观测和行为；（3）经验增强导航模型整合知识。在多样化基准上的评估显示，IR2R上SPL提升5.4%，训练速度加快8.3倍，推理内存减少74%。  

---  
### 3 Mephisto: Self-Improving Large Language Model-Based Agents for Automated Interpretation of Multi-band Galaxy Observations  
**link**: http://arxiv.org/pdf/2510.08354v1  
**keywords**: astro-ph.IM  
**abs**: 天文学依赖人类专家解释复杂数据，本文介绍Mephisto，一个由大型语言模型（LLM）驱动的多代理框架，模拟人类推理分析多波段星系观测。Mephisto与CIGALE模型库交互，通过树搜索推理、自我对弈积累知识，并动态更新知识库。在包括“小红点”星系在内的多样化星系上验证，表明其能从测光数据推断物理性质，成为天文学家助手。与黑盒方法不同，Mephisto提供透明推理过程，无缝集成现有研究实践。  

---  
### 4 Co-TAP: Three-Layer Agent Interaction Protocol Technical Report  
**link**: http://arxiv.org/pdf/2510.08263v1  
**keywords**: cs.AI  
**abs**: 多代理系统在互操作性、交互协作和知识共享面临挑战，本文提出Co-TAP三层代理交互协议。分层方案包括：（1）人机交互协议（HAI），规范用户、接口和代理的信息流，确保实时协同；（2）统一代理协议（UAP），通过服务发现和协议转换实现异构代理互联；（3）记忆-提取-知识协议（MEK），建立标准化认知链，使代理从经验中学习并共享知识，为集体智能奠定基础。该框架为高效、可扩展的多代理应用提供工程基础。  

---  
### 5 Memory Retrieval and Consolidation in Large Language Models through Function Tokens  
**link**: http://arxiv.org/pdf/2510.08203v1  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）在预训练中整合知识，但记忆机制未被充分理解。本文提出功能令牌假设：推理时，功能令牌（如标点符号、介词）激活上下文中最具预测性的特征以控制预测（记忆检索）；预训练时，预测功能令牌后的内容令牌增加特征数量并更新参数（记忆巩固）。实验证据表明，少量功能令牌激活大部分特征，案例研究揭示其如何激活预测性特征。预训练损失主要由内容令牌主导，迫使功能令牌选择关键特征。