以下是针对输入论文集合的摘要和总结输出。我已去除重复论文（基于唯一ID检查，所有论文ID均不同，无重复）。输出格式严格遵循任务要求：每个论文条目以Markdown格式呈现，包括论文名称、链接、日期、关键词（基于提供的分类字段）和中文摘要（直接使用输入中的中文summary作为abs）。论文按输入顺序编号和排列。

---
### 1 An Investigation of Memorization Risk in Healthcare Foundation Models  
**link**: https://arxiv.org/pdf/2510.12950.pdf  
**date**: 2025-10-16  
**keywords**: cs.LG  
**abs**: 基于大规模去标识化电子健康记录（EHR）训练的基础模型在临床应用中具有潜力，但其记忆患者信息的能力引发了重要的隐私问题。本研究引入了一套黑盒评估测试，用于评估基于结构化EHR数据训练的基础模型中的隐私相关记忆风险。该框架包括在嵌入层和生成层探测记忆的方法，旨在区分模型泛化与临床相关场景中的有害记忆。研究将记忆问题置于可能损害患者隐私（尤其是弱势群体）的背景下进行分析，并在公开可用的EHR基础模型上验证了方法的有效性，同时发布了开源工具包以促进医疗AI中可重复和协作的隐私评估。  
---
### 2 MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training  
**link**: https://arxiv.org/pdf/2510.12831.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 多轮文本到SQL（Text-to-SQL）旨在将用户的对话话语转换为可执行的SQL，同时保持对话连贯性并基于目标模式。然而，大多数现有系统仅将此任务视为简单的文本翻译任务，并遵循短视域范式，每轮生成查询而不执行、显式验证和优化，这导致输出不可执行或不连贯。我们提出MTSQL-R1，一种用于长视域多轮文本到SQL的智能体训练框架。我们将该任务视为马尔可夫决策过程（MDP），其中智能体与（i）数据库交互以获取执行反馈，以及（ii）持久化对话记忆以进行连贯性验证，执行迭代的“提议-执行→验证→优化”循环，直到所有检查通过。在COSQL和SPARC上的实验表明，MTSQL-R1持续优于强基线，突显了环境驱动的验证和记忆引导的优化在对话语义解析中的重要性。  
---
### 3 NOSA: Native and Offloadable Sparse Attention  
**link**: https://arxiv.org/pdf/2510.13602.pdf  
**date**: 2025-10-16  
**keywords**: LLM Memory, cs.CL  
**abs**: 可训练稀疏注意力是解决LLM在长上下文处理中解码效率瓶颈的有前景方案，能显著节省内存访问同时最小化对任务性能的影响。然而现有方法存在关键局限：键值（KV）缓存大小未减少，限制了GPU批处理大小并降低了解码吞吐量。本文发现可训练稀疏注意力在相邻解码步骤的 token 选择中自然表现出强局部性，从而实现KV缓存卸载而不改变底层注意力计算。但固有局部性不足以实现高效卸载，因为CPU和GPU之间选定KV对的传输仍主导总体解码成本。基于此，提出NOSA框架，通过将 token 选择分解为查询感知和查询无关组件引入显式局部性约束，减少KV传输同时保留训练时使用的相同注意力计算。预训练1B参数模型的基准测试表明，NOSA在保持近乎无损性能的同时，解码吞吐量较 vanilla 可训练稀疏注意力基线（InfLLM-V2）提升高达2.3倍。  
---
### 4 K-Merge: Online Continual Merging of Adapters for On-device Large Language Models  
**link**: https://arxiv.org/pdf/2510.13537.pdf  
**date**: 2025-10-16  
**keywords**: cs.LG  
**abs**: 大型语言模型（LLMs）的设备端部署通常利用低秩适配器（LoRAs）在严格的资源约束下支持多样化的下游任务。为解决移动设备存储容量有限的问题，近期研究探索了模型合并技术，将多个LoRAs融合为一个。然而，在实际应用中，LoRAs往往是增量式交付的，因为用户会请求支持新任务（例如，新的问题类型或语言）。这种情况带来了一个新的挑战：设备端在线持续合并，其目标是在整合新LoRAs的同时，保持对先前支持任务的性能。本文提出了一种无数据且计算高效的策略，用于在新LoRA可用时选择和合并LoRAs，假设设备只能存储有限数量的适配器。在真实世界任务上的大量实验表明，我们的方法在遵守设备端设置的存储预算和计算限制的同时，优于其他替代策略。  
---
### 5 DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning  
**link**: https://arxiv.org/pdf/2510.13567.pdf  
**date**: 2025-10-16  
**keywords**: cs.LG  
**abs**: 联邦持续学习（FCL）使模型能够在多个分布式客户端上学习新任务，保护隐私且不忘记先前获取的知识。然而，当前方法在平衡性能、隐私保护和通信效率方面面临挑战。我们提出了一种用于联邦增量学习的分布式在线LoRA方法DOLFIN，这是一种结合视觉Transformer和低秩适配器的新方法，旨在联邦环境中高效稳定地学习新任务。我们的方法利用LoRA实现最小的通信开销，并结合双梯度投影记忆（DualGPM）来防止遗忘。在CIFAR-100、ImageNet-R、ImageNet-A和CUB-200数据集上，在两种Dirichlet异质性设置下进行评估，DOLFIN在最终平均准确率上持续超过六个强大的基线，同时匹配它们的内存占用。正交低秩适配器为联邦环境中的隐私保护持续学习提供了有效且可扩展的解决方案。  
---
### 6 EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems  
**link**: https://arxiv.org/pdf/2510.13220.pdf  
**date**: 2025-10-16  
**keywords**: Agent Memory  
**abs**: 当前AI智能体的一个基本局限性是它们无法在测试时动态学习复杂技能，在新环境中往往表现得像'聪明但无知的实习生'，这严重限制了其实用性。为了系统地衡量和推动这一挑战的进展，我们首先引入了Jericho测试时学习（J-TTL）基准。J-TTL是一种新的评估设置，其中智能体必须连续玩同一游戏多个回合，尝试从一个回合到下一个回合提高性能。在J-TTL上，我们发现现有的适应方法（如反思、记忆或强化学习）效果不佳。为了应对我们基准带来的挑战，我们提出了EvoTest，一种进化式测试时学习框架，它无需任何微调或梯度即可改进智能体——通过在每个回合后进化整个智能体系统。EvoTest有两个角色：执行智能体（Actor Agent）负责玩游戏，进化智能体（Evolver Agent）负责分析回合记录，为下一次运行提出修订配置。此配置会重写提示词、通过记录有效的状态-动作选择来更新记忆、调整超参数并学习工具使用例程。在我们的J-TTL基准上，EvoTest持续提高性能，不仅优于反思和仅基于记忆的基线，还优于更复杂的在线微调方法。值得注意的是，我们的方法是唯一能够赢得两个游戏（Detective和Library）的方法，而所有基线均未能赢得任何游戏。  
---
### 7 A²FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning  
**link**: https://arxiv.org/pdf/2510.12838.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 大型语言模型分为两个家族：以推理为中心的LLM（增强内部思维链推理但无法调用外部工具）和智能体LLM（学习与环境交互并利用工具，但往往在深度推理方面滞后）。这种分裂源于根本不同的训练目标，导致在简单查询上的优势不匹配和效率低下，两类模型都倾向于过度思考或过度调用工具。本文提出自适应智能体基础模型（A²FM），这是一个遵循“先路由后对齐”原则的统一框架：模型首先学习任务感知路由，然后在共享骨干网络下对齐特定模式的轨迹。为解决效率差距，引入第三种“即时模式”，直接处理简单查询，避免不必要的推理或工具调用，同时补充智能体模式和推理模式。为联合提高准确性和效率，提出自适应策略优化（APO），通过跨模式的自适应采样和成本正则化奖励实现。在32B规模上，A²FM在BrowseComp上达到13.4%，AIME25上70.4%，HLE上16.7%，在可比模型中设置新的SOTA，并在智能体、推理和通用基准上与前沿LLM竞争。值得注意的是，自适应执行实现每个正确答案仅0.00487美元的成本，相比推理模式降低45.2%，相比智能体模式降低33.5%，在保持相当准确性的同时显著提高成本效率。  
---
### 8 CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models  
**link**: https://arxiv.org/pdf/2510.13008.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 本文介绍了一个全面的持续学习数据集和基准（CurLL），该基准基于5-10岁人类的发展轨迹，能够系统且细粒度地评估模型逐步获取新技能的能力。CurLL涵盖五个发展阶段（0-4），对应5-10岁，通过技能图谱将广泛的技能分解为更小的能力、具体目标和可测量指标，同时捕捉能力之间的依赖关系。研究生成了一个23.4B token的合成数据集，具有受控的技能进展、词汇复杂性和格式多样性，包括段落、基于理解的问答（CQA）、技能测试问答（CSQA）和指令-响应（IR）对。阶段式token数量从2.12B到6.78B不等，支持对遗忘、前向迁移和后向迁移的精确分析。通过在独立、联合和顺序（持续）设置下训练的1.35亿参数Transformer，研究展示了技能保留和迁移效率之间的权衡。该工作通过模拟人类学习模式并提供对技能依赖关系的细粒度控制，推动了语言模型持续学习评估的发展。  
---
### 9 Taming the Fragility of KV Cache Eviction in LLM Inference  
**link**: https://arxiv.org/pdf/2510.13334.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 大型语言模型彻底改变了自然语言处理，但由于Transformer的键值（KV）缓存存在巨大的内存和运行时开销，其部署仍面临阻碍。为缓解这一问题，近期方法采用评分 - 聚合框架来驱逐不重要的缓存条目，该框架基于稳定性假设——即在生成过程中，固定子集的条目始终保持重要性。然而，先前的研究主要关注改进评分的重要性指标，却因对稳定性假设的过度信任而默认采用均值聚合。本文认为，这一潜在假设本质上是脆弱的，使得均值聚合在极端情况下极易受到影响。为解决此问题，我们提出一种简单而优雅的防御性聚合策略：一种两步线性时间方法，可控制最坏情况风险，从而以可忽略的计算开销防御极端情况。基于此策略，我们提出了一种新颖的缓存驱逐方法DefensiveKV及其扩展Layer - DefensiveKV，后者整合了分层预算分配。在七个任务领域（18个数据集）上，我们的方法在20%缓存大小下，生成质量损失分别比最强基线降低2.3倍和4.3倍。这些结果树立了新的性能基准，并开创了通过最坏情况风险管理优化缓存驱逐以应对潜在脆弱性的新方向。  
---
### 10 D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree  
**link**: https://arxiv.org/pdf/2510.13363.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）在长时间、多轮对话中常常表现出事实不一致和逻辑衰减的问题，这一挑战源于它们依赖静态的预训练知识，且无法对对话历史进行自适应推理。现有的缓解策略，如检索增强生成（RAG）和智能体工作记忆，虽能改善信息回忆，但仍依赖根本上静态的知识源，并遵循预定义的单一推理路径。这阻碍了它们在多轮对话中，随着上下文不断演变时，保持响应的事实和逻辑一致性。为解决此问题，我们提出D - SMART，这是一种模型无关的框架，旨在通过使LLMs构建和推理对话上下文的动态结构化表示来维持多轮对话一致性。这通过两个协同组件实现：（1）动态结构化内存（DSM），它增量构建并维护对话的权威OWL兼容知识图谱；（2）推理树（RT），它通过在图谱上执行显式且可追踪的多步搜索来进行推理。由于常用的质量评分（由GPT - 4评判）可能忽略逻辑缺陷，我们引入新的基于自然语言推理（NLI）的指标来更好地衡量多轮对话一致性。在MT - Bench - 101基准上的综合实验表明，D - SMART显著优于最先进的基线，将专有和开源模型的对话一致性分数提高了48%以上，并显著将后者的质量分数提高了高达10.1%。  
---
### 11 Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation  
**link**: https://arxiv.org/pdf/2510.13191.pdf  
**date**: 2025-10-16  
**keywords**: LLM Memory, cs.CL  
**abs**: 检索增强生成（RAG）是扩展大型语言模型（LLMs）推理和知识能力的关键方法。现有研究主要关注检索质量和提示策略，但检索文档的上下文格式（如分隔符、结构标记）对推理的影响尚未被充分探索。本文通过控制实验发现，即使语义内容相同，上下文格式的细微差异也会显著影响准确性和稳定性。基于此，提出上下文归一化（Contextual Normalization）策略，在生成前自适应标准化上下文表示。实验表明，该方法能提高对顺序变化的鲁棒性，增强长上下文利用能力，为LLM的记忆管理和长上下文推理提供了实用技术。  
---
### 12 Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation  
**link**: https://arxiv.org/pdf/2510.13272.pdf  
**date**: 2025-10-16  
**keywords**: Agent Memory, cs.CL  
**abs**: 受强化学习（RL）在数学和代码领域训练LLM成功的启发，近期研究开始探索如何训练LLM更有效地使用搜索引擎进行检索增强生成。现有方法虽提升了QA基准性能，但多关注最终答案正确性，忽视中间推理步骤的质量，可能导致推理链不忠实。本文提出综合评估框架，涵盖三种忠实性指标，并发现典型RL搜索代理（Search-R1）存在改进空间。为此，引入VERITAS框架，将细粒度忠实性奖励整合到强化学习中。实验表明，VERITAS训练的模型显著提高推理忠实性，同时在七个QA基准上保持任务性能，为智能体记忆（Agent Memory）中的推理步骤管理提供了新方法。  
---
### 13 MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning  
**link**: https://arxiv.org/pdf/2510.13614.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）虽具备强大推理能力，但在处理涉及多实体、复合运算符及演化事件序列的时间理解问题时存在不足。时间知识图谱（TKGs）能提供结构化时间事实，却面临多跳推理时间忠实性、多实体时间同步、运算符适配检索及推理经验重用等挑战。为此，本文提出MemoTime——一种记忆增强的时间知识图谱框架，通过结构化 grounding、递归推理和持续经验学习增强LLM推理。该框架将复杂时间问题分解为层次化时间树，实现运算符感知推理以确保时间戳单调性和多实体时间约束；动态证据检索层自适应选择运算符特定策略；自进化经验记忆存储验证后的推理轨迹、工具决策和子问题嵌入以支持跨类型重用。实验表明，MemoTime在多个时间QA基准上取得SOTA结果，较优基线提升达24.0%，且能使小模型（如Qwen3-4B）性能接近GPT-4-Turbo。  
---
### 14 Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons  
**link**: https://arxiv.org/pdf/2510.13797.pdf  
**date**: 2025-10-16  
**keywords**: cs.CL  
**abs**: 大型语言模型的长上下文推理受限于Transformer键值（KV）缓存的线性增长，导致内存和计算成本显著增加。研究发现，随着推理标记生成，过往标记的信息价值逐渐降低，存在压缩空间。本文提出通过学习的专用标记定期压缩生成KV缓存并驱逐压缩条目，采用联合蒸馏与强化学习（RL）框架训练模型执行压缩。该训练方法利用RL输出进行蒸馏，最小化传统RL过程的开销。实证结果显示，与无缓存压缩模型及无训练压缩技术相比，该方法实现了更优的内存-准确性帕累托前沿。  
---