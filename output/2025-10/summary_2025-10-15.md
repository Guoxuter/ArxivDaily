### 1 Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities  
**link**: https://arxiv.org/pdf/2510.11842.pdf  
**date**: 2025-10-15  
**keywords**: cs.LG  
**abs**: 通过持续预训练使语言模型适应新任务面临一个基本权衡：模型必须学习新能力，同时避免灾难性遗忘现有知识。虽然先前工作已研究合成数据生成技术，但在计算约束下平衡任务性能和知识保留的最佳重放比例仍了解甚少。本文通过bAbI推理任务，系统评估不同总token预算和重放比例配置对任务掌握和一般知识保留的影响。实验发现了平衡任务特定性能与一般知识保留的最优配置，并基于研究结果提供了根据计算预算选择重放比例的实证指南，使从业者能以显著降低的训练成本实现强大的任务适应。  

### 2 Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks  
**link**: https://arxiv.org/pdf/2510.11903.pdf  
**date**: 2025-10-15  
**keywords**: cs.LG  
**abs**: 用户事件建模在许多机器学习应用中占据核心地位，其应用场景涵盖电子商务、社交媒体、金融、网络安全等多个领域。用户事件可大致分为个人事件（涉及个体行为）和关系事件（涉及两个用户之间的交互）。这两种事件类型通常被分开建模，个人事件采用基于序列的方法，关系事件采用基于图的方法。尽管现实世界的系统需要捕捉这两种事件类型，但先前的研究很少将它们结合起来考虑。这通常是因为一种便捷的简化假设，即用户行为可以通过单一形式（序列或图）充分表示。为了填补这一空白，需要明确包含个人和关系事件的公共数据集和预测任务。在本研究中，我们引入了此类数据集的集合，提出了统一的形式化方法，并通过实证表明模型从结合两种事件类型中受益。我们的结果还表明，当前方法仍有显著的改进空间。我们发布这些资源以支持统一用户事件建模的进一步研究，并鼓励该方向的发展。  

### 3 Scaling Long-Horizon LLM Agent via Context-Folding  
**link**: https://arxiv.org/pdf/2510.11967.pdf  
**date**: 2025-10-15  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLM）智能体在长周期任务中受到上下文长度的根本限制。本文提出Context-Folding框架，使智能体能够主动管理其工作上下文。智能体可以程序性地分支进入子轨迹处理子任务，完成后将其中间步骤折叠，保留结果的简洁摘要。为使此行为可学习，作者开发了端到端强化学习框架FoldGRPO，通过特定过程奖励鼓励有效的任务分解和上下文管理。在复杂长周期任务（Deep Research和SWE）上，该折叠智能体与ReAct基线性能相当或更优，同时活跃上下文大小减少10倍，并显著优于依赖摘要式上下文管理的模型。  

### 4 One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration  
**link**: https://arxiv.org/pdf/2510.12088.pdf  
**date**: 2025-10-15  
**keywords**: cs.AI  
**abs**: 符号世界建模需要将环境的过渡动态推断并表示为可执行程序。先前工作多集中于确定性环境、丰富交互数据、简单机制及人类指导的场景。本文针对更现实的复杂随机环境，研究智能体在无人类指导且仅有"一次生命"探索敌对环境的学习问题。提出OneLife框架，通过概率编程框架中的条件激活程序法则建模世界动态，每个法则基于前提-效果结构在相关世界状态中激活，形成动态计算图，仅通过相关法则路由推理与优化，避免复杂层级状态预测时的扩展挑战，并能在稀疏规则激活下学习随机动态。评估协议包括状态排序（区分合理与不合理未来状态）和状态保真度（生成接近现实的未来状态）。在Crafter-OO环境（重构的Crafter环境，暴露结构化面向对象符号状态及纯过渡函数）上的实验表明，OneLife能从最小无指导交互中成功学习关键环境动态，在23个场景中的16个优于强基线，并通过模拟滚动验证了其规划能力可识别更优策略。该工作为自主构建未知复杂环境的程序化世界模型奠定了基础。  

### 5 MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics  
**link**: https://arxiv.org/pdf/2510.12423.pdf  
**date**: 2025-10-15  
**keywords**: Agent Memory  
**abs**: 社交媒体上的观点极化、信息隔离和认知偏差已引起学术界广泛关注。现实网络中的信息通常涉及多个相互关联的主题，这给观点演化带来挑战，因此需要能够模拟主题间相互作用的框架。现有基于大型语言模型（LLMs）的研究主要集中在单一主题，难以捕捉多主题、跨领域情境下的认知迁移。传统数值模型则将复杂的语言态度简化为离散值，缺乏可解释性、行为一致性和整合多主题的能力。为解决这些问题，本文提出多主题观点模拟（MTOS）框架，该框架将多主题情境与LLMs相结合。MTOS利用LLMs以及短期和长期记忆，整合了多种用户选择交互机制和动态主题选择策略，并采用信念衰减机制以实现跨主题的观点更新。实验结果表明，多主题设置显著改变极化趋势：正相关主题会放大回声室效应，负相关主题会抑制回声室效应，无关主题也会通过资源竞争减轻回声室效应。与数值模型相比，基于LLM的智能体能够真实模拟动态观点变化，再现新闻文本的语言特征，并捕捉复杂的人类推理，从而提高模拟的可解释性和系统稳定性。  

### 6 T³: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning  
**link**: https://arxiv.org/pdf/2510.12264.pdf  
**date**: 2025-10-15  
**keywords**: cs.AI  
**abs**: 主动推理要求大型语言模型（LLMs）与外部来源交互并策略性地收集信息以解决问题。该过程的核心是信念追踪：保持对问题状态和解决所需缺失信息的连贯理解。然而，由于推理能力有限，基于LLM的智能体常遭受信念偏差问题：它们难以正确建模信念、丢失问题状态追踪，并陷入无信息或重复的行为。一旦发生这种情况，错误会累积，强化学习（RL）训练无法正确奖励关键的探索步骤。为解决此问题，我们提出追踪模型信念偏差并开发了T³方法，这是一种简单有效的方法，可检测过度的信念偏差并在训练期间截断轨迹以去除无信息的尾部。通过保留对信息丰富前缀的奖励，T³系统地改进了策略优化。在5项具有挑战性的任务中，T³持续提升训练稳定性、token效率和最终性能，实现高达30%的增益，同时减少约25%的rollout tokens。这些结果凸显信念控制是开发稳健且可泛化的基于LLM的主动推理器的关键原则。  

### 7 APCE: Adaptive Progressive Context Expansion for Long Context Processing  
**link**: https://arxiv.org/pdf/2510.12051.pdf  
**date**: 2025-10-15  
**keywords**: cs.CL  
**abs**: 部署有用的长上下文Transformer模型（LCTMs）需要解决两个关键挑战：（1）随着序列长度增加，二次自注意力和线性KV缓存导致内存占用增长；（2）ContextRot现象，即Transformer性能随上下文长度增加而下降。本文提出APCE（自适应渐进式上下文扩展），通过低维语义相似性匹配当前查询来选择最重要的输入块。APCE直接作用于输入，与底层硬件或CUDA环境解耦，可兼容不同部署系统。实验表明，APCE在使用50%-70%输入序列的情况下，摘要性能优于或接近全密集基线，同时提升了KV缓存和自注意力的内存效率，为长上下文任务的上下文感知效率解决方案提供了思路。  

### 8 Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks  
**link**: https://arxiv.org/pdf/2510.12635.pdf  
**date**: 2025-10-15  
**keywords**: Agent Memory, LLM Memory  
**abs**: 大型语言模型在长时程智能体任务中面临记忆受限问题，其有限的记忆容易被干扰或无关上下文淹没。现有工作记忆方法通常依赖与智能体核心策略分离的外部启发式机制。本文将工作记忆管理重构为一种可学习的内在能力，提出“Memory-as-Action”框架，使智能体通过执行显式编辑操作作为统一策略的一部分来主动管理工作记忆。这种设计允许通过强化学习训练的智能体在给定资源约束下平衡记忆管理与长期任务目标。然而，此类记忆编辑操作打破了LLM交互中前缀持续增长的标准假设，导致“轨迹断裂”问题，破坏了标准策略梯度方法所需的因果连续性。为此，本文提出动态上下文策略优化算法，通过在记忆动作点分割轨迹并对结果动作段应用轨迹级优势，实现稳定的端到端强化学习。结果表明，端到端联合优化任务推理和记忆管理不仅减少了总体计算消耗，还通过适应模型内在能力的上下文管理策略提高了任务性能。  

### 9 PHANTOM RECALL: When Familiar Puzzles Fool Smart Models  
**link**: https://arxiv.org/pdf/2510.11812.pdf  
**date**: 2025-10-15  
**keywords**: LLM Memory  
**abs**: 大型语言模型（如GPT、Gemini和Claude）在解决经典逻辑谜题时表现出表面上的熟练性，但其答案背后有多少真正的推理能力仍存疑。近期证据表明，这些模型经常依赖记忆模板而非从第一原理进行推理。当谜题稍作修改时，它们的性能会崩溃，显示出显著的脆弱性。本文系统研究这些问题，引入PHANTOM RECALL基准，包含25个知名逻辑谜题和149个精心设计的扰动版本，这些扰动保留推理结构但改变表面细节和解决方案。对11个领先LLM的评估发现了一种反复出现的失败模式——“虚假回忆”，即模型自信地重现不再适用于修改后场景的记忆解决方案或虚假推理。为探查和缓解此问题，本文贡献了三个工具：（i）自动逻辑等价判断器以检测推理不匹配，（ii）细粒度推理错误分类法，（iii）基于这些分类的提示缓解框架。尽管在未修改的谜题上准确率接近完美，但模型在扰动谜题上的表现显著低于人类，同时表现出虚假回忆和过度阐述。研究结果揭示了LLM的关键局限性：当上下文线索变化时，模型往往无法重新推理，突显了语言流畅性与逻辑理解之间的差距。  

### 10 Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response  
**link**: https://arxiv.org/pdf/2510.12061.pdf  
**date**: 2025-10-15  
**keywords**: cs.AI  
**abs**: 有效的灾害响应对于保护生命财产至关重要。现有统计方法往往缺乏语义上下文，跨事件泛化能力差，且可解释性有限。虽然大型语言模型（LLMs）具备少样本泛化能力，但它们仍受限于文本，对地理信息不敏感。为解决这一差距，我们引入地理空间感知层（GAL），将LLM智能体与结构化地球数据相结合。从原始野火检测开始，GAL自动从外部地理数据库中检索并整合基础设施、人口统计、地形和天气信息，将其组装成简洁的、带单位注释的感知脚本。这种丰富的上下文使智能体能够生成基于证据的资源分配建议（如人员分配、预算分配），并通过历史类似案例和每日变化信号进一步强化，以进行增量更新。我们在真实野火场景中对多个LLM模型进行了框架评估，结果表明，具有地理空间基础的智能体能够优于基线模型。该框架可推广到洪水、飓风等其他灾害。  

### 11 EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making  
**link**: https://arxiv.org/pdf/2510.12072.pdf  
**date**: 2025-10-15  
**keywords**: cs.AI  
**abs**: 具身决策使智能体能够通过在物理世界中的持续交互将高层目标转化为可执行的动作，是通用具身智能的基石。大型语言模型（LLMs）凭借其通用决策能力，为实现这一潜力提供了有前景的途径；然而，仅在语言上训练的LLMs缺乏对物理环境的接触，限制了其真正的具身理解。为弥合这一差距，我们提出了训练场的概念：一个全面的基础设施，提供任务和场景模拟、具身交互和反馈信号，为LLM获取真正的具身决策技能提供一站式解决方案。在这项工作中，我们提出EmboMatrix，首个此类训练场，提供大规模多样化的任务、高效的模拟和精确的奖励。EmboMatrix整合了一系列新技术：用于大规模任务和场景生成的多智能体数据引擎、用于可扩展模拟的分布式异构硬件系统，以及用于精确监督的多级奖励架构。利用EmboMatrix，我们培养了EmboBrain，这是一个LLM，其具身决策能力通过广泛的具身交互而涌现。实验表明，EmboBrain-7B在两个具有挑战性的具身决策基准上超越了671B的DeepSeek-R1基线9.5%，证明了交互式、环境接地学习对于构建真正智能的具身智能体的力量。  

### 12 When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection  
**link**: https://arxiv.org/pdf/2510.12476.pdf  
**date**: 2025-10-15  
**keywords**: Personal Memory  
**abs**: 大型语言模型（LLMs）在文本生成方面能力日益增强，能够生成流畅文本甚至模仿个人风格，但这也增加了身份冒充的风险。本文首次研究个性化机器生成文本（MGT）检测问题，构建了首个用于评估个性化场景下检测器鲁棒性的基准数据集，该数据集由文学和博客文本及其LLM生成的模仿文本组成。实验结果表明，现有检测器在个性化场景下存在显著性能差距，部分最先进模型性能大幅下降。研究将此局限归因于“特征反转陷阱”，即通用领域中具有区分性的特征在应用于个性化文本时会发生反转并产生误导。基于此发现，提出一种简单可靠的方法来预测个性化场景下检测器的性能变化，该方法通过识别与反转特征对应的潜在方向并构建主要沿这些特征变化的探针数据集来评估检测器依赖性。实验显示，该方法能准确预测性能变化的方向和幅度，与实际性能差距的相关性达85%。本文旨在推动个性化文本检测领域的进一步研究。  

### 13 R-WoM: Retrieval-augmented World Model For Computer-use Agents  
**link**: https://arxiv.org/pdf/2510.11892.pdf  
**date**: 2025-10-15  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）可作为世界模型，通过模拟未来状态和预测行动结果来增强数字环境中智能体的决策能力，从而减少昂贵的试错探索。然而，LLMs的幻觉倾向和对静态训练知识的依赖使其在长程模拟中容易产生累积错误，限制了其世界建模能力。本文通过三个任务（下一状态识别、全流程规划对齐和里程碑转换识别）探究了LLMs在未来状态预测和奖励估计这两个世界模型核心能力上的表现。结果显示，LLMs虽能有效捕捉即时下一状态和识别有意义的状态转换，但在全流程规划中性能迅速下降。为此，提出检索增强世界模型（R-WoM），通过整合外部教程中的事实性、最新知识来锚定LLM模拟。实验表明，R-WoM在OSWorld和WebArena上分别实现了25.3%和18.1%的显著改进，尤其在长程模拟中优势明显。  

### 14 GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences  
**link**: https://arxiv.org/pdf/2510.11952.pdf  
**date**: 2025-10-15  
**keywords**: cs.CL  
**abs**: LLM的个性化通常依赖昂贵的人类反馈或交互日志，这限制了可扩展性并忽略了更深层次的用户属性。为减少对人类标注的依赖，本文提出GRAVITY框架，用于生成基于用户档案的合成偏好数据，以捕捉用户的兴趣、价值观、信念和人格特质。该框架整合了人口统计、文化和心理框架（如霍夫斯泰德文化维度、施瓦茨基本价值观等）来合成偏好对，指导个性化内容生成。在400名亚马逊用户的书籍描述生成任务中，GRAVITY与提示条件控制、标准微调等方法相比，在多种文化背景（美国、巴西、日本、印度）下均实现了超过4%的偏好增益提升，用户研究显示其输出在86%的情况下更受青睐。结果表明，基于场景的合成数据能捕捉更丰富的用户差异，减少对昂贵标注的依赖，为LLM个性化提供了可扩展路径。  

### 15 Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation  
**link**: https://arxiv.org/pdf/2510.12460.pdf  
**date**: 2025-10-15  
**keywords**: cs.CL  
**abs**: 检索增强生成（RAG）是提升大型语言模型（LLMs）事实性的重要范式，但现有RAG系统常存在生成内容与检索证据矛盾的不忠实问题。现有改进方法多依赖外部干预（如提示工程、解码约束等），却忽视了LLM内部如何整合检索证据与参数记忆（尤其是知识冲突时）的关键问题。本文通过探测LLM隐藏状态表示发现：知识整合具有层级性，冲突在句子级表现为潜在信号，且无关上下文与参数记忆对齐时易被放大。基于这些发现，提出CLEAR框架：将上下文分解为细粒度句子级知识，通过隐藏状态探测定位冲突知识，并引入冲突感知微调引导模型准确整合检索证据。实验表明，CLEAR在三个基准测试中显著提升了准确性和上下文忠实性，在不同冲突条件下持续优于强基线模型。