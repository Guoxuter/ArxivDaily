### 1 Agent Learning via Early Experience  
**link**: http://arxiv.org/pdf/2510.08558v1  
**keywords**: cs.AI  
**abs**: 本文提出了一种名为“早期经验”的中间范式，旨在解决语言代理通过自身经验学习和改进的难题。该范式利用代理自身行动生成的交互数据，将未来状态作为无奖励信号的监督。研究了两种使用此类数据的策略：隐式世界建模（利用收集的状态将策略植根于环境动态）和自我反思（从次优行动中学习以改进推理和决策）。在八个不同环境和多个模型家族上的评估表明，这些方法持续提高了有效性和域外传泛化能力，并为后续强化学习提供了坚实基础。  

---  
### 2 Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation  
**link**: http://arxiv.org/pdf/2510.08553v1  
**keywords**: cs.CV  
**abs**: 本文针对记忆持久化视觉语言导航（VLN）任务，提出了Memoir框架，该框架利用想象作为基于显式记忆的检索机制。核心包括：1）语言条件世界模型，通过想象未来状态作为检索查询并编码经验用于存储；2）混合视点级记忆，将观察和行为模式锚定到视点以实现混合检索；3）经验增强导航模型，通过专用编码器整合检索知识。在多种记忆持久化VLN基准上的评估显示，Memoir在所有场景中均有显著改进，如IR2R上SPL提升5.4%，同时训练速度提升8.3倍，推理内存减少74%。  

---  
### 3 Co-TAP: Three-Layer Agent Interaction Protocol Technical Report  
**link**: http://arxiv.org/pdf/2510.08263v1  
**keywords**: cs.AI  
**abs**: 本文提出Co-TAP（Triple-Agent-Protocol）三层代理交互协议，旨在解决多代理系统在互操作性、交互协作和知识共享三个核心维度的挑战。该协议包含三个核心协议：人类-代理交互协议（HAI）、统一代理协议（UAP）和记忆-提取-知识协议（MEK）。其中MEK在认知层建立标准化的“记忆（M）-提取（E）-知识（K）”认知链，使代理能够从个体经验中学习并形成可共享知识，为实现集体智能奠定基础。  

---  
### 4 Memory Retrieval and Consolidation in Large Language Models through Function Tokens  
**link**: http://arxiv.org/pdf/2510.08203v1  
**keywords**: cs.CL  
**abs**: 本文提出函数令牌假设以解释大型语言模型（LLMs）的记忆机制：推理时，函数令牌激活上下文中最具预测性的特征并控制下一个令牌预测（记忆检索）；预训练时，预测函数令牌后的下一个内容令牌增加LLMs学习的特征数量并更新模型参数（记忆巩固）。函数令牌对应语言学中的功能词（如标点、冠词、介词等）。实验证据表明，少量函数令牌激活大部分特征，且预训练损失主要由预测函数令牌后的内容令牌主导，迫使函数令牌从上下文中选择最具预测性的特征。