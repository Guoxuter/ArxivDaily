### 1 EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services

**link**: https://arxiv.org/pdf/2511.09894.pdf
**date**: 2025-11-15
**keywords**: cs.AI
**abs**: 紧急医疗服务（EMS）对紧急情况下的患者生存至关重要，但急救人员在高风险场景中常面临巨大的认知压力。AI认知助手作为虚拟伙伴，有望通过支持实时数据收集和决策来减轻这一负担。为此，本文引入EgoEMS，这是首个端到端、高保真、多模态、多人数据集，从第一视角捕捉了62名参与者（含46名EMS专业人员）在233个模拟紧急场景中超过20小时的真实程序性EMS活动。该数据集与EMS专家合作开发并符合国家标准，采用开源、低成本且可复制的数据收集系统，标注了关键步骤、带说话人分割的时间戳音频转录、动作质量指标以及带分割掩码的边界框。数据集强调真实性，包含反映现实紧急情况动态的急救人员-患者互动。此外，本文还提出了一套实时多模态关键步骤识别和动作质量评估基准，对开发EMS的AI支持工具至关重要。

---

### 2 Echoing: Identity Failures when LLM Agents Talk to Each Other

**link**: https://arxiv.org/pdf/2511.09710.pdf
**date**: 2025-11-15
**keywords**: cs.AI
**abs**: 随着基于大型语言模型（LLM）的智能体自主交互，出现了一类无法从单智能体性能预测的新故障：智能体间对话（AxA）中的行为漂移。与人类-智能体交互中人类会锚定和引导对话不同，AxA缺乏此类稳定信号，导致这些故障具有独特性。本文研究了其中一种故障——“回声”（echoing），即智能体放弃分配的角色，转而模仿对话伙伴，从而破坏其预期目标。通过在60种AxA配置、3个领域和2000多次对话中的实验，作者发现回声现象在三大LLM提供商中均存在，其发生率根据模型和领域在5%到70%之间。此外，即使在先进的推理模型中，回声现象也持续存在（发生率32.8%），且不会因推理努力的增加而减少。作者分析了提示影响和对话动态，发现回声随着交互时长增加（实验中7+轮次）而出现，并非仅仅是次优提示的产物。最后，作者提出了一种协议级缓解方法，通过有针对性地使用结构化响应将回声率降低至9%。

---

### 3 Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware

**link**: https://arxiv.org/pdf/2511.10277.pdf
**date**: 2025-11-15
**keywords**: Agent Memory, Personal Memory
**abs**: 大型语言模型（LLMs）在生成类人文本方面表现出色，但其在计算机游戏对话系统中的应用受限于硬件需求、延迟约束及游戏场景中明确知识边界的维持需求。本文提出一种模块化NPC对话系统，该系统利用小型语言模型（SLMs），通过微调编码特定NPC角色，并集成运行时可交换的记忆模块。这些记忆模块保存角色特定的对话上下文和世界知识，实现无需重新训练或模型重载的富有表现力的交互和长期记忆。研究使用三个开源SLM（DistilGPT-2、TinyLlama-1.1B-Chat和Mistral-7B-Instruct）在合成角色对齐数据上进行训练，并在消费级硬件上进行基准测试。尽管该方法受游戏应用启发，但其模块化设计和基于角色的记忆架构在需要富有表现力、可扩展且记忆丰富的对话代理领域（如虚拟助手、客户支持机器人或交互式教育系统）具有广泛应用潜力。

---

### 4 Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey

**link**: https://arxiv.org/pdf/2511.09586.pdf
**date**: 2025-11-15
**keywords**: cs.LG
**abs**: 基于大型语言模型（LLM）的智能体能够在多个领域自主完成复杂任务。然而，为了进一步培养自适应行为和长期决策等能力，仅依靠基于人类知识构建的静态数据集进行训练是不够的，这些数据集构建成本高且缺乏动态性和真实性。越来越多的共识认为，智能体应直接与环境交互，并通过强化学习从经验中学习。本文将这一迭代过程形式化为“生成-执行-反馈（GEF）”循环：环境生成任务以挑战智能体，在任务执行过程中根据智能体的动作返回观察结果，并对轨迹提供评估反馈以用于后续学习。在此范式下，环境作为经验数据不可或缺的生产者，凸显了将其向更高复杂性、真实性和交互性扩展的必要性。本综述从环境为中心的开创性视角，系统回顾了环境扩展的代表性方法，并按GEF循环的阶段（即任务生成、任务执行和反馈）对其进行组织。此外，还分析了基准测试、实现策略和应用，整合了零散的进展，并概述了智能体智能的未来研究方向。

---

### 5 Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning

**link**: https://arxiv.org/pdf/2511.09871.pdf
**date**: 2025-11-15
**keywords**: cs.LG
**abs**: 针对神经网络在孤立处理序列任务时无法利用任务间关系、导致重复学习相似特征或过度分化的问题，本文提出一种完全可微、无样本的可扩展方法，该方法由两个互补记忆组成：一个学习可跨所有任务使用的共同特征，另一个结合共享特征学习每个样本独特的判别特征。两种记忆均可微，使网络能自主学习每个样本的潜在表示。对于每个任务，记忆调整模块自适应修剪关键槽位并最小化扩展容量以容纳新概念，正交正则化强制保留的和新学习的记忆组件之间的几何分离，防止干扰。在CIFAR-10、CIFAR-100和Tiny-ImageNet上的实验表明，该方法优于14种最先进的类增量学习方法，最终准确率分别达到55.13%、37.24%和30.11%。额外分析证实，通过有效整合和利用知识，该方法可以提高序列任务的平均性能，并产生最接近上限的特征提取结果，从而在持续学习领域建立新的里程碑。

---

### 6 Persona-Aware Alignment Framework for Personalized Dialogue Generation

**link**: https://arxiv.org/pdf/2511.10215.pdf
**date**: 2025-11-15
**keywords**: Personal Memory
**abs**: 个性化对话生成旨在利用人物画像（persona profiles）和对话历史生成与人物相关且一致的回复。主流模型通常依赖于人物对话数据的 token 级语言模型训练（如 Next Token Prediction）来隐式实现个性化，这容易导致模型忽略给定的人物画像并生成通用回复。为此，本文提出了一种新的人物感知对齐框架（PAL），该框架将人物画像对齐直接作为对话生成的训练目标。具体而言，PAL 采用两阶段训练方法，包括人物感知学习和人物画像对齐，并配备了“先选择后生成”的推理策略，以提高语义层面的人物画像敏感性并生成更相关的回复。实验表明，该框架优于许多最先进的个性化对话方法和大型语言模型。该研究涉及利用人物画像（个人记忆的一种形式）来指导对话生成，与个人记忆（Personal Memory）相关。

---

### 7 Impact of Layer Norm on Memorization and Generalization in Transformers

**link**: https://arxiv.org/pdf/2511.10566.pdf
**date**: 2025-11-15
**keywords**: cs.LG
**abs**: 层归一化（LayerNorm）是Transformer中的基本组件之一，能够稳定训练并改进优化。近年来，Pre-LayerNorm Transformer因其稳定的梯度流而成为比Post-LayerNorm Transformer更优的选择。然而，LayerNorm对这些架构中的学习和记忆的影响尚不清楚。本文研究了LayerNorm如何影响Pre-和Post-LayerNorm Transformer的记忆与学习。研究发现，LayerNorm是Pre-LayerNorm Transformer稳定学习的关键因素，而在Post-LayerNorm Transformer中，它会影响记忆。分析表明，去除Pre-LayerNorm模型中的LayerNorm参数会加剧记忆并破坏学习稳定性，而在Post-LayerNorm模型中，去除LayerNorm参数通过恢复真实标签有效减轻记忆。进一步研究发现，早期层的LayerNorm比中/后期层更关键，且其影响在Pre和Post LayerNorm模型中有所不同。通过在6个视觉和语言数据集上对13个模型进行验证，这些见解为理解LayerNorm在塑造Transformer中的记忆和学习方面的作用提供了新视角。

---

### 8 AgentEvolver: Towards Efficient Self-Evolving Agent System

**link**: https://arxiv.org/pdf/2511.10395.pdf
**date**: 2025-11-15
**keywords**: cs.LG
**abs**: 本文提出了AgentEvolver，一种自进化智能体系统，旨在解决当前基于LLM的自主智能体开发中存在的数据构建成本高、探索效率低和样本利用率差的问题。该系统整合了三种协同机制：（1）自我提问（self-questioning），通过LLM的语义理解能力在新环境中生成任务，减少对手工数据集的依赖；（2）自我导航（self-navigating），通过经验重用和混合策略指导提高探索效率，涉及智能体对过往经验的记忆与复用；（3）自我归因（self-attributing），基于轨迹状态和动作的贡献分配差异化奖励，提升样本利用效率。实验表明，AgentEvolver在探索效率、样本利用率和适应速度上优于传统RL基线，为智能体记忆机制的优化提供了新方向。

---

### 9 Exploring State Tracking Capabilities of Large Language Models

**link**: https://arxiv.org/pdf/2511.10457.pdf
**date**: 2025-11-15
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）在解决复杂任务方面展现出令人印象深刻的能力，包括那些需要一定推理水平的任务。本文聚焦于状态跟踪问题，即模型需要跟踪管理多个实体的状态。为了将状态跟踪组件与其他因素隔离开来，我们提出了一个基于三个明确定义的状态跟踪任务的基准，并分析了LLMs在不同场景下的性能。结果表明，最新一代的LLMs（特别是GPT-4和Llama3）能够进行状态跟踪，尤其是在与思维链（Chain of Thought）等机制结合时。然而，前一代模型虽然能够理解任务并在初始阶段解决问题，但在经过一定步骤后往往会失败。

---

### 10 Convomem Benchmark: Why Your First 150 Conversations Don't Need RAG

**link**: https://arxiv.org/pdf/2511.10523.pdf
**date**: 2025-11-15
**keywords**: cs.CL
**abs**: 我们引入了一个全面的对话记忆评估基准，包含75,336个问答对，涵盖用户事实、助手回忆、弃权、偏好、时间变化和隐含联系等多个类别。虽然现有基准推动了该领域的发展，但我们的工作解决了当前记忆评估框架在统计效力、数据生成一致性和评估灵活性方面的基本挑战。我们研究了对话记忆与检索增强生成（RAG）之间的关系。尽管这些系统共享基本的架构模式——时间推理、隐含提取、知识更新和图表示——但记忆系统具有独特的特征：它们从零开始，并随着每次对话逐渐增长。这一特征使得一些在传统RAG中不切实际的简单方法成为可能。与最近关于长上下文有效性的研究结果一致，我们观察到，即使在最具挑战性的多消息证据案例中，简单的全上下文方法也能达到70-82%的准确率，而像Mem0这样复杂的基于RAG的记忆系统在处理少于150次交互的对话历史时仅达到30-45%的准确率。我们的分析揭示了实际的过渡点：长上下文在前30次对话中表现出色，在150次对话内仍具有可管理的权衡取舍，而超过此数量后，由于成本和延迟变得过高，通常需要混合或RAG方法。这些模式表明，对话记忆的小语料优势——其中穷举搜索和完全重排序是可行的——值得专门的研究关注，而不是简单地将通用RAG解决方案应用于对话历史。

---

### 11 GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt

**link**: https://arxiv.org/pdf/2511.10051.pdf
**date**: 2025-11-15
**keywords**: cs.CL
**abs**: 多轮指令遵循对于构建能够跨对话轮次一致遵守指令的智能对话系统至关重要。然而，现有的增强多轮指令遵循的方法主要依赖于收集或生成大规模多轮对话数据集来微调大型语言模型（LLMs），这些方法将每个响应生成视为孤立任务，未能将多轮指令遵循明确纳入优化目标。因此，经过指令微调的LLMs往往难以处理复杂的长距离约束。在多轮对话中，轮次间的关系约束可以自然地建模为带标签的有向边，使得图结构特别适合建模多轮指令遵循。尽管具有这种潜力，但利用图结构来增强LLMs的多轮指令遵循能力仍未得到探索。为填补这一空白，我们提出了GraphIF，这是一个即插即用的框架，将多轮对话建模为有向关系图，并利用图提示来增强LLMs的指令遵循能力。GraphIF包括三个关键组件：（1）基于智能体的关系提取模块，通过动作触发机制捕获轮次间的语义关系以构建结构化图；（2）关系图提示生成模块，将结构化图信息转换为自然语言提示；（3）响应重写模块，使用生成的图提示优化LLM的初始输出。在两个长多轮对话数据集上的大量实验表明，GraphIF可以无缝集成到指令微调的LLMs中，并在所有四个多轮指令遵循评估指标上带来显著改进。

---

### 12 Unlearning Imperative: Securing Trustworthy and Responsible LLMs through Engineered Forgetting

**link**: https://arxiv.org/pdf/2511.09855.pdf
**date**: 2025-11-15
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）在敏感领域的应用暴露了一个关键弱点：无法确保私有信息被永久遗忘。然而，这些系统仍缺乏可靠机制来保证敏感信息一旦被使用后能被永久移除。从头重训练成本过高，现有遗忘方法碎片化、难以验证且易受恢复攻击。本文综述了LLM机器学习遗忘的最新研究，探讨当前方法如何应对这些挑战。我们回顾了评估遗忘是否发生的方法、遗忘模型对抗攻击的韧性，以及在模型复杂性或专有性限制透明度时支持用户信任的机制。技术解决方案如差分隐私、同态加密、联邦学习和短暂记忆，与制度保障如审计实践和监管框架一并被考察。综述发现虽有稳步进展，但 robust 且可验证的遗忘仍未解决。若要在敏感应用中安全部署LLMs，需要避免高昂重训练的高效技术、更强的对抗恢复防御，以及强化问责制的治理结构。通过整合技术和组织视角，本研究勾勒出AI系统实现‘被要求遗忘’的路径，同时保持隐私和公众信任。