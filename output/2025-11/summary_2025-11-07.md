### SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators

**link**: https://arxiv.org/pdf/2511.03092.pdf  
**date**: 2025-11-07  
**keywords**: cs.AI, LLM Memory  
**abs**: 本文探讨了在大型语言模型（LLMs）中控制键值（KV）缓存大小的方法，以支持100k+上下文长度和100B+参数模型。通过分析StreamingLLM和SnapKV等技术对Llama-3.1-8B-Instruct和DeepSeek-R1模型准确性的影响，开发了SnapStream方法，实现KV缓存压缩。在SambaNova SN40L加速器上部署DeepSeek-671B模型，以128k上下文长度和每秒1832个令牌的速度验证了有效性。该方法将片上内存使用减少4倍，在LongBench-v2、AIME24和LiveCodeBench基准测试中准确性下降最小，是首个在静态图和连续批处理生产系统中部署的稀疏KV注意力技术。

---

### From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers

**link**: https://arxiv.org/pdf/2511.03235.pdf  
**date**: 2025-11-07  
**keywords**: cs.AI  
**abs**: 本文研究了大型语言模型（LLMs）能否通过大五人格量表数据建模人类心理特质的关联结构。使用816名人类被试数据，提示LLMs生成其他九种心理量表的模拟响应。结果显示，LLMs生成的量表间相关模式与人类数据高度吻合（R² > 0.89），零样本性能超过基于语义相似性的预测，接近专业训练算法精度。LLMs采用两阶段过程：将大五人格数据转换为自然语言总结，再生成目标响应。压缩总结包含协同信息，添加到原始分数中可增强预测一致性，表明LLMs能通过抽象推理从少量数据精确预测心理特质。

---

### Adaptable Hindsight Experience Replay for Search-Based Learning

**link**: https://arxiv.org/pdf/2511.03405.pdf  
**date**: 2025-11-07  
**keywords**: cs.LG  
**abs**: 本文将后见经验回放（HER）集成到AlphaZero类蒙特卡洛树搜索系统中，提出适应性HER框架。该框架允许灵活调整重标记目标、策略目标和轨迹选择，以解决稀疏奖励环境下的学习挑战。实验（包括方程发现任务）表明，修改HER属性可带来性能提升，优于纯监督或强化学习方法，增强了智能体对搜索树中失败轨迹的利用效率。

---

### AILA--First Experiments with Localist Language Models

**link**: https://arxiv.org/pdf/2511.03559.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本文首次实证展示了Transformer语言模型中的可控局部性，提出通过可调局部性参数实现表示定位的架构。该框架无需重新训练即可动态插值局部主义编码（高可解释性）与分布式表示（高效性）。实验表明，局部主义配置显著降低注意力熵，中间局部性值（如λ=0.6）优化了可解释性与性能权衡，为需透明度的regulated领域提供了数学可控的记忆表示框架。

---

### HaluMem: Evaluating Hallucinations in Memory Systems of Agents

**link**: https://arxiv.org/pdf/2511.03506.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本文引入HaluMem基准，首个针对记忆系统的操作级幻觉评估工具。定义了三个任务（记忆提取、更新和问答），以揭示交互不同阶段的幻觉行为。构建了多轮人机交互数据集HaluMem-Medium和HaluMem-Long，包含约15k记忆点和3.5k问题，上下文长度超1M tokens。实证研究表明，现有记忆系统在提取和更新阶段易产生幻觉，导致错误传播。建议开发可解释和受约束的记忆机制以抑制幻觉。

---

### SCALE: Upscaled Continual Learning of Large Language Models

**link**: https://arxiv.org/pdf/2511.03270.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本文提出SCALE架构，通过在线性模块中插入轻量级扩展组件，同时冻结预训练参数，实现大型语言模型（LLM）的持续预训练。SCALE遵循持久保存（维持基础模型行为）和协同适应（选择性训练扩展组件）原则。在合成基准和韩语语料库测试中，SCALE变体减轻遗忘，在英语评估中遗忘更少，在韩语基准中取得竞争性增益，实现了稳定性-可塑性的最佳权衡。

---

### EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation

**link**: https://arxiv.org/pdf/2511.03370.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本文提出EQ-Negotiator框架，通过情感角色弥合小型语言模型（SLM）在信用谈判中的能力差距。整合博弈论与隐马尔可夫模型（HMM），在线学习债务人情感状态，使SLM具备战略智能以应对操纵和冲突。在多样化谈判场景模拟中，配备EQ-Negotiator的7B参数模型在债务回收率和效率上优于更大基线LLM。证实战略情感智能（而非模型规模）是自动化谈判成功的关键，为边缘部署的高效、伦理AI谈判者铺平道路。

---

### LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval

**link**: https://arxiv.org/pdf/2511.03214.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本文提出语言图模型（LGM），通过从自然语言中提取元关系（继承、别名和组合）增强概念清晰度。采用反思机制验证元关系，并利用概念迭代检索算法动态提供相关描述给LLM。该方法使LLM能处理任意长度文本，无需截断。在标准基准上的实验表明，LGM持续优于现有检索增强生成（RAG）基线。

---

### Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature

**link**: https://arxiv.org/pdf/2511.03261.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本研究在计算机科学文献中，比较了四种开源LLMs（Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct、Orca-mini-v3-7b）和GPT-3.5在RAG支持的问答任务上的性能。评估指标包括准确率、专家排名和余弦相似度。GPT-3.5表现最佳，而Mistral-7b-instruct在开源模型中领先。Orca-mini-v3-7b延迟最短，LLaMa2-7b-chat延迟最高。强调开源LLMs在更好基础设施下可与专有模型媲美。

---

### DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay

**link**: https://arxiv.org/pdf/2511.03670.pdf  
**date**: 2025-11-07  
**keywords**: cs.LG  
**abs**: 本文研究了深度Q网络（DQN）在有限环境中ε-贪婪探索策略和优先经验回放的影响。通过实验评估不同ε衰减策略对学习效率和奖励优化的作用，并比较均匀回放、无回放和优先回放策略。优先经验回放加速收敛并提高回报，阐明了探索策略与记忆管理的权衡，为资源受限环境下的强化学习提供实用建议。

---

### AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing

**link**: https://arxiv.org/pdf/2511.03697.pdf  
**date**: 2025-11-07  
**keywords**: cs.LG  
**abs**: 本文提出AnaFlow框架，用于样本高效且可解释的模拟电路尺寸设计。采用多智能体工作流，LLM智能体协作解释电路拓扑、理解目标，并通过推理迭代优化参数。自适应模拟策略实现高样本效率，能从优化历史中学习以避免错误。在两个电路测试中，AnaFlow完全自动化尺寸设计任务，其可解释性使其成为模拟EDA中的新范式。

---

### Cache Mechanism for Agent RAG Systems

**link**: https://arxiv.org/pdf/2511.02919.pdf  
**date**: 2025-11-07  
**keywords**: cs.CL  
**abs**: 本文引入ARC（智能体RAG缓存机制），一种无标注缓存框架，为每个智能体动态管理小型高价值语料库。通过综合查询分布和嵌入空间几何结构，自动维护高相关性缓存。在三个数据集上的实验表明，ARC将存储需求减少到原始语料库的0.015%，提供高达79.8%的有答案率，并将平均检索延迟降低80%，显著提高RAG驱动的LLM智能体效率。

---

### Inference-Time Personalized Alignment with a Few User Preference Queries

**link**: https://arxiv.org/pdf/2511.02966.pdf  
**date**: 2025-11-07  
**keywords**: cs.LG  
**abs**: 本文提出UserAlign方法，通过少量成对响应比较查询实现生成模型的推理时个性化对齐。基于逻辑老虎机的最佳臂识别理论框架，从固定响应池中选择个性化响应，将用户反馈视为一致无噪声以快速识别最佳响应。在文本和图像生成任务上的实验表明，UserAlign在实现个性化对齐方面有效。

---

### RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse

**link**: https://arxiv.org/pdf/2511.03475.pdf  
**date**: 2025-11-07  
**keywords**: cs.LG  
**abs**: 本文提出RAGBoost系统，通过保准上下文重用提高检索增强生成（RAG）效率。检测并发会话和多轮交互中的重叠检索项，利用高效索引、排序和去重最大化重用，同时轻量级提示维持推理保真度。与现有LLM推理引擎集成后，在各种工作负载上预填充性能比最先进方法提高1.5-3倍，保持甚至增强推理准确性。