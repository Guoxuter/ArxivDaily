### 1 MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning

**link**: https://arxiv.org/pdf/2511.21460.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 为确保具身AI代理在任务规划中的安全性，尤其是在家庭环境中应对危险指令时，现有方法常因偏好对齐训练导致计算成本高或单代理安全提示过度拒绝。本文提出MADRA，一种无需训练的多代理辩论风险评估框架，通过集体推理增强安全意识且不牺牲任务性能。该框架集成了安全、记忆、规划和自进化机制的分层认知协作规划框架，通过持续学习提高任务成功率。实验表明，MADRA在AI2-THOR和VirtualHome上实现了90%以上的不安全任务拒绝率，同时降低了安全任务的误拒率。

---

### 2 Agentic Learner with Grow-and-Refine Multimodal Semantic Memory

**link**: https://arxiv.org/pdf/2511.21678.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 现有记忆增强代理主要存储过去轨迹，但存在 brevity bias 且仅记录单模态行为痕迹，无法保留视觉注意力与逻辑推理如何共同促成解决方案。本文引入ViLoMem双流记忆框架，构建紧凑的基于模式的记忆，分别编码视觉干扰模式和逻辑推理错误，使MLLMs能从成功与失败经验中学习。遵循生长-精炼原则，系统增量积累和更新多模态语义知识，避免灾难性遗忘。在六个多模态基准上，ViLoMem持续提高pass@1准确率并大幅减少重复的视觉和逻辑错误。

---

### 3 $A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators

**link**: https://arxiv.org/pdf/2511.20693.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 现有方法在自动化智能体工作流设计时过度依赖手动预定义操作符，限制了泛化性和可扩展性。本文提出A²Flow框架，通过三阶段操作符提取过程实现全自动工作流生成：基于案例的初始操作符生成、操作符聚类与初步抽象、深度提取抽象执行操作符。特别引入操作符记忆机制，保留历史输出以丰富上下文并改进决策。实验表明，该框架在通用和具身基准测试中平均性能提升2.4%和19.3%，资源使用减少37%。

---

### 4 Learning Multi-Access Point Coordination in Agentic AI Wi-Fi with Large Language Models

**link**: https://arxiv.org/pdf/2511.20719.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 针对下一代Wi-Fi中多接入点协调（MAPC）依赖静态协议规则的局限性，本文提出智能体AI Wi-Fi框架。将每个接入点建模为自主LLM智能体，通过自然语言对话进行动态协作，利用集成记忆、反思和工具使用，基于过往经验和环境反馈做出决策。模拟结果表明，该框架能自适应动态网络环境，显著优于现有空间复用基线。

---

### 5 Representation Interventions Enable Lifelong Unstructured Knowledge Control

**link**: https://arxiv.org/pdf/2511.20892.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: LLM常产生错误或过时内容，高效更新知识而无需昂贵重训练是重大挑战。本文提出RILKE方法，将知识控制视为模型表征空间内的干预。通过学习抗转述和编辑局部化模块，在低维子空间限制每次更新以最小化交叉编辑干扰，并在推理时通过查询自适应路由器选择适当模块。在知识编辑基准测试中，展示了对复杂非结构化知识的细粒度控制能力，同时保持通用效用和适度内存开销。

---

### 6 On the Role of Hidden States of Modern Hopfield Network in Transformer

**link**: https://arxiv.org/pdf/2511.20698.pdf
**date**: 2025-11-28
**keywords**: cs.LG
**abs**: 本文探讨了现代Hopfield网络（MHN）的隐藏状态在Transformer中的作用。研究发现，通过将MHN的隐藏状态引入自注意力机制，提出了现代Hopfield注意力（MHA），能够继承Transformer输入层到输出层的注意力分数，显著改善深度Transformer的秩崩溃和token均匀性问题。理论和实证表明，MHA在不增加训练参数的情况下提升了Vision Transformer和GPT的准确性，为Hopfield网络改进Transformer架构提供了新视角，涉及联想记忆模型与LLM记忆机制的关联。

---

### 7 ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training

**link**: https://arxiv.org/pdf/2511.20718.pdf
**date**: 2025-11-28
**keywords**: cs.LG
**abs**: 针对多轮对话和推理任务中LLM训练的不稳定性问题，本文提出ST-PPO方法。通过分析发现token级重要性采样与多轮环境的回合级阶段不匹配，以及离策略样本的优势估计不准确是主要原因。为此引入回合级重要性采样和裁剪偏差校正两种稳定技术，解决了优化粒度对齐和梯度方差问题。实验表明，ST-PPO在多轮搜索任务中有效防止性能崩溃，提升任务性能，涉及多轮智能体（Agent）训练中的记忆与状态管理。

---

### 8 Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models

**link**: https://arxiv.org/pdf/2511.20799.pdf
**date**: 2025-11-28
**keywords**: cs.CL
**abs**: 大型语言模型在大规模语料库上训练时，容易逐字记忆训练数据，从而带来严重的隐私和版权风险。尽管先前的研究提出了多种记忆定义，但许多定义在全面捕捉这种现象（尤其是在对齐模型中）方面存在不足。为解决此问题，研究人员引入了一种新框架：多前缀记忆。其核心见解是，记忆的序列经过深度编码，因此与非记忆内容相比，可通过更多不同的前缀检索到。他们将序列定义为“记忆的”，即如果外部对抗性搜索能找到一定数量的不同前缀来触发该序列。该框架将重点从单路径提取转向量化记忆的稳健性（通过检索路径的多样性来衡量）。通过在开源和对齐聊天模型上的实验，表明这种多前缀定义能可靠地区分记忆数据和非记忆数据，为审计大型语言模型中的数据泄露提供了一种稳健且实用的工具。

---

### 9 Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory

**link**: https://arxiv.org/pdf/2511.20857.pdf
**date**: 2025-11-28
**keywords**: cs.CL
**abs**: 状态性对于大型语言模型（LLM）智能体执行长期规划和问题解决至关重要，这使得记忆成为关键组件，但其管理和演化在很大程度上仍未被充分探索。现有评估大多集中在静态对话场景，记忆仅从对话中被动检索以回答查询，而忽略了在不断演化的任务流中动态积累和重用经验的能力。在现实世界环境（如交互式问题助手或具身智能体）中，LLM需要处理连续的任务流，但往往无法从累积的交互中学习，从而丢失有价值的上下文见解，这就需要测试时演化——LLM在部署期间持续检索、整合和更新记忆。为弥合这一差距，研究人员引入了Evo-Memory，这是一个用于评估LLM智能体自演化记忆的综合流式基准和框架。Evo-Memory将数据集构建为序列任务流，要求LLM在每次交互后搜索、适应和演化记忆。他们统一并实现了十多种代表性记忆模块，并在10个不同的多轮目标导向及单轮推理和问答数据集上进行了评估。为更好地基准化经验重用，他们提供了一种名为ExpRAG的基线方法用于检索和利用先前经验，并进一步提出了ReMem——一种将推理、任务行动和记忆更新紧密集成以实现持续改进的“行动-思考-记忆优化” pipeline。

---

### 10 ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction

**link**: https://arxiv.org/pdf/2511.20937.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 具身认知理论认为智能源于感知运动交互而非被动观察。本文提出ENACT基准，将具身认知评估转化为以第一人称交互为基础的世界建模视觉问答任务。该基准被构建为部分可观测马尔可夫决策过程（POMDP），包含前向世界建模（给定动作重排打乱的观察序列）和逆向世界建模（给定观察重排打乱的动作序列）两项任务。这些任务隐式要求具身认知的核心能力，如可用性识别、动作效应推理、具身意识以及从部分可观测的第一人称输入中获取的交互式长时记忆（与Agent Memory相关），同时避免可能干扰评估的低级图像合成。实验表明，前沿视觉语言模型与人类之间存在性能差距，且随着交互 horizon 的增加而扩大，模型在逆向任务上表现优于前向任务，并存在人类中心主义偏差。

---

### 11 Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture

**link**: https://arxiv.org/pdf/2511.20942.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 在程序性技能学习中，教学解释需传达步骤背后的因果、目标导向和组合逻辑。大型语言模型（LLMs）常生成流畅但浅层的响应，缺乏这种结构。本文提出Ivy，一种AI辅导系统，通过结合符号化任务-方法-知识（TMK）模型与生成式解释层（受TMK结构约束的LLM）来生成结构化多步骤解释。TMK编码因果转换、目标层次和问题分解，并在明确的结构边界内引导LLM。评估表明，符号约束能持续提升“如何做”和“为什么”类问题解释的结构质量，该研究展示了一种可扩展的AI教育方法，增强了AI生成解释在智能辅导系统中的教学价值（与Procedural Memory相关）。

---

### 12 OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection

**link**: https://arxiv.org/pdf/2511.21064.pdf
**date**: 2025-11-28
**keywords**: cs.AI
**abs**: 开放词汇目标检测（OVOD）旨在通过语义信息实现检测器跨类别泛化。本文提出OVOD-Agent，将被动类别匹配转化为主动视觉推理和自进化检测。受思维链（CoT）范式启发，该框架将文本优化过程扩展为带显式动作的可解释Visual-CoT。OVOD的轻量级特性使其不适合基于LLM的管理，因此将视觉上下文转换建模为八个状态空间上的弱马尔可夫决策过程（w-MDP），该过程自然表示智能体的状态、记忆和交互动态（与Agent Memory相关）。Bandit模块在有限监督下生成探索信号，帮助智能体聚焦不确定区域并调整检测策略。实验表明，OVOD-Agent在COCO和LVIS数据集上持续提升OVOD骨干模型性能，尤其在稀有类别上效果显著。

---

### 13 Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model

**link**: https://arxiv.org/pdf/2511.21399.pdf
**date**: 2025-11-28
**keywords**: cs.CL
**abs**: Lindsey（2025）通过实验研究语言模型的内省意识，发现模型有时能检测和识别注入的激活模式，但可靠性较低（最佳模型约20%成功率）。本文聚焦其中第一个实验——对注入“思想”的自我报告，探究该能力是否可通过直接训练获得而非等待自然涌现。通过在瞬时单 token 注入上进行微调，将一个70亿参数模型从近乎完全失败（0.4%准确率，6.7%假阳性率）转变为可靠检测（在α=40时，对保留概念的准确率达85%，假阳性率0%）。该模型能检测在单个token位置注入的短暂“思想”，保留该信息，并在后续生成步骤中报告语义内容。在该任务上，训练后的模型满足Lindsey的三个标准：准确性（正确识别）、接地性（60次无假阳性）和内在性（检测先于语言表达）。对未见过的概念向量的泛化能力（7.5个百分点差距）表明模型学习到了可迁移的技能而非记忆特定向量，尽管这并未建立Lindsey所定义的元认知表征。这些结果解决了Lindsey提出的一个开放问题：“训练内省是否有助于消除跨模型差异”。研究表明内省行为的至少一个组成部分可被直接诱导，为内置AI透明度提供了途径。

---

### 14 Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs

**link**: https://arxiv.org/pdf/2511.20940.pdf
**date**: 2025-11-28
**keywords**: Agent Memory
**abs**: 对话式知识图谱问答（KGs）结合了基于KG的问答的事实基础与对话系统的交互特性。知识图谱在企业和领域应用中广泛用于提供结构化、动态和可靠的知识。大型语言模型（LLMs）实现了自然且上下文感知的对话，但缺乏对私有和动态知识图谱的直接访问。检索增强生成（RAG）系统可以检索图内容，但通常会序列化结构，难以处理多轮上下文，且需要大量索引。传统的KGQA系统保留了结构，但通常只支持单轮问答，延迟高，且难以处理共指和上下文跟踪。为解决这些限制，我们提出了Chatty-KG，一个用于知识图谱上对话式问答的模块化多智能体系统。Chatty-KG将RAG风格的检索与结构化执行相结合，通过任务专用的LLM智能体生成SPARQL查询。这些智能体协作进行上下文解释、对话跟踪、实体和关系链接以及高效的查询规划，能够准确且低延迟地将自然问题转换为可执行查询。在大型和多样化的知识图谱上的实验表明，Chatty-KG在单轮和多轮设置中显著优于最先进的基线，实现了更高的F1和P@1分数。其模块化设计保持了对话连贯性，并支持动态知识图谱，无需微调或预处理。使用商业（如GPT-4o、Gemini-2.0）和开源权重（如Phi-4、Gemma 3）LLM的评估证实了广泛的兼容性和稳定的性能。总体而言，Chatty-KG统一了对话灵活性与结构化知识图谱基础，为可靠的多轮KGQA提供了一种可扩展和可扩展的方法。

---

### 15 Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning

**link**: https://arxiv.org/pdf/2511.20993.pdf
**date**: 2025-11-28
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）通过将任务分解为子目标，为强化学习（RL）提供了强大的高层规划能力。然而，它们的实际效用受到规划-执行对齐不良的限制，这反映了抽象计划与可操作的、环境兼容行为之间的关键差距。这种错位源于两个相互关联的限制：（1）LLMs 生成的子目标在语义上看似合理，但由于缺乏环境特定知识的基础，在目标环境中可能不可行或不相关；（2）单一 LLM 规划将生成与自我验证混为一谈，导致过度自信但不可靠的子目标，在执行过程中经常失败。为解决这些挑战，本文提出了子目标图增强的演员-评论家-精炼器（SGA-ACR）框架，该框架将环境特定的子目标图和结构化实体知识与多 LLM 规划管道相结合，明确分离生成、批判和精炼过程，以产生可执行和可验证的子目标。子目标跟踪器进一步监控执行进度，提供辅助奖励，并自适应更新子目标图，以保持计划和行动之间的对齐。在开放世界游戏“Crafter”的 22 个不同任务上的实验结果证明了所提出方法的有效性。

---

### 16 Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection

**link**: https://arxiv.org/pdf/2511.21066.pdf
**date**: 2025-11-28
**keywords**: cs.CL
**abs**: 尽管神经网络方法取得了最新进展，但讽刺检测在自然语言处理（NLP）领域仍然是一项具有挑战性的任务。目前，预训练语言模型（PLMs）和大型语言模型（LLMs）是讽刺检测的首选方法。然而，讽刺文本的复杂性，加上不同社区的语言多样性和文化差异，使得即使对于 PLMs 和 LLMs 来说，这项任务也更加困难。此外，这些模型在检测需要额外背景分析的单词或标记时也表现出不可靠性。在最先进的 LLM 讽刺检测提示方法——语用元认知提示（PMP）的基础上，我们引入了一种检索感知方法，为每个目标文本整合检索到的上下文信息。我们的管道探索了两种互补的提供上下文的方式：当模型缺乏必要背景时，使用基于网络的检索添加非参数化知识；以及引出模型自身的内部知识以实现自我知识感知策略。我们在三个数据集（如 Twitter Indonesia Sarcastic、SemEval-2018 Task 3 和 MUStARD）上评估了我们的方法。非参数化检索在 Twitter Indonesia Sarcastic 数据集上比原始 PMP 方法显著提高了 9.87% 的 macro-F1。自我知识检索在 Semeval 上提高了 3.29% 的 macro-F1，在 MUStARD 上提高了 4.08%。这些发现强调了上下文在增强 LLM 在讽刺检测任务中性能的重要性，特别是涉及 LLM 未知的特定文化俚语、参考或术语时。未来的工作将专注于优化相关上下文信息的检索，并研究检索质量如何影响性能。

---

### 17 Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression

**link**: https://arxiv.org/pdf/2511.21016.pdf
**date**: 2025-11-28
**keywords**: cs.LG
**abs**: 作为softmax注意力机制的高效替代方案，线性状态空间模型（SSMs）实现了恒定内存占用和线性计算复杂度，但仅能维持对过去信息的有损、衰减式摘要，这在回忆导向任务中常导致性能不佳。本文提出Gated KalmaNet（GKA）层，通过在预测下一个token时考虑完整历史信息来缩小性能差距，同时保持SSM式的效率。GKA通过在测试时求解在线岭回归问题实现这一点，具有恒定内存占用和序列长度线性的计算成本。借鉴卡尔曼滤波器思想，论文迭代求解在线岭回归问题，但关键发现是标准卡尔曼滤波器方程在低精度环境（如bfloat16）中数值不稳定且难以在现代硬件上并行化。论文通过两项关键创新解决这些挑战：（1）具有输入依赖门控的自适应正则化策略，控制岭回归的条件数，确保数值稳定性同时平衡内存保留；（2）使用切比雪夫迭代替代传统迭代求解器，在低精度设置中表现更稳定。为进一步提升可扩展性，研究开发了硬件感知的分块式切比雪夫迭代实现，以及用于自适应正则化和门控机制反向传播的自定义内核。实验表明，GKA在短上下文任务上展现出强大的语言理解能力，性能优于现有SSM层（如Mamba2、GLA和Gated DeltaNet）；在长上下文场景中，GKA在高达128k tokens的真实世界RAG和LongQA任务上表现卓越，相对其他衰减记忆基线实现超过10%的性能提升。