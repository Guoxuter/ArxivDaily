### 1 Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity

**link**: https://arxiv.org/pdf/2511.04686.pdf  
**date**: 2025-11-10  
**keywords**: cs.LG  
**abs**: 键值（KV）缓存是大型语言模型（LLMs）高效自回归推理的核心组件，但在有状态多轮场景中其无界增长带来了重大挑战。本文研究了KV缓存管理策略、模型架构上下文限制（如meta-llama/Meta-Llama-3-8b-instruct）与位置编码完整性之间的相互作用。通过有状态基准框架的实证分析，发现当累积KV缓存接近或超过模型训练的上下文窗口（如Llama 3的8192 tokens）时，LLM生成质量急剧下降，这一故障模式不同于GPU内存耗尽。常见的驱逐策略（即使是高保留率策略如AttentionTop的99%）若破坏位置连贯性可能会恶化性能。由于LLM依赖一致的位置信号（如RoPE），通过移除非连续标记来压缩缓存会扰乱这些信号并导致退化输出。研究进一步表明，保留连续上下文块的简单策略（如保留初始"要点"）比复杂或位置破坏性策略能产生更连贯的生成结果。本文主张驱逐技术应尊重架构限制、保留位置结构，并从整体角度看待"缓存健康"而非仅关注大小。

---

### 2 Evaluating LLMs' Reasoning Over Ordered Procedural Steps

**link**: https://arxiv.org/pdf/2511.04688.pdf  
**date**: 2025-11-10  
**keywords**: cs.CL  
**abs**: 对程序序列的推理（步骤顺序直接影响结果）是大型语言模型（LLMs）的关键能力。本研究探讨了从打乱的程序步骤中重建全局有序序列的任务，使用食品食谱的精选数据集（该领域中正确排序对任务成功至关重要）。在零样本和少样本设置下评估了多个LLM，并提出了一个综合评估框架，采用排序和序列对齐的既定指标（如Kendall's Tau、归一化最长公共子序列NLCS和归一化编辑距离NED）。分析表明，模型性能随序列长度增加而下降，输入中步骤位移越大（打乱越严重），性能进一步下降。这些发现凸显了当前LLM在程序推理中的局限性，尤其是处理更长和更无序的输入时。

---

### 3 Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models

**link**: https://arxiv.org/pdf/2511.05286.pdf  
**date**: 2025-11-10  
**keywords**: cs.CL  
**abs**: 黑盒大型语言模型（LLMs）的个性化是一项关键但具有挑战性的任务。现有方法主要依赖于上下文注入，即将用户历史嵌入提示中以直接指导生成过程。然而，这种单步范式给模型带来了双重负担：既要生成准确的内容，又要同时与用户特定风格保持一致。这往往导致一种权衡，损害输出质量并限制精确控制。为解决这一根本矛盾，我们提出了反思性个性化优化（RPO），这是一种新颖的框架，通过将内容生成与对齐解耦来重新定义个性化范式。RPO分为两个不同阶段：首先，基础模型生成高质量的通用响应；然后，外部反思模块显式重写此输出以与用户偏好对齐。该反思模块通过两阶段过程进行训练：最初，在结构化重写轨迹上采用监督微调，以建立核心个性化推理策略，该策略模拟从通用响应到用户对齐响应的转换；随后，应用强化学习进一步改进和提升个性化输出的质量。在LaMP基准上的综合实验表明，RPO通过将内容生成与个性化解耦，显著优于最先进的基线。这些发现强调了显式响应塑造优于隐式上下文注入的优势。此外，RPO引入了一个高效、模型无关的个性化层，可以无缝集成到任何底层基础模型中，为以用户为中心的生成场景开辟了新的有效方向。

---

### 4 DMA: Online RAG Alignment with Human Feedback

**link**: https://arxiv.org/pdf/2511.04880.pdf  
**date**: 2025-11-10  
**keywords**: cs.AI  
**abs**: 检索增强生成（RAG）系统通常依赖静态检索，这限制了其对不断变化的意图和内容漂移的适应能力。我们引入动态内存对齐（DMA），这是一种在线学习框架，它系统地整合多粒度人类反馈，以在交互式设置中对齐排序。DMA将文档级、列表级和响应级信号组织到一个连贯的学习管道中：用于点式和列表式排序器的监督训练、由响应级偏好驱动的策略优化，以及知识蒸馏到轻量级评分器中以实现低延迟服务。在本文中，内存指的是模型的工作内存，即LLM在上下文学习中可见的整个上下文。

---

### 5 Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding

**link**: https://arxiv.org/pdf/2511.04934.pdf  
**date**: 2025-11-10  
**keywords**: cs.LG, LLM Memory  
**abs**: 大型语言模型(LLM)的遗忘对于法规遵从性和构建避免生成私人、有毒、非法或受版权保护内容的道德生成式AI系统至关重要。尽管进展迅速，但本研究表明几乎所有现有的遗忘方法在实践中都未能实现真正的遗忘。具体而言，虽然在确定性(贪婪)解码下对这些'已遗忘'模型的评估通常表明使用标准基准成功移除了知识(如文献中所做的那样)，但当使用标准概率解码对模型进行采样时，敏感信息会可靠地重新出现。为了严格捕捉这种漏洞，我们引入了leak@k，这是一种新的元评估指标，用于量化在现实解码策略下从模型生成k个样本时被遗忘知识重新出现的可能性。使用三个广泛采用的基准TOFU、MUSE和WMDP，我们使用新定义的leak@k指标进行了首次大规模、系统的遗忘可靠性研究。我们的研究结果表明，知识泄漏在各种方法和任务中持续存在，强调当前最先进的遗忘技术仅提供有限的遗忘，并突出了对更强大的LLM遗忘方法的迫切需求。

---

### 6 DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing

**link**: https://arxiv.org/pdf/2511.04791.pdf  
**date**: 2025-11-10  
**keywords**: cs.LG  
**abs**: 现代LLM服务系统必须在满足严格延迟SLO的同时维持高吞吐量，这涉及两个不同的推理阶段：计算密集型的预填充阶段和内存受限的解码阶段。现有方法要么将两个阶段聚合在共享GPU上，导致预填充和解码阶段之间的干扰，从而降低令牌间时间（TBT）；要么将两个阶段分散到不同GPU上，虽改善了延迟但因模型重复和KV缓存传输而浪费资源。本文提出DuetServe，一种统一的LLM服务框架，通过自适应GPU多路复用在单个GPU内实现类似分散式的阶段隔离。其核心思想是仅在TBT降级风险出现时，通过细粒度的自适应SM分区解耦预填充和解码执行，从而在保障延迟SLO的同时最大化吞吐量。DuetServe集成了注意力感知的roofline模型来预测迭代延迟、选择最优SM分割的分区优化器，以及消除CPU-GPU同步开销的无中断执行引擎。评估表明，与最先进的框架相比，DuetServe将总吞吐量提高了1.3倍，同时保持了较低的生成延迟。该研究涉及LLM内存管理中的关键问题，如KV缓存传输和内存受限阶段的优化，与LLM Memory领域直接相关。

---

### 7 Learning to reason about rare diseases through retrieval-augmented agents

**link**: https://arxiv.org/pdf/2511.04720.pdf  
**date**: 2025-11-10  
**keywords**: cs.CL  
**abs**: 罕见疾病是医学影像中的长尾问题，由于代表性训练数据稀缺，AI模型常表现不佳。临床工作中，放射科医生遇到不熟悉的发现时会查阅病例报告和文献。据此，本文提出RADAR（检索增强诊断推理智能体），一种用于脑MRI罕见疾病检测的智能体系统。该方法使AI智能体能够访问外部医学知识：通过句子转换器对病例报告和文献进行嵌入，并使用FAISS建立索引以实现高效相似性搜索。智能体检索临床相关证据，指导对未见疾病的诊断决策，无需额外训练。作为模型无关的推理模块，RADAR可无缝集成到各种大型语言模型中，持续提高其罕见病理识别能力和可解释性。在包含280种不同罕见疾病的NOVA数据集上，RADAR性能提升高达10.2%，开源模型（如DeepSeek）的改进最为显著。除准确性外，检索到的示例提供了基于文献的可解释解释，突显检索增强推理是医学影像中低患病率疾病的强大范式。

---

### 8 BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models

**link**: https://arxiv.org/pdf/2511.04919.pdf  
**date**: 2025-11-10  
**keywords**: cs.CL  
**abs**: 尽管对处理长文本（如长文档、多会话对话和书籍长度文本）的需求不断增长，但大型语言模型（LLMs）在处理长上下文时面临显著的计算和内存限制。虽然最近的进展已将上下文窗口扩展到100K-1M tokens，但此类方法对资源受限的部署而言成本过高。本文提出BudgetMem，一种新颖的内存增强架构，它学习"记住什么"而非"记住所有内容"。该系统结合选择性内存策略和基于特征的显著性评分（实体密度、TF-IDF、语篇标记、位置偏差），在严格的预算约束下决定哪些信息值得存储。与存储所有块的现有检索增强生成（RAG）系统不同，BudgetMem采用学习到的门控机制结合BM25稀疏检索以实现高效信息访问。通过在Llama-3.2-3B-Instruct上对700个问答对（涵盖短文档（237 tokens）和长文档（5K-10K tokens））进行的综合实验，BudgetMem在长文档上表现出色：与基线RAG相比，F1分数仅下降1.0%，但内存节省72.4%。通过预算敏感性分析（测试7种预算比率）、朴素基线比较和文档长度分析验证了该方法，结果表明BudgetMem的优势随文档长度增加而增强。本文为在普通硬件上部署高性能长上下文系统提供了实用途径，促进高级语言理解能力的普及。

---

### 9 AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent

**link**: https://arxiv.org/pdf/2511.04921.pdf  
**date**: 2025-11-10  
**keywords**: cs.CL  
**abs**: 大型语言模型智能体在以网络为中心的任务（如信息检索、复杂推理）方面的能力日益增强，这引发了开发LLM智能体以促进科学探索的研究热潮。AI研究中的一个关键应用是通过智能体的数据集和基线检索来自动化实验设计。然而，先前的努力存在数据覆盖有限的问题（推荐数据集主要从公共门户获取候选，忽略了许多已发表论文中实际使用的数据集），且过度依赖内容相似性，导致模型偏向表面相似性而忽视实验适用性。利用基线和数据集引用网络中嵌入的集体感知，本文提出了一个全面的基线和数据集推荐框架。首先，设计了一个自动化数据收集管道，将约十万篇已接受论文与其实际使用的基线和数据集链接起来。其次，提出了集体感知增强检索器：为表示每个数据集或基线在学术网络中的位置，将自我描述与聚合的引用上下文连接起来；通过在这些表示上微调嵌入模型实现高效候选召回。最后，开发了推理增强重排序器，提取交互链以构建显式推理链，并微调大型语言模型以生成可解释的理由和优化排序。所构建的数据集涵盖了过去五年顶级AI会议中使用的85%的数据集和基线。在该数据集上，所提方法优于最强的先前基线，Recall@20平均提升+5.85%，HitRate@5平均提升+8.30%。研究结果推进了可靠、可解释的实验设计自动化。

---

### 10 Grounded Test-Time Adaptation for LLM Agents

**link**: https://arxiv.org/pdf/2511.04847.pdf  
**date**: 2025-11-10  
**keywords**: cs.LG  
**abs**: LLM智能体由于预训练与测试时条件的不匹配，难以泛化到新环境（如未见过的网站或新函数集），这源于两种失败模式：对环境特定组件（如观察格式）的语法误解，以及仅在测试时才显现的状态转换动态的语义误解。为解决这些问题，本文提出两种互补策略：在线分布适应方法，通过学习轻量级适应向量来参数化环境细微差别，快速对齐环境响应格式；部署时动态接地方法，在任务执行前通过角色驱动的探索阶段系统探测和学习环境的因果动态，为智能体配备非参数化世界模型。在函数调用和网页导航等基准测试中，两种策略均以最小计算成本有效提升性能，动态接地在复杂环境中尤为有效，例如在WebArena多站点拆分任务中，成功率从2%提升至23%。

---

### 11 Attention and Compression is all you need for Controllably Efficient Language Models

**link**: https://arxiv.org/pdf/2511.05313.pdf  
**date**: 2025-11-10  
**keywords**: cs.LG  
**abs**: Transformer中注意力机制的二次成本推动了高效方法的发展，如稀疏注意力、滑动窗口注意力、卷积和线性注意力等。尽管这些方法显著降低了计算和内存消耗，但往往会以质量为代价，特别是在上下文内回忆性能方面。此外，预先固定这种质量-计算权衡从一开始就是次优的：一些下游应用需要更多内存用于上下文内回忆，而另一些则需要更低的延迟和内存。本文提出压缩与注意力Transformer（CAT），这是一种概念简单的架构，仅采用两个要素：密集注意力和压缩。CAT通过关注迄今为止序列的压缩块来解码令牌块。压缩使得解码序列长度减少，从而节省计算和内存，而选择特定的块大小可以权衡质量与效率。此外，CAT可以同时使用多个块大小进行训练，无需重新训练即可在测试时直接控制质量-计算权衡，实现单一自适应架构。在常见语言建模任务、上下文内回忆和长上下文理解的详尽评估中，单个自适应CAT模型在不同计算-内存预算下优于现有高效基线（包括混合架构）。此外，单个CAT在模型各尺度上的语言建模性能与密集Transformer相当，同时速度快1.4-3倍，总内存使用量低2-9倍。