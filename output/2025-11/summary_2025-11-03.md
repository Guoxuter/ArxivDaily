### 1 Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering

**link**: https://arxiv.org/pdf/2510.27206.pdf  
**date**: 2025-11-03  
**keywords**: LLM Memory, Personal Memory  
**abs**: 针对大型语言模型（LLMs）的个性化需求，本文提出了一种细粒度实例定制导向框架。该框架通过动态生成样本级干扰向量并注入模型前向传播过程，实现推理时的个性化适配。核心创新包括细粒度导向组件（捕获注意力和MLP层的激活信号）和输入感知聚合模块（合成上下文相关增强信号）。方法具有高灵活性和数据效率，适用于动态用户模式和高数据稀疏场景，可作为插件与现有个性化技术兼容。实验验证了在长短文本生成、Web函数调用等任务中的有效性，提升了快速变化环境下的个性化性能。  

---

### 2 GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation

**link**: https://arxiv.org/pdf/2510.27210.pdf  
**date**: 2025-11-03  
**keywords**: Agent Memory  
**abs**: 针对多模态大型语言模型（MLLMs）在GUI导航任务中跨域泛化能力和历史信息利用不足的问题，本文提出一种集成结构化推理与历史总结的增强框架。该框架通过生成连贯的思维链分析（结合进度估计和决策推理），同时指导即时动作预测和紧凑历史总结生成。基于此框架训练的GUI代理GUI-Rise，通过伪标签轨迹的监督微调与Group Relative Policy Optimization（GRPO）强化学习实现优化，并引入历史感知奖励机制直接关联总结质量与后续动作性能。实验表明，该方法在标准基准测试中实现了领域外场景的最先进性能，验证了其在复杂导航任务中维持稳健推理和泛化能力的有效性。  

---

### 3 Detecting Data Contamination in LLMs via In-Context Learning

**link**: https://arxiv.org/pdf/2510.27055.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 本文提出了一种实用且准确的方法Contamination Detection via Context (CoDeC)，用于检测和量化大型语言模型中的训练数据污染。CoDeC通过测量上下文学习对模型性能的影响，区分训练期间记忆的数据和训练分布之外的数据。研究发现，上下文示例通常会提高未见过数据集的置信度，但当数据集是训练数据的一部分时，可能会因记忆模式被破坏而降低置信度。实验表明，CoDeC产生的污染分数能够清晰区分见过和未见过的数据集，并揭示了具有未公开训练语料库的开源模型中存在强烈的记忆证据。该方法简单、自动化，且与模型和数据集无关，易于集成到基准评估中。  

---

### 4 Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement

**link**: https://arxiv.org/pdf/2510.27051.pdf  
**date**: 2025-11-03  
**keywords**: cs.AI  
**abs**: 企业AI代理必须持续适应以保持准确性、减少延迟并与用户需求保持一致。本文介绍了在NVIDIA的混合专家（MoE）知识助手NVInfo AI（为超过30,000名员工提供服务）中数据飞轮的实际实现。通过实施基于MAPE（监控-分析-计划-执行）的闭环数据飞轮系统，构建了一个能够系统解决检索增强生成（RAG）管道故障并实现持续学习的系统。在部署后的3个月内，监控反馈并收集了495个负面样本，分析揭示了路由错误（5.25%）和查询重述错误（3.2%）两种主要故障模式。利用NVIDIA NeMo微服务，通过微调实现了针对性改进：对于路由任务，用微调的8B模型变体替换Llama 3.1 70B模型，准确率达96%，模型大小减少10倍，延迟降低70%；对于查询重述任务，微调使准确率提升3.7%，延迟减少40%。该方法展示了当人类在环（HITL）反馈在数据飞轮中结构化时，如何将企业AI代理转变为自我改进系统，并提供了构建能够从大规模实际使用中学习的稳健、自适应企业AI代理的可重复蓝图。  

---

### 5 Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs

**link**: https://arxiv.org/pdf/2510.27246.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 现有基准在评估大型语言模型（LLMs）的长期记忆和长上下文推理能力时存在不足，如缺乏叙事连贯性、领域狭窄且仅测试简单回忆任务。本文提出综合解决方案：首先，构建BEAM基准，包含100个对话和2000个验证问题，通过自动生成长达1000万 tokens 的连贯、主题多样的对话及探测问题实现。其次，提出LIGHT框架，受人类认知启发，为LLMs配备三种互补记忆系统：长期情景记忆、短期工作记忆和用于积累关键事实的暂存器。实验表明，即使具有100万token上下文窗口的LLM（含检索增强）在对话延长时仍表现不佳，而LIGHT框架持续提升各模型性能，平均改进3.5%-12.69%，消融实验证实各记忆组件的贡献。  

---

### 6 Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity

**link**: https://arxiv.org/pdf/2510.27378.pdf  
**date**: 2025-11-03  
**keywords**: cs.LG  
**abs**: 思维链（CoT）输出使我们能够读取模型的逐步推理过程。由于任何长串行推理过程都必须通过这种文本痕迹，CoT的质量是了解模型思考过程的直接窗口。这种可见性有助于发现不安全或错位行为（可监控性），但前提是CoT能透明反映内部推理（忠实性）。本文通过引入冗长性（CoT是否列出解决任务所需的所有因素），将忠实性和冗长性结合为单一可监控性分数，衡量CoT作为模型“外部工作记忆”的性能，这是许多基于CoT监控的安全方案所依赖的属性。在BBH、GPQA和MMLU数据集上的评估显示，模型可能看似忠实但因遗漏关键因素而难以监控，且不同模型家族的可监控性差异显著。  

---

### 7 Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs

**link**: https://arxiv.org/pdf/2510.27400.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 知识编辑已成为更新大型语言模型（LLMs）中事实知识的有效方法，通常定位知识存储模块并修改其参数。然而现有方法大多关注多层感知器（MLP）模块，忽视了注意力（Attn）模块。这种不平衡会留下残留的过时知识并限制编辑效果。通过对先进LLMs的全面知识定位实验发现，Attn模块在事实知识存储和检索中发挥重要作用，尤其是在早期层。基于此，提出IntAttn-Edit方法，将联想记忆范式扩展到联合更新MLP和Attn模块。该方法采用知识平衡策略，根据每个模块对知识存储的测量贡献成比例分配更新幅度。标准基准实验表明，IntAttn-Edit比现有方法具有更高的编辑成功率、更好的泛化能力和更强的知识保留能力。  

---

### 8 Thought Branches: Interpreting LLM Reasoning Requires Resampling

**link**: https://arxiv.org/pdf/2510.27484.pdf  
**date**: 2025-11-03  
**keywords**: cs.LG  
**abs**: 大多数解释推理模型的工作仅研究单个思维链（CoT），但这些模型定义了多种可能CoT的分布。本文认为研究单个样本不足以理解因果影响和底层计算，提出通过重采样来理解该分布。案例研究包括：验证模型陈述的原因是否真正导致行动（如自保护语句对勒索的因果影响较小）、评估CoT编辑的转向效果（发现离策略干预效果小且不稳定）、引入弹性度量以理解移除推理步骤的影响（关键规划语句难以移除但移除后影响大），以及分析不忠实CoT中的因果中介效应（提示在不被明确提及的情况下对输出产生累积影响）。重采样方法能实现可靠的因果分析和更清晰的模型推理叙事。  

---

### 9 AI Agents in Drug Discovery

**link**: https://arxiv.org/pdf/2510.27130.pdf  
**date**: 2025-11-03  
**keywords**: cs.LG  
**abs**: 本文探讨了人工智能智能体（AI Agents）在药物发现中的应用，指出这些智能体基于大型语言模型（LLMs），并结合感知、计算、行动和记忆工具，能够自主推理、行动和学习。文中提到智能体系统整合了记忆工具，形成具有记忆能力的智能体架构（如ReAct、Reflection等），可应用于文献合成、毒性预测、实验方案生成等药物发现关键阶段。这表明论文涉及Agent Memory和LLM Memory相关内容，强调记忆工具在智能体中的作用。  

---

### 10 Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling

**link**: https://arxiv.org/pdf/2510.26912.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 混合模型通过结合状态空间模型（SSMs）和注意力机制，在利用SSMs的效率和注意力的高召回能力方面表现出强大性能。然而，这些混合模型的架构设计选择仍未被充分理解。本研究通过记忆利用和整体性能的视角分析混合架构，并提出一种补充方法以进一步增强其有效性。首先，研究考察了SSM和注意力层的顺序与并行集成区别，发现顺序混合模型在较短上下文上表现更好，而并行混合模型在较长上下文上更有效。此外，引入了一种以数据为中心的方法，通过在增强了释义的数据集上持续训练，在保持其他能力的同时进一步提升召回率。该方法在不同基础模型上泛化良好，且优于旨在增强召回的架构修改。研究结果为混合SSM-注意力模型提供了更深入的理解，并为设计适应各种用例的架构提供了实用指导。  

---

### 11 Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services

**link**: https://arxiv.org/pdf/2510.27016.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 随着对话式AI系统的广泛应用，用户在与大型语言模型（LLMs）交互时共享敏感个人数据引发的隐私泄露问题日益受到关注。对话中可能包含的个人身份信息（PII）若被泄露，可能导致安全漏洞或身份盗用。为解决这一挑战，本文提出LOPSIDED框架——一种语义感知的隐私代理，旨在使用远程LLMs时保护敏感PII数据。与以往常降低响应质量的方法不同，该方法动态将用户提示中的敏感PII实体替换为语义一致的假名，同时保留对话的上下文完整性。模型生成响应后，假名会自动恢复，确保用户获得准确且隐私保护的输出。通过ShareGPT的真实对话数据进行评估，结果表明LOPSIDED相比基线技术将语义效用错误减少了5倍，同时增强了隐私保护。  

---

### 12 MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval

**link**: https://arxiv.org/pdf/2510.27569.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 大型语言模型（LLMs）在推理和生成方面表现出色，但受限于静态预训练数据，存在事实不准确和对新信息适应性弱的问题。检索增强生成（RAG）通过将LLMs与外部知识关联来解决此问题，但其有效性关键取决于模型能否充分获取相关信息。现有RAG系统依赖单一检索器和固定的top-k选择，限制了对语料库狭窄且静态子集的访问，成为全面获取外部信息的主要瓶颈，尤其在需要语料库级推理的任务中。为克服这一限制，本文提出MARAG-R1，一种强化学习的多工具RAG框架，使LLMs能动态协调多种检索机制以实现更广泛、精确的信息访问。MARAG-R1配备四种检索工具——语义搜索、关键词搜索、过滤和聚合，并通过监督微调后强化学习的两阶段训练过程学习如何及何时使用它们。该设计允许模型交织推理与检索，逐步收集语料库级综合所需的充分证据。在GlobalQA、HotpotQA和2WikiMultiHopQA上的实验表明，MARAG-R1显著优于强基线，并在语料库级推理任务中取得新的最先进结果。  

---

### 13 Dynamic Affective Memory Management for Personalized LLM Agents

**link**: https://arxiv.org/pdf/2510.27418.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 大型语言模型的进步使个性化AI代理成为新的研究焦点。当前的代理系统主要依赖个性化的外部记忆数据库来提供定制化体验，但它们面临着记忆冗余、记忆过时以及记忆-上下文整合不佳等挑战，这主要是由于交互过程中缺乏有效的记忆更新。为解决这些问题，我们提出了一种专为情感场景设计的新型记忆管理系统。我们的方法采用了受贝叶斯启发的记忆更新算法，并引入了记忆熵的概念，使代理能够通过最小化全局熵来自主维护动态更新的记忆向量数据库，从而提供更个性化的服务。为了更好地评估该系统在情感场景中的有效性，我们提出了DABench基准，该基准侧重于对对象的情感表达和情感变化。实验结果表明，我们的系统在个性化、逻辑连贯性和准确性方面表现出优异性能。消融研究进一步验证了受贝叶斯启发的更新机制在缓解记忆膨胀方面的有效性。我们的工作为长期记忆系统的设计提供了新的见解。  

---

### 14 LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints

**link**: https://arxiv.org/pdf/2510.27054.pdf  
**date**: 2025-11-03  
**keywords**: cs.CL  
**abs**: 本文针对复杂知识环境下检索增强生成（RAG）存在的覆盖不足、结果不稳定和可靠性有限等问题，提出了一种融合多粒度记忆索引与不确定性估计的置信度控制方法。该方法构建了层次化记忆结构，将知识表示划分为不同粒度级别，实现从局部细节到全局上下文的动态索引与检索，从而在检索与生成之间建立更紧密的语义联系。在此基础上，引入不确定性估计机制，在生成过程中显式约束和过滤低置信度路径，使模型在保持信息覆盖度的同时有效抑制噪声和虚假内容。整体优化目标包括生成损失、熵约束和方差正则化，形成统一的置信度控制框架。实验中设计了全面的敏感性测试和比较分析，涵盖超参数、环境条件和数据结构，验证了所提方法在不同场景下的稳定性和鲁棒性。结果表明，该方法在问答准确性、检索召回率、排序质量和事实一致性方面均优于现有模型，证明了多粒度索引与置信度控制相结合的有效性。本研究不仅为检索增强生成提供了新的技术途径，也为提高大型模型在复杂上下文中的可靠性和可控性提供了实践依据。