### 1 LiteraryTaste: A Preference Dataset for Creative Writing Personalization

**link**: https://arxiv.org/pdf/2511.09310.pdf  
**date**: 2025-11-13  
**keywords**: cs.CL  
**abs**: 本文引入LiteraryTaste数据集，包含60人的创意写作偏好数据（包括自我报告和文本偏好标注），用于个性化大型语言模型开发。研究发现：1）创意写作偏好存在个体差异；2）微调transformer编码器在建模个人偏好时准确率达75.8%；3）陈述偏好对建模显示偏好作用有限。该数据集为个性化创意写作技术奠定了基础。

---

### 2 GMTRouter: Personalized LLM Router over Multi-turn User Interactions

**link**: https://arxiv.org/pdf/2511.08590.pdf  
**date**: 2025-11-13  
**keywords**: cs.CL  
**abs**: 本文提出GMTRouter，一种基于异构图的多轮用户交互个性化路由框架。它将用户-LLM交互建模为四类节点图，通过定制消息传递机制从少样本数据中学习用户偏好。实验表明，GMTRouter在多个数据集上准确率提升0.9%-21.6%，AUC提高0.006-0.309，并能高效适应新用户和动态偏好，无需大量微调。

---

### 3 Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models

**link**: https://arxiv.org/pdf/2511.08877.pdf  
**date**: 2025-11-13  
**keywords**: cs.CL  
**abs**: 本研究探究大型语言模型（LLMs）生成引文时的幻觉问题。通过分析GPT-4.1生成的100条计算机科学引文，发现：1）引文数量与事实准确性正相关；2）超过约1000次引用的论文几乎被逐字记忆；3）高被引论文间存在记忆干扰现象。结果揭示了LLMs从泛化转向记忆的阈值。

---

### 4 Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM

**link**: https://arxiv.org/pdf/2511.08620.pdf  
**date**: 2025-11-13  
**keywords**: cs.CL  
**abs**: 本文提出GrADS方法，一种基于梯度的自适应数据选择策略，用于缓解监督微调中的灾难性遗忘问题。该方法利用梯度大小和分布优先关键样本，仅需5%数据即可超越全数据集微调性能，50%数据时性能显著提升。在医疗、法律和金融领域的实验验证了其高效性，大幅减少遗忘现象。

---

### 5 TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations

**link**: https://arxiv.org/pdf/2511.08832.pdf  
**date**: 2025-11-13  
**keywords**: cs.LG  
**abs**: 本文提出TIGER-MARL框架，通过动态时间图建模智能体间协调结构的演变，并采用时间注意力编码器生成时间感知嵌入。实验表明，该方法在任务性能和样本效率上优于现有价值分解及基于图的多智能体强化学习基线，有效提升协作策略学习。

---

### 6 History-Aware Reasoning for GUI Agents

**link**: https://arxiv.org/pdf/2511.09127.pdf  
**date**: 2025-11-13  
**keywords**: cs.AI  
**abs**: 针对GUI智能体在长程任务中短期记忆薄弱的问题，本文提出历史感知推理（HAR）框架。通过构建反思学习场景和混合奖励函数，HAR-GUI-3B模型将推理模式从历史无关转变为历史感知，增强对屏幕细节的可靠感知和短期记忆稳定性，在GUI基准测试中展现优异泛化能力。

---

### 7 Mixture-of-Channels: Exploiting Sparse FFNs for Efficient LLMs Pre-Training and Inference

**link**: https://arxiv.org/pdf/2511.09323.pdf  
**date**: 2025-11-13  
**keywords**: cs.LG  
**abs**: 本文提出Mixture-of-Channels（MoC）架构，通过SwiGLU门控机制为每个token选择性激活Top-K通道，减少前馈网络的激活内存开销。实验表明，MoC在保持模型性能的同时显著降低预训练内存消耗和推理延迟，提升吞吐量，解决了LLMs扩展中的关键瓶颈问题。

---

### 8 Compact Memory for Continual Logistic Regression

**link**: https://arxiv.org/pdf/2511.09167.pdf  
**date**: 2025-11-13  
**keywords**: cs.LG  
**abs**: 本文提出一种为逻辑回归构建紧凑记忆的新方法，通过Hessian匹配和概率PCA估计最优记忆。在Split-ImageNet上，仅需0.3%记忆量即达60%准确率（超越回放方法的30%），2%记忆量时提升至74%，接近批量训练性能（77.6%），为持续学习中的灾难性遗忘问题提供高效解决方案。

---

### 9 PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness

**link**: https://arxiv.org/pdf/2511.09487.pdf  
**date**: 2025-11-13  
**keywords**: cs.LG  
**abs**: 本文提出概率密度感知核心集（PDAC）方法，利用投影高斯混合模型估计样本联合密度，实现高效缓冲区选择。通过流式EM算法扩展为SPDAC，适用于流式场景。实验表明，PDAC在多种持续学习设置中优于基线，以最小存储开销近似完整数据集训练效果，显著提升计算效率。

---

### 10 CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?

**link**: https://arxiv.org/pdf/2511.09483.pdf  
**date**: 2025-11-13  
**keywords**: cs.AI  
**abs**: 本文提出CrochetBench基准，评估多模态大语言模型在钩针领域的细粒度过程推理能力。该基准要求模型识别针脚、生成可编译指令，并通过CrochetPARADE DSL实现结构验证。实验揭示模型在长程符号推理和3D感知过程合成中的局限性，突显表面理解与可执行精度间的差距。

---

### 11 Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds

**link**: https://arxiv.org/pdf/2511.08892.pdf  
**date**: 2025-11-13  
**keywords**: cs.AI, Agent Memory, Procedural Memory  
**abs**: 本文介绍Lumine，首个用于3D开放世界实时任务的通用智能体方案。通过视觉语言模型统一感知、推理和行动，以5Hz处理像素生成30Hz操作。在《原神》中完成五小时主线任务后，展示零样本跨游戏泛化能力（如《鸣潮》《崩坏：星穹铁道》），标志开放环境通用智能体的重要进展。

---

### 12 ProBench: Benchmarking GUI Agents with Accurate Process Information

**link**: https://arxiv.org/pdf/2511.09157.pdf  
**date**: 2025-11-13  
**keywords**: cs.AI, Agent Memory, Procedural Memory  
**abs**: 本文提出ProBench基准，包含200+移动GUI任务，扩展过程相关评估并设计自动过程提供器。实验揭示GUI智能体在长程任务中的普遍缺陷（如中间步骤错误），暴露模型在细节感知和记忆稳定性上的不足，为未来改进提供具体方向。

---

### 13 Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning

**link**: https://arxiv.org/pdf/2511.08749.pdf  
**date**: 2025-11-13  
**keywords**: cs.AI  
**abs**: 本文设计QDIN架构，将强化学习系统重构为可回答环境查询的推理引擎。通过专用神经模块处理策略、可达性等查询类型，在确定性环境中实现99%可达性IoU准确率，同时保持竞争性控制性能，为原生可解释RL系统奠定基础，涉及智能体对环境知识的高效存储与检索。

---

### 14 UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models

**link**: https://arxiv.org/pdf/2511.08873.pdf  
**date**: 2025-11-13  
**keywords**: cs.AI  
**abs**: 本文提出单向认知优化（UCO）方法，通过进度奖励和支架奖励机制实现LLMs教学动态适应。进度奖励捕获学生认知转变，支架奖励匹配最近发展区。在BigMath和MathTutorBench基准上超越11个基线，接近先进闭源模型性能，强调通过持续跟踪学生认知状态实现个性化教学。