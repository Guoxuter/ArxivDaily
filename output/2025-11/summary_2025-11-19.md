### 1 Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces

**link**: https://arxiv.org/pdf/2511.12759.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 本文探讨了人类记忆提取过程，该过程常类似于动物在斑块环境中觅食的生态行为。最优觅食意味着遵循边际价值定理（MVT），即个体在语义相关概念斑块中进行信息提取，直至其回报降低后转向新的概念集群。尽管人类行为数据表明语义流畅性任务中存在类似觅食的模式，但现代高维嵌入空间是否能为算法提供匹配人类观察行为的表示仍不明确。利用最先进的嵌入技术和先前的语义流畅性数据，研究发现这些嵌入空间上的随机游走产生的结果与最优觅食和MVT一致。出乎意料的是，引入本应模拟策略性集群接受与拒绝的自适应算法Metropolis-Hastings采样，并未产生与人类行为一致的结果。这些发现挑战了“更复杂的采样机制必然导致更好的记忆提取认知模型”这一假设。相反，它们表明，即使采用简单采样，适当结构化的嵌入也能产生接近最优的觅食动态。这支持了Hills（2012）而非Abbott（2015）的观点，证明现代嵌入无需依赖复杂的接受标准即可近似人类记忆觅食过程。

---

### 2 MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization

**link**: https://arxiv.org/pdf/2511.12113.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 大型语言模型（LLMs）展现出强大的推理能力，但其能力压缩到较小模型时面临灾难性遗忘问题。现有方法存在两方面不足：一是数据集常忽略训练数据知识与模型固有能力的关系，难以保留先验知识；二是传统训练目标无法有效约束固有知识的保留，导致已学技能遗忘。为解决这些问题，本文从数据和微调方法两方面提出综合方案。在数据层面，构建了包含5K实例的数据集，涵盖多类推理任务并融入元认知知识，更适合向小模型蒸馏。在训练层面，提出GDPO（Group Direction Preference Optimization），适用于资源受限场景，能高效逼近GRPO性能。GDPO在大模型指导下，通过参考模型隐式约束优化路径，实现更有效的知识迁移并限制参数过度漂移。实验表明，该方法显著缓解了小模型的灾难性遗忘，提升了推理性能。

---

### 3 Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation

**link**: https://arxiv.org/pdf/2511.12254.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 移动代理展现出巨大潜力，但当前最先进的代理在现实世界的长周期跨应用任务中成功率不足。本文将此瓶颈归因于代理过度依赖大型语言模型（MLLMs）中的静态内部知识，导致两个关键失败点：1）高层规划中的策略幻觉；2）用户界面（UI）低层执行中的操作错误。核心见解是高层规划和低层UI操作需要根本不同类型的知识：规划需要高层、面向策略的经验，而操作需要与特定应用UI紧密相关的低层、精确指令。基于这些见解，本文提出Mobile-Agent-RAG，一种新颖的分层多智能体框架，创新性地整合了双层检索增强。在规划阶段，引入Manager-RAG通过检索人类验证的综合任务计划提供高层指导，以减少策略幻觉。在执行阶段，开发Operator-RAG通过检索最精确的低层指导来提高原子操作的准确性，与当前应用和子任务对齐。为准确提供这些知识类型，构建了两个专门的面向检索的知识库。此外，引入Mobile-Eval-RAG，一个具有挑战性的基准，用于在现实的多应用、长周期任务上评估此类代理。大量实验表明，Mobile-Agent-RAG显著优于最先进的基线，任务完成率提高11.0%，步骤效率提高10.2%，为上下文感知、可靠的多智能体移动自动化建立了稳健范式。

---

### 4 More Than Irrational: Modeling Belief-Biased Agents

**link**: https://arxiv.org/pdf/2511.12359.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 尽管人工智能及其相关技术发展迅速，但预测和推断用户或人类协作者的次优行为仍然是一个关键挑战。在许多情况下，此类行为并非非理性的结果，而是在固有的认知限制和对世界的有偏信念下做出的理性决策。本文正式引入一类计算理性（CR）用户模型，用于在有偏信念下最优行动的认知受限智能体。核心创新在于明确建模有限记忆过程如何导致动态不一致和有偏的信念状态，进而导致次优的顺序决策。本文解决了从被动观察中识别潜在用户特定限制和实时推断有偏信念状态的挑战。认为对于具有明确和参数化认知过程的形式化CR模型家族，此挑战是可处理的。为支持这一主张，提出一种基于嵌套粒子滤波的高效在线推理方法，该方法同时跟踪用户的潜在信念状态并从观察到的行动流中估计未知的认知限制。在代表性导航任务中使用记忆衰减作为认知限制的例子进行验证。模拟结果表明：（1）CR模型生成与不同记忆容量水平对应的直观合理行为；（2）推理方法能从有限观察（≤100步）中准确高效地恢复真实认知限制。进一步展示了该方法如何为开发自适应AI助手提供原则性基础，实现考虑用户记忆限制的自适应辅助。

---

### 5 Forgetting-MarI: LLM Unlearning via Marginal Information Regularization

**link**: https://arxiv.org/pdf/2511.11914.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 随着AI模型训练数据的不断扩大，从已训练模型中移除特定数据的影响对于隐私保护和法规遵从变得至关重要。遗忘（Unlearning）技术通过选择性地从训练好的模型中移除参数知识，无需从头重新训练，这对于大型语言模型（LLMs）等资源密集型模型尤为关键。现有遗忘方法在尝试“遗忘”特定数据时，往往会移除过多必要信息，导致模型性能下降。本文提出Forgetting-MarI，一种LLM遗忘框架，能够证明性地仅移除待遗忘数据贡献的额外（边际）信息，同时保留需保留数据所支持的信息。通过惩罚边际信息，该方法为训练模型中遗忘数据集的残余影响提供了明确的上限，实现了可证明的不可检测性。大量实验证实，该方法优于当前最先进的遗忘方法，在多种基准测试中实现了可靠的遗忘效果，并更好地保留了模型的整体性能。这一进展为使AI系统在不损害其有效性的前提下更具可控性，并符合隐私和版权法规迈出了重要一步。

---

### 6 Towards autonomous quantum physics research using LLM agents with access to intelligent tools

**link**: https://arxiv.org/pdf/2511.11752.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 人工智能（AI）已应用于众多科学领域，但初始研究问题和目标几乎仍由人类研究者提出。AI在科学中生成的创造性想法较为罕见且往往模糊，因此执行这些想法仍是人类的任务。将想法生成与实施自动化整合到一个连贯系统中，将显著改变人类在科学过程中的角色。本文提出AI-Mandel，这是一个能够在量子物理学领域生成并实施想法的LLM智能体。AI-Mandel从文献中提炼想法，并使用特定领域的AI工具将其转化为可在实验室中直接实施的具体实验设计。AI-Mandel生成的想法通常具有科学价值——其中两个想法已被用于撰写独立的后续科学论文，包括量子隐形传态的新变体、不定因果顺序的量子网络基元，以及基于量子信息传输闭环的几何相位新概念。AI-Mandel是能够生成和实施具体、可操作想法的AI物理学家原型演示。构建此类系统不仅有助于加速科学研究，还揭示了通往人类水平人工科学家道路上的具体开放挑战。

---

### 7 Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction

**link**: https://arxiv.org/pdf/2511.11770.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 生成复杂、逻辑正确的多跳问题SPARQL查询仍是知识图谱问答的关键瓶颈，因为大型语言模型（LLMs）的一次性生成脆弱性阻碍了与结构化数据的可靠交互。当前方法缺乏基于实时执行反馈动态调试查询所需的自适应策略。本文介绍了一种新颖的智能体框架，其中LLM学习用于迭代SPARQL构建顺序过程的弹性策略。我们表明，一个仅通过结果驱动的强化学习（GRPO）训练、无需监督微调的紧凑3B参数模型，能够为此任务学习有效策略，发现如何系统地从执行错误中恢复并优化查询以获得正确答案。在LC-QuAD 2.0的精选可执行单答案子集上，我们的智能体在实体链接后达到49.7%的准确率，比最强的迭代零样本基线显著提高17.5个百分点。进一步分析表明，虽然智能体的能力由RL驱动，但其性能通过明确的 deliberative推理步骤得到增强，该步骤作为认知支架提高策略精度。这项工作为教授智能体通过交互掌握形式化、符号化工具提供了可推广的蓝图，弥合了概率LLMs与知识图谱结构化世界之间的差距。

---

### 8 WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance

**link**: https://arxiv.org/pdf/2511.12997.pdf
**date**: 2025-11-19
**keywords**: cs.AI, Agent Memory
**abs**: 多模态LLM驱动的智能体在网页导航方面展现出令人印象深刻的能力，但当前智能体难以从跨会话的过往经验中学习，限制了其长期鲁棒性和样本效率。WebCoach是一个模型无关的自进化框架，为网页浏览智能体配备持久的跨会话记忆，无需重新训练即可实现更好的长期规划、反思和持续学习。该框架包含三个关键组件：(1)WebCondenser，将原始导航日志标准化为简洁摘要；(2)外部记忆存储，将完整轨迹组织为情景经验；(3)Coach，基于相似性和时效性检索相关经验，并通过运行时钩子向智能体注入特定任务建议。这种设计使网页智能体能够访问超出其原生上下文窗口的长期记忆，提高复杂浏览任务的鲁棒性。此外，WebCoach通过不断从新导航轨迹中整理情景记忆实现自进化，使智能体能够随时间改进而无需重新训练。在WebVoyager基准测试中，WebCoach持续提高了三种不同LLM backbone的浏览器智能体性能，使用38B模型时，任务成功率从47%提升至61%，同时减少或保持平均步骤数。值得注意的是，配备WebCoach的较小基础模型性能可与使用GPT-4o的相同网页智能体相媲美。

---

### 9 PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics

**link**: https://arxiv.org/pdf/2511.13021.pdf
**date**: 2025-11-19
**keywords**: cs.AI, LLM Memory
**abs**: 现实世界对话富含实体提及、指代和隐含意义等语用元素。理解这些细微差别是成功自然交流的必要条件，通常需要构建一个编码这些元素并捕捉其状态演化动态的局部世界模型。然而，目前尚不清楚语言模型（LMs）是否构建或维持对话的稳健隐式表示。本研究评估了LMs在二元对话中编码和更新其内部世界模型的能力，并测试了它们在语言改动下的可塑性。为此，我们对源自流行数据集的对话应用七种最小语言改动，并构建了两个包含是非题的基准。对多种开源和闭源LMs的评估表明，它们难以维持稳健的准确性。分析发现，LMs难以记住关键细节，例如在对话语言改动下追踪实体。我们随后提出双视角可解释性框架，识别有用或有害的Transformer层，并突出受有害层影响最大的语言改动，这些改动通常是由于编码虚假信号或依赖捷径。受这些见解启发，我们提出两种基于层正则化的微调策略来抑制有害层的影响。

---

### 10 Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations

**link**: https://arxiv.org/pdf/2511.11583.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 大语言模型（LLMs）在个性化金融推荐中具有潜力，但受限于上下文长度、幻觉问题和缺乏行为基础。本文提出RAG-FLARKO，作为FLARKO的检索增强扩展，通过多阶段和并行知识图谱（KG）检索过程克服可扩展性和相关性挑战。该方法首先从用户交易知识图谱中检索行为相关实体，再利用此上下文过滤市场知识图谱中的时间一致信号，为LLM构建紧凑且有依据的子图。此 pipeline 减少了上下文开销并增强了模型对相关信息的关注。实证评估表明，RAG-FLARKO显著提升了推荐质量，使更小、更高效的模型在盈利能力和行为对齐方面实现高性能，为资源受限环境下部署基于知识的金融AI提供了可行路径。

---

### 11 MedDCR: Learning to Design Agentic Workflows for Medical Coding

**link**: https://arxiv.org/pdf/2511.13361.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 医疗编码需将自由文本临床记录转换为标准化代码，涉及多步推理。现有基于智能体LLM的方法依赖手动设计的刚性工作流，难以应对真实场景的复杂性。MedDCR框架将工作流设计视为学习问题：Designer提出工作流，Coder执行，Reflector评估并提供反馈，同时通过记忆档案（memory archive）保存先前设计以实现重用和迭代优化。该框架在基准数据集上优于现有基线，生成的工作流可解释且适应性强，提升了自动化系统的可靠性和可信度。

---

### 12 Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning

**link**: https://arxiv.org/pdf/2511.13371.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 本文研究大型语言模型如何解决空间导航任务，通过在网格环境中训练GPT-2模型于三种空间学习范式（被动探索、目标导向规划、混合模型）。行为和机制分析揭示两种学习算法：被动探索模型（Foraging Model）发展出类似“认知地图”的空间表征，能将空间信息整合为自足坐标系，减少对历史方向 token 的依赖；目标导向模型则采用路径依赖算法，始终依赖显式方向输入。研究表明，Transformer的空间智能可能处于从探索数据塑造的通用世界模型到目标优化启发式的频谱上，训练方式影响策略涌现。

---

### 13 An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence

**link**: https://arxiv.org/pdf/2511.13411.pdf
**date**: 2025-11-19
**keywords**: cs.AI
**abs**: 本文提出基于卡尔达肖夫指数的自主AI（AAI）量表，用于衡量从固定机器人流程自动化（AAI-0）到人工通用智能（AAI-4）及以上的进展。该多轴可测试量表包含十个能力轴，其中“记忆/持久性”（Memory/Persistence）是关键轴之一，与自主AI智能体的记忆存储和持久化能力直接相关。量表还定义了自改进系数κ、闭包属性及OWA-Bench基准，为评估智能体记忆等核心能力提供了操作化框架。

---

### 14 Learning with Preserving for Continual Multitask Learning

**link**: https://arxiv.org/pdf/2511.11676.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 人工智能系统在自动驾驶和医学影像分析等关键领域经常需要使用共享的输入数据流持续学习新任务。例如，模型在学会检测交通标志后，可能需要使用同一摄像头输入学习分类交通灯或不同类型的车辆。这引入了持续多任务学习（CMTL）这一具有挑战性的场景，即模型在底层数据分布上依次学习新任务，同时不忘记先前习得的能力。现有持续学习方法在此场景中常失败，因为它们学习的是碎片化、特定于任务的特征，这些特征会相互干扰。为解决此问题，我们提出了学习与保留（LwP）框架，将重点从保留任务输出转向维护共享表示空间的几何结构。LwP的核心是动态加权距离保留（DWDP）损失，通过正则化潜在数据表示之间的成对距离来防止表示漂移。这种保留底层几何结构的机制使模型能够保留隐式知识并支持多样化任务，且无需重放缓冲区，适用于隐私敏感应用。在时间序列和图像基准上的广泛评估表明，LwP不仅减轻了灾难性遗忘，还在CMTL任务中持续优于最先进的基线。值得注意的是，我们的方法对分布偏移表现出更强的鲁棒性，并且是唯一超过强单任务学习基线的方法，突显了其在现实世界动态环境中的有效性。

---

### 15 LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models

**link**: https://arxiv.org/pdf/2511.12116.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）在特定时间截止点前的文本数据上进行预训练，形成了严格的知识边界，超出该边界模型无法在不查询外部来源的情况下提供准确信息。更关键的是，当此限制未知或被忽略时，LLMs可能在推理任务中无意中将过时的时间敏感信息与一般知识混合，从而影响响应准确性。本文提出LLMLagBench，一个LLM新鲜度基准，作为系统方法来识别LLM训练数据的最早可能时间边界，通过评估模型对近期事件的知识实现。该基准应用于大量LLMs（包括明确声明和未声明训练截止日期的模型），并通过手动验证及与LLM预训练公开信息的比较来评估其可靠性。

---

### 16 Adaptive Focus Memory for Language Models

**link**: https://arxiv.org/pdf/2511.12712.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 大型语言模型（LLMs）越来越多地应用于多轮对话场景，但其性能仍受限于固定上下文窗口和简单的记忆策略。每轮对话都重放完整对话历史虽简单但成本高昂，而静态总结或仅基于近期性的启发式方法往往会丢失用户安全关键细节。本文提出自适应焦点记忆（AFM），一种动态上下文管理器，它基于与当前查询的语义相似度、半衰期近期权重和重要性分类，为每条过往消息分配三种保真度级别之一——完整（FULL）、压缩（COMPRESSED）或占位符（PLACEHOLDER）。AFM在严格的令牌预算下按时间顺序打包消息，优先为最相关的对话轮次保留高保真度，同时旨在保留对话的低成本痕迹。在一个涉及严重花生过敏用户计划泰国旅行的安全导向基准测试中，AFM在短对话和中等长度对话中均能保留过敏信息，匹配了简单重放的安全性能，并将平均令牌使用量较重放基线减少了66%。我们发布了AFM的模块化Python实现，适用于OpenAI兼容API和离线操作，使从业者能够在评估场景中降低推理成本，同时不牺牲安全性或事实连续性。

---

### 17 Catastrophic Forgetting in Kolmogorov-Arnold Networks

**link**: https://arxiv.org/pdf/2511.12828.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 灾难性遗忘是持续学习中的长期挑战，模型在学习新任务时会丢失早期任务的知识。虽然已针对多层感知器（MLPs）提出多种缓解策略，但最近的架构进展如科尔莫戈罗夫-阿诺德网络（KANs）被认为通过利用局部样条激活函数具有内在的抗遗忘能力。然而，KANs在持续学习中的实际行为尚不明确，其局限性也未被充分理解。为此，本文对KANs中的灾难性遗忘进行了全面研究，并开发了一个理论框架，将遗忘与激活支持重叠和内在数据维度联系起来。通过在合成任务和视觉任务上的系统实验，验证了这些分析，测量了不同模型配置和数据复杂度下的遗忘动态。此外，本文还引入了KAN-LoRA，一种用于语言模型参数高效持续微调的新型适配器设计，并在知识编辑任务中评估了其有效性。研究结果表明，尽管KANs在低维算法设置中表现出良好的知识保留能力，但在高维领域（如图像分类和语言建模）中仍然容易受到遗忘的影响。这些结果促进了对KANs优缺点的理解，为持续学习系统设计提供了实用见解。

---

### 18 On the Fundamental Limits of LLMs at Scale

**link**: https://arxiv.org/pdf/2511.12869.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 大型语言模型（LLMs）从规模扩展中获益巨大，但其增益受到五个基本限制：（1）幻觉，（2）上下文压缩，（3）推理退化，（4）检索脆弱性，（5）多模态错位。现有综述虽从经验上描述了这些现象，但缺乏将其与计算、信息和学习的基础限制联系起来的严格理论综合。本文通过提出一个统一的、基于证明的框架填补了这一空白，该框架形式化了LLM扩展的内在理论上限。首先，可计算性和不可计算性意味着存在不可约的误差残留：对于任何可计算枚举的模型族，对角线化保证存在某些模型必须失败的输入，而不可判定的查询（如停机类任务）会导致所有可计算预测器的无限失败集。其次，信息论和统计约束限制了即使在可判定任务上的可达到精度，有限描述长度导致压缩误差，而长尾事实知识需要极高的样本复杂度。第三，几何和计算效应使长上下文的实际大小远低于其标称大小，这源于位置训练不足、编码衰减和softmax拥挤。本文进一步展示了基于似然的训练如何倾向于模式补全而非推理，令牌限制下的检索如何受到语义漂移和耦合噪声的影响，以及多模态扩展如何继承浅层跨模态对齐。这些内容揭示了LLM在记忆（如上下文记忆、检索记忆）方面的根本限制。

---

### 19 Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning

**link**: https://arxiv.org/pdf/2511.13351.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 针对多模态食物学习中的灾难性遗忘问题，本文提出一种结合双LoRA架构与高质量伪重放的持续学习框架。该框架为每个任务设计两个互补的低秩适配器：专门化LoRA通过正交约束学习任务特定知识，协作LoRA通过伪重放巩固跨任务共享知识。为提升重放数据可靠性，高质量伪重放策略利用自一致性和语义相似性减少生成样本的幻觉。在Uni-Food数据集上的实验表明，该方法有效缓解了遗忘问题，是首个适用于复杂食物任务的持续学习方案。

---

### 20 PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection

**link**: https://arxiv.org/pdf/2511.12130.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 本文针对多模态对话立场检测（MCSD）中存在的伪多模态（视觉线索仅存在于源帖而评论被视为纯文本）和用户同质性（忽视塑造立场表达的个人特质）问题，提出了首个以用户为中心的MCSD数据集U-MStance，并设计了PRISM框架。该框架通过从用户历史帖子和评论中提取纵向用户画像（persona）以捕捉个体特质，涉及个人记忆（Personal Memory）相关内容，因为用户画像基于历史数据反映了用户的个人特征和经历。实验表明，PRISM在U-MStance数据集上显著优于强基线模型，强调了以用户为中心和基于上下文的多模态推理对真实立场理解的有效性。

---

### 21 CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic

**link**: https://arxiv.org/pdf/2511.12159.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 本文针对工具集成推理（TIR）中搜索代理依赖强化学习优化时面临的稀疏结果奖励导致探索效率低和训练不稳定问题，提出了CriticSearch框架。该框架通过回顾性评论家机制，利用完整轨迹和黄金答案的特权信息提供密集的回合级反馈，涉及智能体记忆（Agent Memory）相关内容，因为代理需要记忆其推理过程中的轨迹信息以进行后续评估和策略改进。实验表明，CriticSearch在多种多跳推理基准上持续优于现有基线，实现了更快的收敛、更高的训练稳定性和性能。

---

### 22 MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling

**link**: https://arxiv.org/pdf/2511.11793.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 我们提出了 MiroThinker v1.0，这是一个开源研究代理，旨在提升工具增强推理和信息检索能力。与以往仅扩大模型规模或上下文长度的代理不同，MiroThinker 在模型层面探索交互扩展，系统地训练模型处理更深层次、更频繁的代理-环境交互，将其作为性能提升的第三个维度。与 LLM 测试时扩展（孤立运行且长推理链可能导致性能下降）不同，交互扩展利用环境反馈和外部信息获取来纠正错误并优化轨迹。通过强化学习，该模型实现了高效的交互扩展：凭借 256K 上下文窗口，它每个任务可执行多达 600 次工具调用，支持持续的多轮推理和复杂的现实世界研究工作流。在四个代表性基准测试（GAIA、HLE、BrowseComp 和 BrowseComp-ZH）上，72B 变体的准确率分别高达 81.9%、37.7%、47.1% 和 55.6%，超越了以往的开源代理，并接近 GPT-5-high 等商业产品。我们的分析表明，MiroThinker 持续受益于交互扩展：随着模型参与更深层次、更频繁的代理-环境交互，研究性能可预测地提升，证明交互深度展现出与模型规模和上下文长度类似的扩展行为。这些发现确立了交互扩展作为构建下一代开源研究代理的第三个关键维度，补充了模型容量和上下文窗口。

---

### 23 Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction

**link**: https://arxiv.org/pdf/2511.13410.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 随着智能个人设备的兴起，面向服务的人机交互日益普遍。这一趋势凸显了对个性化对话助手的需求，此类助手需理解用户特定特征以准确解读需求并根据个人偏好定制响应。然而，现有方法常忽视长期交互的复杂性，且未能捕捉用户的主观特征。为解决这些问题，本文提出PAL-Bench，一个新的基准，用于评估面向服务的助手在长期用户-智能体交互中的个性化能力。在缺乏真实世界数据的情况下，本文开发了基于LLM的多步骤合成 pipeline，并经人工标注者验证和完善，生成了PAL-Set——首个包含多会话用户日志和对话历史的中文数据集，作为PAL-Bench的基础。此外，为改进个性化服务交互，本文提出H²Memory，一种分层异构内存框架，结合检索增强生成以提升个性化响应生成。在PAL-Bench和外部数据集上的综合实验证明了所提内存框架的有效性。

---

### 24 Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents

**link**: https://arxiv.org/pdf/2511.13593.pdf
**date**: 2025-11-19
**keywords**: cs.CL
**abs**: 近年来，基于LLM的智能体在生成类人响应方面展现出巨大潜力，但在复杂环境中维持长期交互仍面临挑战，主要受限于上下文一致性和动态个性化。现有内存系统通常依赖检索前的语义分组，可能忽略语义无关但关键的用户信息并引入检索噪声。本文提出O-Mem的初步设计，这是一种基于主动用户画像的新型内存框架，能从用户与智能体的主动交互中动态提取和更新用户特征及事件记录。O-Mem支持人物属性和主题相关上下文的分层检索，实现更具适应性和连贯性的个性化响应。实验表明，O-Mem在公共LoCoMo基准上达到51.76%（较之前最先进的LangMem提升近3%），在PERSONAMEM上达到62.99%（较之前最先进的A-Mem提升3.5%），且与以往内存框架相比，提高了令牌和交互响应时间效率。该工作为开发高效、类人的个性化AI助手开辟了广阔前景。

---

### 25 Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge

**link**: https://arxiv.org/pdf/2511.11585.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 大型生成模型（如语言和扩散模型）虽能实现高质量文本和图像合成，但在跨设备联邦场景中因计算通信负担重及统计/系统异质性，难以训练或适应。本文提出FedGen-Edge框架，将冻结的预训练全局骨干网络与轻量级客户端适配器解耦，仅联邦化适配器。通过低秩适应（LoRA）将客户端更新限制在紧凑子空间，比全模型FedAvg减少99%以上上行流量，在非IID数据下稳定聚合，并自然支持个性化（每个客户端可保留本地调优适配器）。在语言建模（PTB）和图像生成（CIFAR-10）任务上，FedGen-Edge比强基线具有更低的困惑度/FID和更快收敛，同时保留简单的FedAvg风格服务器。消融实验表明，LoRA秩超过中等水平后收益递减，且本地轮次与客户端漂移存在权衡。该框架为异构边缘设备上隐私保护、资源感知和个性化生成AI提供实用路径。

---

### 26 R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models

**link**: https://arxiv.org/pdf/2511.11685.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 预训练模型在时间序列预测中展现出卓越的泛化能力，然而，将其适应于不断演变的数据分布仍然是一个重大挑战。关键障碍在于无法访问原始训练数据，仅使用新数据进行微调往往会导致灾难性遗忘。为解决这一问题，我们提出了重放调优（R-Tuning），这是一种专为预训练时间序列模型的持续适应而设计的新型框架。R-Tuning通过频率感知重放策略构建了一个统一的 latent 空间，以捕获先前任务和当前任务的知识。具体而言，它通过多频段小波分解增强模型生成的样本，生成保留趋势和融合增强的变体，从而提高表示多样性和重放效率。为进一步减少对合成样本的依赖，R-Tuning引入了 latent 一致性约束，使新表示与先前任务空间对齐。该约束引导在紧凑且语义连贯的 latent 空间内进行联合优化，确保稳健的知识保留和适应。大量实验结果表明，R-Tuning具有优越性，在新任务上分别将MAE和MSE降低了高达46.9%和46.8%，同时在旧任务上保留知识的增益高达5.7%和6.0%。值得注意的是，在少样本设置下，即使合成代理样本仅占新任务数据集的5%，R-Tuning仍优于所有最先进的基线方法。

---

### 27 FSC-Net: Fast-Slow Consolidation Networks for Continual Learning

**link**: https://arxiv.org/pdf/2511.11707.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 持续学习面临灾难性遗忘的挑战，即神经网络在学习新任务时会丢失先前获得的知识。受神经科学中记忆巩固的启发，本文提出了FSC-Net（快速-慢速巩固网络），这是一种双网络架构，将快速任务学习与渐进式知识巩固分开。该方法采用快速网络（NN1）用于快速适应新任务，以及慢速网络（NN2）通过蒸馏和重放来巩固知识。在评估的基于MLP的NN1变体中，巩固效果更多地由方法驱动而非架构装饰——简单的MLP比更复杂的相似性门控变体性能高出1.2个百分点。通过系统的超参数分析，发现在巩固过程中纯重放（无蒸馏）能实现更优性能，这与快速网络的蒸馏会引入近期偏差的假设一致。在Split-MNIST（30个种子）上，FSC-Net实现了91.71%±0.62%的保留准确率，比单独的快速网络（87.43%±1.27%）提高了4.27个百分点（配对t=23.585，p<1e-10）。在Split-CIFAR-10（5个种子）上，该方法实现了33.31%±0.38%的保留率，比单独的快速网络（25.11%±1.61%）提高了8.20个百分点（配对t=9.75，p<1e-3），尽管绝对性能（33.31%）仍然适中且低于随机预期，突出了对更强骨干网络的需求。结果提供了经验证据，表明双时间尺度巩固机制而非架构复杂性是在此设置下减轻灾难性遗忘的核心。

---

### 28 RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems

**link**: https://arxiv.org/pdf/2511.12979.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 检索增强生成（RAG）是构建可靠、知识密集型大型语言模型（LLM）应用的关键范式。然而，RAG系统的多阶段 pipeline（检索、生成）和独特的工作负载特征（如知识依赖性）对服务性能优化提出了重大挑战。现有通用LLM推理轨迹未能捕捉这些RAG特定动态，导致学术研究与实际部署之间存在显著性能差距。本文介绍了RAGPulse，一个开源RAG工作负载跟踪数据集。该数据集来自2024年4月以来为4万多名师生服务的大学范围问答系统。详细阐述了RAGPulse的系统架构、基于隐私保护哈希的数据格式，并提供了深入的统计分析。分析表明，真实世界的RAG工作负载表现出显著的时间局部性和高度倾斜的热门文档访问模式。RAGPulse为研究人员开发和验证RAG系统的新型优化策略（如内容感知批处理和检索缓存）提供了高保真基础，最终提高RAG服务的效率和可靠性。

---

### 29 Incoherent Beliefs & Inconsistent Actions in Large Language Models

**link**: https://arxiv.org/pdf/2511.13240.pdf
**date**: 2025-11-19
**keywords**: cs.LG
**abs**: 现实世界的任务和环境与大型语言模型（LLMs）通常被评估的静态数据集存在差异。此类任务可能涉及顺序交互，需要根据新证据连贯地更新信念，并基于这些信念做出适当决策。本文研究了LLM性能的两个关键方面：LLM连贯更新信念的能力，以及其采取的行动与所持信念的一致性程度。研究发现，LLM在信念更新方面存在很大不一致性，模型直接得出的后验概率与先验概率的正确更新之间平均可相差高达30%。此外，LLM经常采取与其所持信念不一致的行动，例如在 betting市场中，LLM的下注方向往往与其对潜在结果的内部信念不一致。研究还发现，LLM在用户对给定答案提出质疑时也存在一定程度的自我不一致性。最后，即使是在相关任务上准确率高或校准良好的强模型，也存在上述问题。这些结果凸显了在复杂现实世界环境中预测LLM行为的困难，与LLM的记忆机制中信念维持与更新相关。