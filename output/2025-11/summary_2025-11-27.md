### 1 MTA: A Merge-then-Adapt Framework for Personalized Large Language Model

**link**: https://arxiv.org/pdf/2511.20072.pdf  
**date**: 2025-11-27  
**keywords**: cs.CL  
**abs**: 本文提出MTA框架，解决个性化大型语言模型（PLLMs）的存储成本高和少样本性能差问题。MTA分三阶段：预训练共享Meta-LoRA Bank、自适应LoRA融合实现动态个性化组合、LoRA堆叠支持少样本微调。在LaMP基准上，该方法优于现有SOTA，显著降低存储开销并提升灵活性。  

---

### 2 Latent Collaboration in Multi-Agent Systems

**link**: https://arxiv.org/pdf/2511.20639.pdf  
**date**: 2025-11-27  
**keywords**: cs.CL  
**abs**: 本文引入LatentMAS框架，实现多智能体系统（MAS）在连续潜在空间中的协作，避免基于文本通信的瓶颈。通过隐藏嵌入生成和共享工作记忆，确保无损信息交换。理论分析显示其表达能力和效率更高，在9个基准测试中准确率提升14.6%，推理速度加快4倍。  

---

### 3 What does it mean to understand language?

**link**: https://arxiv.org/pdf/2511.19757.pdf  
**date**: 2025-11-27  
**keywords**: Personal Memory  
**abs**: 本文探讨语言理解的本质，提出深度理解需将语言系统输出与其他大脑区域整合，以构建心理模型和调用世界知识。通过认知神经科学证据，强调语言处理的核心限制，并建议新策略揭示理解在神经层面的机制。  

---

### 4 Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs

**link**: https://arxiv.org/pdf/2511.19852.pdf  
**date**: 2025-11-27  
**keywords**: Personal Memory  
**abs**: 本文提出PersonaPulse框架，优化大型语言模型（LLMs）的人格表达提示。利用LLMs固有知识迭代增强角色扮演提示，结合情境响应基准评估。实验显示其提示优于心理学设计方法，并揭示模型大小与人格控制的关系，为自适应AI交互提供基础。  

---

### 5 Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data

**link**: https://arxiv.org/pdf/2511.19498.pdf  
**date**: 2025-11-27  
**keywords**: cs.LG  
**abs**: 本文设计分层双策略遗忘框架，解决医疗LLMs隐私风险。通过几何约束梯度更新和概念感知令牌干预，精确移除敏感知识并保留基础能力。在MedMCQA和MHQA数据集上，遗忘率达82.7%，保留率88.5%，仅修改0.1%参数，满足临床合规需求。  

---

### 6 Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning

**link**: https://arxiv.org/pdf/2511.20196.pdf  
**date**: 2025-11-27  
**keywords**: cs.AI  
**abs**: 本文提出雕刻记忆遗忘适配器（SMFA），解决多模态LLMs（MLLMs）遗忘敏感信息时的性能下降问题。SMFA微调模型拒绝敏感响应，并应用保留锚点掩码机制保护无关知识。在S-MLLMUn Bench基准上，实现精确遗忘同时保持图像理解能力。  

---

### 7 Improving Language Agents through BREW

**link**: https://arxiv.org/pdf/2511.20297.pdf  
**date**: 2025-11-27  
**keywords**: cs.AI  
**abs**: 本文引入BREW框架，通过结构化记忆优化LLM智能体。利用知识库构建和细化，结合记忆分区方法提升检索效率。在OSWorld等基准上，任务精度提高10-20%，API调用减少10-15%，推理时间缩短，为智能体提供透明可控的行为基础。  

---

### 8 Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems

**link**: https://arxiv.org/pdf/2511.19490.pdf  
**date**: 2025-11-27  
**keywords**: cs.LG  
**abs**: 本文提出基于GAN的CSI反馈学习方法，解决大规模MIMO-OFDM系统中动态环境适应问题。将GAN生成器作为记忆单元保存历史知识，避免灾难性遗忘。仿真显示其增强DAE框架泛化能力，保持低内存开销，并兼容高级CSI模型。  

---

### 9 MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology

**link**: https://arxiv.org/pdf/2511.20490.pdf  
**date**: 2025-11-27  
**keywords**: cs.LG  
**abs**: 本文推出MTBBench基准，评估多模态LLMs在肿瘤临床决策中的纵向推理能力。通过模拟分子委员会环境，揭示LLMs在处理时间分辨数据和多模态冲突时的幻觉问题。提供工具框架提升推理性能，任务级改进达11.2%。  

---

### 10 Automating Deception: Scalable Multi-Turn LLM Jailbreaks

**link**: https://arxiv.org/pdf/2511.19517.pdf  
**date**: 2025-11-27  
**keywords**: cs.LG  
**abs**: 本文自动化生成多轮越狱数据集，基于心理学原理（如登门槛效应）绕过LLM安全机制。在1500个非法场景测试中，GPT模型攻击成功率最高增32%，Gemini和Claude显示强抗性，突显对话上下文安全漏洞。  

---

### 11 Learning Massively Multitask World Models for Continuous Control

**link**: https://arxiv.org/pdf/2511.19584.pdf  
**date**: 2025-11-27  
**keywords**: cs.LG  
**abs**: 本文提出Newt框架，实现单个智能体在200个任务上的在线强化学习。通过语言条件多任务世界模型预训练任务感知表示，再联合优化在线交互。实验显示其多任务性能和数据效率优于基线，支持开环控制和快速适应未见任务。  

---

### 12 Adaptive Hopfield Network: Rethinking Similarities in Associative Memory

**link**: https://arxiv.org/pdf/2511.20609.pdf  
**date**: 2025-11-27  
**keywords**: cs.LG  
**abs**: 本文重新定义联想记忆检索问题，提出自适应相似性机制，通过上下文学习近似查询生成概率。理论证明其在噪声、掩码和偏置变体下的最优性。集成到自适应Hopfield网络（A-Hop）中，在记忆检索和分类任务上达到SOTA性能。